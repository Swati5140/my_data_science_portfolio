{
  
    
        "post0": {
            "title": "Predicting the Compressive Strength of concrete using machine learning",
            "content": "Connection to the drive . # Mounting drive from google.colab import drive drive.mount(&#39;/content/drive&#39;) . . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . Install and import the required libraries and packages . !pip install pingouin . Requirement already satisfied: pingouin in /usr/local/lib/python3.7/dist-packages (0.4.0) Requirement already satisfied: pandas-flavor&gt;=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.2.0) Requirement already satisfied: seaborn&gt;=0.9.0 in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.11.1) Requirement already satisfied: outdated in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.2.1) Requirement already satisfied: matplotlib&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from pingouin) (3.2.2) Requirement already satisfied: numpy&gt;=1.19 in /usr/local/lib/python3.7/dist-packages (from pingouin) (1.19.5) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.22.2.post1) Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.8.9) Requirement already satisfied: scipy&gt;=1.7 in /usr/local/lib/python3.7/dist-packages (from pingouin) (1.7.1) Requirement already satisfied: statsmodels&gt;=0.12.0 in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.12.2) Requirement already satisfied: pandas&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from pingouin) (1.1.5) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.0.2-&gt;pingouin) (2.8.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.0.2-&gt;pingouin) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.0.2-&gt;pingouin) (1.3.1) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.0.2-&gt;pingouin) (0.10.0) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler&gt;=0.10-&gt;matplotlib&gt;=3.0.2-&gt;pingouin) (1.15.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=1.0-&gt;pingouin) (2018.9) Requirement already satisfied: xarray in /usr/local/lib/python3.7/dist-packages (from pandas-flavor&gt;=0.2.0-&gt;pingouin) (0.18.2) Requirement already satisfied: patsy&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels&gt;=0.12.0-&gt;pingouin) (0.5.1) Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated-&gt;pingouin) (0.2.2) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated-&gt;pingouin) (2.23.0) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;outdated-&gt;pingouin) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;outdated-&gt;pingouin) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;outdated-&gt;pingouin) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;outdated-&gt;pingouin) (2021.5.30) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;pingouin) (1.0.1) Requirement already satisfied: setuptools&gt;=40.4 in /usr/local/lib/python3.7/dist-packages (from xarray-&gt;pandas-flavor&gt;=0.2.0-&gt;pingouin) (57.4.0) . . !pip install emoji . Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.4.2) . . #collapse_output !pip install emojis . Requirement already satisfied: emojis in /usr/local/lib/python3.7/dist-packages (0.6.0) . . #Packages related to general operating system &amp; warnings import os import warnings warnings.filterwarnings(&#39;ignore&#39;) # importing required libraries and packages import pandas as pd from pandas.plotting import table import numpy as np import matplotlib.pyplot as plt import seaborn as sns from IPython.display import display_html import emoji import emojis import scipy from scipy.stats import pearsonr, spearmanr import pingouin as pg from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn import metrics from sklearn.linear_model import LinearRegression, Lasso, Ridge from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import cross_val_score, KFold, GridSearchCV from sklearn.ensemble import GradientBoostingRegressor from sklearn.ensemble import AdaBoostRegressor from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import BaggingRegressor from sklearn.svm import SVR from xgboost import XGBRegressor from xgboost import cv from sklearn.tree import DecisionTreeRegressor from sklearn.model_selection import RepeatedKFold print(emoji.emojize(&quot;:laptop:&quot;)*28 ,&quot; n nAll the required libraries and packages are imported successfully !!! n n&quot; ,emoji.emojize(&quot;:laptop:&quot;)*28) . . 💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻 All the required libraries and packages are imported successfully !!! 💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻 . Load the dataset . #loading the datasets data = pd.read_csv(&#39;/content/drive/MyDrive/ColabNotebooks/1019_CONCRETE_STRENGTH/concrete.csv&#39;) print(emoji.emojize(&quot;:file_folder:&quot;)*12 ,&quot; n nData loaded successfully !!! n n&quot; ,emoji.emojize(&quot;:file_folder:&quot;)*12) . . 📁📁📁📁📁📁📁📁📁📁📁📁 Data loaded successfully !!! 📁📁📁📁📁📁📁📁📁📁📁📁 . # To have a glimpse of the data print(&quot; nGlimpse of data : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A60B2E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).highlight_max(color=&#39;#0074FF&#39;,axis=0) .highlight_min(color=&#39;#00FFE5&#39;,axis=0) .highlight_null(null_color=&#39;#CCB3C&#39;) . . Glimpse of data : 👇🏻👇🏻👇🏻 . cement slag ash water superplastic coarseagg fineagg age strength . 0 540.000000 | 0.000000 | 0.000000 | 162.000000 | 2.500000 | 1040.000000 | 676.000000 | 28 | 79.990000 | . 1 540.000000 | 0.000000 | 0.000000 | 162.000000 | 2.500000 | 1055.000000 | 676.000000 | 28 | 61.890000 | . 2 332.500000 | 142.500000 | 0.000000 | 228.000000 | 0.000000 | 932.000000 | 594.000000 | 270 | 40.270000 | . 3 332.500000 | 142.500000 | 0.000000 | 228.000000 | 0.000000 | 932.000000 | 594.000000 | 365 | 41.050000 | . 4 198.600000 | 132.400000 | 0.000000 | 192.000000 | 0.000000 | 978.400000 | 825.500000 | 360 | 44.300000 | . 5 266.000000 | 114.000000 | 0.000000 | 228.000000 | 0.000000 | 932.000000 | 670.000000 | 90 | 47.030000 | . 6 380.000000 | 95.000000 | 0.000000 | 228.000000 | 0.000000 | 932.000000 | 594.000000 | 365 | 43.700000 | . 7 380.000000 | 95.000000 | 0.000000 | 228.000000 | 0.000000 | 932.000000 | 594.000000 | 28 | 36.450000 | . 8 266.000000 | 114.000000 | 0.000000 | 228.000000 | 0.000000 | 932.000000 | 670.000000 | 28 | 45.850000 | . 9 475.000000 | 0.000000 | 0.000000 | 228.000000 | 0.000000 | 932.000000 | 594.000000 | 28 | 39.290000 | . # Renaming some columns for better typing and calling variables data.rename(columns={&quot;coarseagg&quot;:&quot;coarse_agg&quot;,&quot;fineagg&quot;:&quot;fine_agg&quot;},inplace=True) print(emojis.encode(&quot;:pencil2:&quot;)*25 ,&quot; n nRenamed some columns for better typing and calling variables n n&quot; ,emojis.encode(&quot;:pencil2:&quot;)*25) . . ✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️ Renamed some columns for better typing and calling variables ✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️ . Basic Info of the dataset . #finding the no. of rows and cols print(&quot; nFinding the no. of rows and cols in the dataset : n n&quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;No. of Cement Samples : {}&quot;.format(data.shape[0])) print(&quot;No. of Cement Components : {} including target(Strength)&quot;.format(data.shape[1])) . . Finding the no. of rows and cols in the dataset : 👇🏻👇🏻👇🏻 No. of Cement Samples : 1030 No. of Cement Components : 9 including target(Strength) . # Overview of shape, attributes, types and missing values print(&quot; nOverview of shape, attributes, types and missing values : n n&quot; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.info() . . Overview of shape, attributes, types and missing values : 👇🏻👇🏻👇🏻 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1030 entries, 0 to 1029 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 cement 1030 non-null float64 1 slag 1030 non-null float64 2 ash 1030 non-null float64 3 water 1030 non-null float64 4 superplastic 1030 non-null float64 5 coarse_agg 1030 non-null float64 6 fine_agg 1030 non-null float64 7 age 1030 non-null int64 8 strength 1030 non-null float64 dtypes: float64(8), int64(1) memory usage: 72.5 KB . 📝📝 Essence of above : ✏️ It shows that there are 8 independent variables (cement, slag, ash, water, superplastic, coarse_agg, fine_agg &amp; age) and 1 dependent variable (strength). ✏️ All the records are numeric. . # General stats of the numerical variables print(&quot; nGeneral stats of the numerical variables : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.describe().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#753976&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . General stats of the numerical variables : 👇🏻👇🏻👇🏻 . cement slag ash water superplastic coarse_agg fine_agg age strength . count 1030.000000 | 1030.000000 | 1030.000000 | 1030.000000 | 1030.000000 | 1030.000000 | 1030.000000 | 1030.000000 | 1030.000000 | . mean 281.167864 | 73.895825 | 54.188350 | 181.567282 | 6.204660 | 972.918932 | 773.580485 | 45.662136 | 35.817961 | . std 104.506364 | 86.279342 | 63.997004 | 21.354219 | 5.973841 | 77.753954 | 80.175980 | 63.169912 | 16.705742 | . min 102.000000 | 0.000000 | 0.000000 | 121.800000 | 0.000000 | 801.000000 | 594.000000 | 1.000000 | 2.330000 | . 25% 192.375000 | 0.000000 | 0.000000 | 164.900000 | 0.000000 | 932.000000 | 730.950000 | 7.000000 | 23.710000 | . 50% 272.900000 | 22.000000 | 0.000000 | 185.000000 | 6.400000 | 968.000000 | 779.500000 | 28.000000 | 34.445000 | . 75% 350.000000 | 142.950000 | 118.300000 | 192.000000 | 10.200000 | 1029.400000 | 824.000000 | 56.000000 | 46.135000 | . max 540.000000 | 359.400000 | 200.100000 | 247.000000 | 32.200000 | 1145.000000 | 992.600000 | 365.000000 | 82.600000 | . 📝📝 Essence of above dataframe : ✏️ cement: The min. value of the cement component is 102 &amp; the max. value is 540 while the average value is 281.16. ✏️ slag: The average value of the slag component is 73.90 and the max. value is 359.50. ✏️ ash: The average value of the ash component is 54.18 and the max. value is 200.10. ✏️ water: The min. value of the water component is 121.80 &amp; the max. value is 247 while the average value is 181.57. ✏️ superplastic: The average value of the superplastic component is 6.20 and the max. value is 32.20. ✏️ coarse_agg: The min. value of the coarse_agg component is 801 &amp; the max. value is 1145 while the average value is 972.92. ✏️ fine_agg: The min. value of the fine_agg component is 594 &amp; the max. value is 992.60 while the average value is 773.58. ✏️ age: The min. value of the age component is 1 day &amp; the max. value is 365 days while the average value is 45.66 days. ✏️ strength: The min. value of the concrete compressive strength component is 2.33 &amp; the max. value is 82.60 while the average value is 35.82. . Exploratory Data Analysis . 1. Finding duplicate values . # checking for duplicate values if present in the dataframe print(&quot;Duplicate Data&quot; ,emoji.emojize(&quot;:red_question_mark:&quot;)*2,&quot; n&quot;) print(emoji.emojize(&quot;:cross_mark_button:&quot;)*3 ,&quot; n n&quot;,data.duplicated().any() ,&quot; n n&quot;,emoji.emojize(&quot;:cross_mark_button:&quot;)*3) . . Duplicate Data ❓❓ ❎❎❎ True ❎❎❎ . # Selecting duplicate rows except first occurence based on all columns duplicate_data = data[data.duplicated(keep = &quot;last&quot;)] print(&#39; nduplicate rows : &#39; ,&quot;{} n&quot;.format(duplicate_data.shape[0]) ,&quot; n n&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) # print the resulatant dataframe containing duplicate rows duplicate_data.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#2B0E46&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . duplicate rows : 25 👇🏻👇🏻👇🏻 . cement slag ash water superplastic coarse_agg fine_agg age strength . 72 425.000000 | 106.300000 | 0.000000 | 153.500000 | 16.500000 | 852.100000 | 887.100000 | 3 | 33.400000 | . 77 425.000000 | 106.300000 | 0.000000 | 153.500000 | 16.500000 | 852.100000 | 887.100000 | 3 | 33.400000 | . 83 362.600000 | 189.000000 | 0.000000 | 164.900000 | 11.600000 | 944.700000 | 755.800000 | 3 | 35.300000 | . 86 362.600000 | 189.000000 | 0.000000 | 164.900000 | 11.600000 | 944.700000 | 755.800000 | 3 | 35.300000 | . 88 362.600000 | 189.000000 | 0.000000 | 164.900000 | 11.600000 | 944.700000 | 755.800000 | 3 | 35.300000 | . 95 425.000000 | 106.300000 | 0.000000 | 153.500000 | 16.500000 | 852.100000 | 887.100000 | 7 | 49.200000 | . 100 425.000000 | 106.300000 | 0.000000 | 153.500000 | 16.500000 | 852.100000 | 887.100000 | 7 | 49.200000 | . 106 362.600000 | 189.000000 | 0.000000 | 164.900000 | 11.600000 | 944.700000 | 755.800000 | 7 | 55.900000 | . 109 362.600000 | 189.000000 | 0.000000 | 164.900000 | 11.600000 | 944.700000 | 755.800000 | 7 | 55.900000 | . 118 425.000000 | 106.300000 | 0.000000 | 153.500000 | 16.500000 | 852.100000 | 887.100000 | 28 | 60.290000 | . # removing duplicate values data = data.drop_duplicates(keep=&#39;last&#39;) print(emojis.encode(&quot;:scissors:&quot;)*16 ,&quot; n nDuplicate data removed successfully !! n n&quot; ,emojis.encode(&quot;:scissors:&quot;)*16) . . ✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️ Duplicate data removed successfully !! ✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️ . 📝📝 There are 25 rows of duplicated data as shown above and after removing those rows we are left with 1005 rows of data. . 2. Finding Missing Values . #checking for missing values print(&quot;Missing values&quot; ,emoji.emojize(&quot;:red_question_mark:&quot;)*2,&quot; n&quot;) print(emoji.emojize(&quot;:check_mark_button:&quot;)*3 ,&quot; n n&quot;,data.isnull().values.any() ,&quot; n n&quot;,emoji.emojize(&quot;:check_mark_button:&quot;)*3) . . Missing values ❓❓ ✅✅✅ False ✅✅✅ . print(&#39; nMissing Values in each column of the data : n n&#39; ,&quot; &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.isnull().sum().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A15F86&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Missing Values in each column of the data : 👇🏻👇🏻👇🏻 . 0 . cement 0 | . slag 0 | . ash 0 | . water 0 | . superplastic 0 | . coarse_agg 0 | . fine_agg 0 | . age 0 | . strength 0 | . 📝📝 There are no missing or null values in the data and can be seen from the above dataframe. . 3. Finding Components with one value . # All the features with their unique values print(&#39; nUnique Values in each column of the data : n n&#39; ,&quot; &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) for column in data.columns: print(emoji.emojize(&quot;:arrow_right:&quot;, use_aliases=True) ,column ,emoji.emojize(&quot;:1234:&quot;, use_aliases=True) ,data[column].nunique()) . . Unique Values in each column of the data : 👇🏻👇🏻👇🏻 ➡ cement 🔢 278 ➡ slag 🔢 185 ➡ ash 🔢 156 ➡ water 🔢 195 ➡ superplastic 🔢 111 ➡ coarse_agg 🔢 284 ➡ fine_agg 🔢 302 ➡ age 🔢 14 ➡ strength 🔢 845 . 📝📝 There is no cement component with 1 value. . 4. Exploring Cement Components . #list of independent numerical variables numerical_features = [feature for feature in data.columns if ((data[feature].dtypes!=&#39;O&#39;) &amp; (feature not in [&#39;strength&#39;]))] print(&#39;No. of independent components&#39; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;)*2 ,len(numerical_features)) print(&quot;&quot;) #all the independent numerical variables for feature in numerical_features: print(&quot;The component&quot; ,emoji.emojize(&quot;:memo:&quot;) ,&quot;&#39;{}&#39;&quot;.format(feature) ,&quot;has datatype&quot; ,emoji.emojize(&quot;:1234:&quot;, use_aliases=True) ,&quot;{}&quot;.format(data[feature].dtypes) ,&quot;and&quot; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;) ,&quot;{}&quot;.format(len(data[feature].unique())) ,&quot;unique values&quot;) # dependent feature target_feature = [feature for feature in data.columns if ((data[feature].dtypes!=&#39;O&#39;) &amp; (feature in [&#39;strength&#39;]))] for feature in target_feature: print(&quot; nThe dependent component&quot; ,emoji.emojize(&quot;:memo:&quot;) ,&quot;&#39;{}&#39;&quot;.format(feature) ,&quot;has datatype&quot; ,emoji.emojize(&quot;:1234:&quot;, use_aliases=True) ,&quot;{}&quot;.format(data[feature].dtypes) ,&quot;and&quot; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;) ,&quot;{}&quot;.format(len(data[feature].unique())) ,&quot;unique values&quot;) . . No. of independent components 👉🏻👉🏻 8 The component 📝 &#39;cement&#39; has datatype 🔢 float64 and 👉🏻 278 unique values The component 📝 &#39;slag&#39; has datatype 🔢 float64 and 👉🏻 185 unique values The component 📝 &#39;ash&#39; has datatype 🔢 float64 and 👉🏻 156 unique values The component 📝 &#39;water&#39; has datatype 🔢 float64 and 👉🏻 195 unique values The component 📝 &#39;superplastic&#39; has datatype 🔢 float64 and 👉🏻 111 unique values The component 📝 &#39;coarse_agg&#39; has datatype 🔢 float64 and 👉🏻 284 unique values The component 📝 &#39;fine_agg&#39; has datatype 🔢 float64 and 👉🏻 302 unique values The component 📝 &#39;age&#39; has datatype 🔢 int64 and 👉🏻 14 unique values The dependent component 📝 &#39;strength&#39; has datatype 🔢 float64 and 👉🏻 845 unique values . 5. Distribution of Cement Components . def num_histplot(feature, dataset): &quot;&quot;&quot; It takes the numerical variable and dataset as input and plots the histogram for the particular. &quot;&quot;&quot; if feature in numerical_features: print(&quot; n&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&quot;Distribution of &#39;{}&#39; &quot;.format(feature) ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(10,8)) ax = sns.histplot(dataset[feature], kde=True, bins=10,color=&#39;#F78AB2&#39;) plt.show() else: print(emoji.emojize(&quot;:cross_mark:&quot;)*2 ,&quot;The component is not numeric !!!&quot; ,emoji.emojize(&quot;:cross_mark:&quot;)*2) . . 6. Distribution of all the Cement Components . def num_boxplot(feature, dataset): &quot;&quot;&quot; It takes the numerical variable and dataset as input and plots the boxplot for the particular with styler object of Skewness, Kurtosis, Median, Count, Mean ,Standard Deviation, Min. value, Q1, Q2, Q3, Q4, Max. value, IQR, Lower Outliers Limit ,Upper Outliers Limit, Lower Outliers Count with percentage, Upper Outliers Count with percentage,Outliers Count with percentage . &quot;&quot;&quot; if feature in numerical_features: print(&quot; n&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&quot;Boxplot of &#39;{}&#39; &quot;.format(feature) ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&quot; n&quot;) print(&quot;&quot;) #plotting the boxplot plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(10,8)) sns.boxplot(x=feature , data=dataset, palette=&#39;hls&#39;) plt.xlabel(feature) plt.show() # Parameters to check presence of outliers in the distribution v1 = pd.DataFrame({&#39;Parameters&#39;: &#39;Skewness&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(dataset[feature].skew())},index={&#39;1&#39;}) v2 = pd.DataFrame({&#39;Parameters&#39;: &#39;Kurtosis&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(dataset[feature].kurtosis())},index={&#39;2&#39;}) v3 = pd.DataFrame({&#39;Parameters&#39;: &#39;Median&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(dataset[feature].quantile())},index={&#39;3&#39;}) v4 = pd.DataFrame({&#39;Parameters&#39;: &#39;Count&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[0])},index={&#39;4&#39;}) v5 = pd.DataFrame({&#39;Parameters&#39;: &#39;Mean&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[1])},index={&#39;5&#39;}) v6 = pd.DataFrame({&#39;Parameters&#39;: &#39;Stand. Dev.&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[2])},index={&#39;6&#39;}) v7 = pd.DataFrame({&#39;Parameters&#39;: &#39;Minimum&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[3])},index={&#39;7&#39;}) v8 = pd.DataFrame({&#39;Parameters&#39;: &#39;Q1 (25%)&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[4])},index={&#39;8&#39;}) v9 = pd.DataFrame({&#39;Parameters&#39;: &#39;Q2 (50%)&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[5])},index={&#39;9&#39;}) v10 = pd.DataFrame({&#39;Parameters&#39;: &#39;Q3 (75%)&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[6])},index={&#39;10&#39;}) v11 = pd.DataFrame({&#39;Parameters&#39;: &#39;Maximum&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[7])},index={&#39;11&#39;}) #finding Interquartile range iqr = dataset[feature].quantile(q = 0.75) - dataset[feature].quantile(q = 0.25) v12 = pd.DataFrame({&#39;Parameters&#39;: &#39;IQR&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(iqr)},index={&#39;12&#39;}) #Outlier detection from IQR lower_outliers = dataset[feature].quantile(q = 0.25) - (iqr*1.5) v13 = pd.DataFrame({&#39;Parameters&#39;: &#39;Lower outliers Limit&#39; , &#39;Values&#39; : &quot;{:.2f} &quot;.format(lower_outliers)},index={&#39;13&#39;}) upper_outliers = dataset[feature].quantile(q = 0.75) + (iqr*1.5) v14 = pd.DataFrame({&#39;Parameters&#39;: &#39;Upper outliers Limit&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(upper_outliers)},index={&#39;14&#39;}) #checking the presence of outliers with upper &amp; lower limits lower_out_count = dataset[(dataset[feature] &lt; (dataset[feature].quantile(q = 0.25)-(iqr*1.5)) )][feature].count() lower_out_pct = round(lower_out_count / dataset[feature].count() * 100, 1) v15 = pd.DataFrame({&#39;Parameters&#39;: &#39;No. of Lower outliers&#39; , &#39;Values&#39; : &quot;{} ({}%)&quot;.format(lower_out_count, lower_out_pct)},index={&#39;15&#39;}) upper_out_count = dataset[(dataset[feature] &gt; (dataset[feature].quantile(q = 0.75)+(iqr*1.5)) )][feature].count() upper_out_pct = round(upper_out_count / dataset[feature].count() * 100, 1) v16 = pd.DataFrame({&#39;Parameters&#39;: &#39;No. of Upper outliers&#39; , &#39;Values&#39; : &quot;{} ({}%)&quot;.format(upper_out_count, upper_out_pct)},index={&#39;16&#39;}) outliers = dataset[(dataset[feature]&lt; (dataset[feature].quantile(q = 0.25)-(iqr*1.5))) | (dataset[feature] &gt; (dataset[feature].quantile(q = 0.75)+(iqr*1.5)) )][feature].count() outliers_pct = round(outliers / dataset[feature].count() * 100, 1) v17 = pd.DataFrame({&#39;Parameters&#39;: &#39;No. of Outliers&#39; , &#39;Values&#39; : &quot;{} ({}%)&quot;.format(outliers, outliers_pct)},index={&#39;17&#39;}) result = pd.concat([v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17]) result.columns = [&#39;Parameters&#39;,&#39;Values&#39;] if outliers==0: print(&quot; n&quot;,emoji.emojize(&quot;:check_mark:&quot;)*2 ,emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*2 ,&quot;NO Outliers&quot; ,emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*2 ,emoji.emojize(&quot;:check_mark:&quot;)*2,&quot; n&quot;) not_outliers = dataset[(dataset[feature]&gt;= (dataset[feature].quantile(q = 0.25)-(iqr*1.5))) | (dataset[feature] &lt;= (dataset[feature].quantile(q = 0.75)+(iqr*1.5)) )][feature].count() not_outliers_pct = round(not_outliers / dataset[feature].count() * 100, 1) result = result.iloc[:-5,:] v18 = pd.DataFrame({&#39;Parameters&#39;: &#39;No.of observ. w/o outliers&#39; , &#39;Values&#39; : &quot;{} ({}%)&quot;.format(not_outliers, not_outliers_pct)},index={&#39;13&#39;}) result = pd.concat([result,v18]) result.columns = [&#39;Parameters&#39;,&#39;Values&#39;] else: print(&quot; n&quot;,emoji.emojize(&quot;:cross_mark:&quot;)*2 ,emoji.emojize(&quot;:thumbs_down_light_skin_tone:&quot;)*2 ,&quot;Outliers Present&quot; ,emoji.emojize(&quot;:thumbs_down_light_skin_tone:&quot;)*2 ,emoji.emojize(&quot;:cross_mark:&quot;)*2,&quot; n&quot;) result = result.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#34495E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#FEF5E7&#39;}, subset=[&#39;Parameters&#39;]) return result else: print(emoji.emojize(&quot;:cross_mark:&quot;)*2 ,&quot;It&#39;s not a numerical feature !!!&quot; ,emoji.emojize(&quot;:cross_mark:&quot;)*2) . . Component : cement . num_histplot(&#39;cement&#39;, data) . . 📈📈 Distribution of &#39;cement&#39; 📈📈 . 📝📝 This plot is similar to normal distribution. We can see that most of the samples of cement component are in the range of 150-300. . num_boxplot(&#39;cement&#39;, data) . . 📈📈 Boxplot of &#39;cement&#39; 📈📈 . ✔✔ 👍🏻👍🏻 NO Outliers 👍🏻👍🏻 ✔✔ . Parameters Values . 1 Skewness | 0.56 | . 2 Kurtosis | -0.43 | . 3 Median | 265.00 | . 4 Count | 1005.00 | . 5 Mean | 278.63 | . 6 Stand. Dev. | 104.34 | . 7 Minimum | 102.00 | . 8 Q1 (25%) | 190.70 | . 9 Q2 (50%) | 265.00 | . 10 Q3 (75%) | 349.00 | . 11 Maximum | 540.00 | . 12 IQR | 158.30 | . 13 No.of observ. w/o outliers | 1005 (100.0%) | . Component : slag . num_histplot(&#39;slag&#39;, data) . . 📈📈 Distribution of &#39;slag&#39; 📈📈 . . num_boxplot(&#39;slag&#39;, data) . . 📈📈 Boxplot of &#39;slag&#39; 📈📈 . ❌❌ 👎🏻👎🏻 Outliers Present 👎🏻👎🏻 ❌❌ . Parameters Values . 1 Skewness | 0.86 | . 2 Kurtosis | -0.41 | . 3 Median | 20.00 | . 4 Count | 1005.00 | . 5 Mean | 72.04 | . 6 Stand. Dev. | 86.17 | . 7 Minimum | 0.00 | . 8 Q1 (25%) | 0.00 | . 9 Q2 (50%) | 20.00 | . 10 Q3 (75%) | 142.50 | . 11 Maximum | 359.40 | . 12 IQR | 142.50 | . 13 Lower outliers Limit | -213.75 | . 14 Upper outliers Limit | 356.25 | . 15 No. of Lower outliers | 0 (0.0%) | . 16 No. of Upper outliers | 2 (0.2%) | . 17 No. of Outliers | 2 (0.2%) | . Component : ash . num_histplot(&#39;ash&#39;, data) . . 📈📈 Distribution of &#39;ash&#39; 📈📈 . . num_boxplot(&#39;ash&#39;, data) . . 📈📈 Boxplot of &#39;ash&#39; 📈📈 . ✔✔ 👍🏻👍🏻 NO Outliers 👍🏻👍🏻 ✔✔ . Parameters Values . 1 Skewness | 0.50 | . 2 Kurtosis | -1.37 | . 3 Median | 0.00 | . 4 Count | 1005.00 | . 5 Mean | 55.54 | . 6 Stand. Dev. | 64.21 | . 7 Minimum | 0.00 | . 8 Q1 (25%) | 0.00 | . 9 Q2 (50%) | 0.00 | . 10 Q3 (75%) | 118.30 | . 11 Maximum | 200.10 | . 12 IQR | 118.30 | . 13 No.of observ. w/o outliers | 1005 (100.0%) | . Component : water . num_histplot(&#39;water&#39;, data) . . 📈📈 Distribution of &#39;water&#39; 📈📈 . . num_boxplot(&#39;water&#39;, data) . . 📈📈 Boxplot of &#39;water&#39; 📈📈 . ❌❌ 👎🏻👎🏻 Outliers Present 👎🏻👎🏻 ❌❌ . Parameters Values . 1 Skewness | 0.03 | . 2 Kurtosis | 0.17 | . 3 Median | 185.70 | . 4 Count | 1005.00 | . 5 Mean | 182.08 | . 6 Stand. Dev. | 21.34 | . 7 Minimum | 121.80 | . 8 Q1 (25%) | 166.60 | . 9 Q2 (50%) | 185.70 | . 10 Q3 (75%) | 192.90 | . 11 Maximum | 247.00 | . 12 IQR | 26.30 | . 13 Lower outliers Limit | 127.15 | . 14 Upper outliers Limit | 232.35 | . 15 No. of Lower outliers | 11 (1.1%) | . 16 No. of Upper outliers | 4 (0.4%) | . 17 No. of Outliers | 15 (1.5%) | . Component : superplastic . num_histplot(&#39;superplastic&#39;, data) . . 📈📈 Distribution of &#39;superplastic&#39; 📈📈 . . num_boxplot(&#39;superplastic&#39;, data) . . 📈📈 Boxplot of &#39;superplastic&#39; 📈📈 . ❌❌ 👎🏻👎🏻 Outliers Present 👎🏻👎🏻 ❌❌ . Parameters Values . 1 Skewness | 0.98 | . 2 Kurtosis | 1.70 | . 3 Median | 6.10 | . 4 Count | 1005.00 | . 5 Mean | 6.03 | . 6 Stand. Dev. | 5.92 | . 7 Minimum | 0.00 | . 8 Q1 (25%) | 0.00 | . 9 Q2 (50%) | 6.10 | . 10 Q3 (75%) | 10.00 | . 11 Maximum | 32.20 | . 12 IQR | 10.00 | . 13 Lower outliers Limit | -15.00 | . 14 Upper outliers Limit | 25.00 | . 15 No. of Lower outliers | 0 (0.0%) | . 16 No. of Upper outliers | 10 (1.0%) | . 17 No. of Outliers | 10 (1.0%) | . Component : coarse_agg . num_histplot(&#39;coarse_agg&#39;, data) . . 📈📈 Distribution of &#39;coarse_agg&#39; 📈📈 . . num_boxplot(&#39;coarse_agg&#39;, data) . . 📈📈 Boxplot of &#39;coarse_agg&#39; 📈📈 . ✔✔ 👍🏻👍🏻 NO Outliers 👍🏻👍🏻 ✔✔ . Parameters Values . 1 Skewness | -0.07 | . 2 Kurtosis | -0.58 | . 3 Median | 968.00 | . 4 Count | 1005.00 | . 5 Mean | 974.38 | . 6 Stand. Dev. | 77.58 | . 7 Minimum | 801.00 | . 8 Q1 (25%) | 932.00 | . 9 Q2 (50%) | 968.00 | . 10 Q3 (75%) | 1031.00 | . 11 Maximum | 1145.00 | . 12 IQR | 99.00 | . 13 No.of observ. w/o outliers | 1005 (100.0%) | . Component : fine_agg . num_histplot(&#39;fine_agg&#39;, data) . . 📈📈 Distribution of &#39;fine_agg&#39; 📈📈 . . num_boxplot(&#39;fine_agg&#39;, data) . . 📈📈 Boxplot of &#39;fine_agg&#39; 📈📈 . ❌❌ 👎🏻👎🏻 Outliers Present 👎🏻👎🏻 ❌❌ . Parameters Values . 1 Skewness | -0.25 | . 2 Kurtosis | -0.11 | . 3 Median | 780.00 | . 4 Count | 1005.00 | . 5 Mean | 772.69 | . 6 Stand. Dev. | 80.34 | . 7 Minimum | 594.00 | . 8 Q1 (25%) | 724.30 | . 9 Q2 (50%) | 780.00 | . 10 Q3 (75%) | 822.20 | . 11 Maximum | 992.60 | . 12 IQR | 97.90 | . 13 Lower outliers Limit | 577.45 | . 14 Upper outliers Limit | 969.05 | . 15 No. of Lower outliers | 0 (0.0%) | . 16 No. of Upper outliers | 5 (0.5%) | . 17 No. of Outliers | 5 (0.5%) | . Component : age . num_histplot(&#39;age&#39;, data) . . 📈📈 Distribution of &#39;age&#39; 📈📈 . . num_boxplot(&#39;age&#39;, data) . . 📈📈 Boxplot of &#39;age&#39; 📈📈 . ❌❌ 👎🏻👎🏻 Outliers Present 👎🏻👎🏻 ❌❌ . Parameters Values . 1 Skewness | 3.25 | . 2 Kurtosis | 11.96 | . 3 Median | 28.00 | . 4 Count | 1005.00 | . 5 Mean | 45.86 | . 6 Stand. Dev. | 63.73 | . 7 Minimum | 1.00 | . 8 Q1 (25%) | 7.00 | . 9 Q2 (50%) | 28.00 | . 10 Q3 (75%) | 56.00 | . 11 Maximum | 365.00 | . 12 IQR | 49.00 | . 13 Lower outliers Limit | -66.50 | . 14 Upper outliers Limit | 129.50 | . 15 No. of Lower outliers | 0 (0.0%) | . 16 No. of Upper outliers | 59 (5.9%) | . 17 No. of Outliers | 59 (5.9%) | . 7. Multivariate Analysis . # Displot print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8 ,&#39; t&#39; ,&#39;Distplot of all the components&#39; ,&#39; t&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8,&#39; n&#39;) fig, ax1 = plt.subplots(3,3, figsize=(16,16)) sns.distplot(data[&#39;cement&#39;],ax=ax1[0][0]) sns.distplot(data[&#39;slag&#39;],ax=ax1[0][1]) sns.distplot(data[&#39;ash&#39;],ax=ax1[0][2]) sns.distplot(data[&#39;water&#39;],ax=ax1[1][0]) sns.distplot(data[&#39;superplastic&#39;],ax=ax1[1][1]) sns.distplot(data[&#39;coarse_agg&#39;],ax=ax1[1][2]) sns.distplot(data[&#39;fine_agg&#39;],ax=ax1[2][0]) sns.distplot(data[&#39;age&#39;],ax=ax1[2][1]) sns.distplot(data[&#39;strength&#39;],ax=ax1[2][2]) . . 📈📈📈📈📈📈📈📈 Distplot of all the components 📈📈📈📈📈📈📈📈 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6294daea90&gt; . 📝📝 Essence of above : ✏️ cement : cement is almost normal. ✏️ slag : slag has two gaussians and rightly skewed. ✏️ ash : ash has two gaussians and rightly skewed. ✏️ water : water has three gaussians and slightly left skewed. ✏️ superplastic : superplastic has two gaussians and rightly skewed. ✏️ coarse_agg : coarse_agg has three gaussians and almost normal. ✏️ fine_agg : fine_agg has almost two gaussians and looks like normal. ✏️ age : age has multiple gaussians and rightly skewed. . 8. Exploring Correlation b/w Components . # Heatmap visualization: Pearson print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8 ,&#39; t&#39; ,&#39;Heatmap visualization: Pearson&#39; ,&#39; t&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8,&#39; n&#39;) mask = np.triu(data.corr(method=&quot;pearson&quot;), 1) plt.figure(figsize=(19,9)) sns.heatmap(data.corr(method=&quot;pearson&quot;), annot=True, vmax=1, vmin = -1, square=True, mask=mask) plt.title(&#39;Correlation b/w different components&#39;) plt.show() . . 📈📈📈📈📈📈📈📈 Heatmap visualization: Pearson 📈📈📈📈📈📈📈📈 . 📝📝 Essence of above : ✏️ We can observe a high positive correlation between compressive strength and cement. This is true because concrete strength indeed increases with an increase in the amount of cement used in preparing it. ✏️ Also, age and superplastic are the other two factors influencing compressive strength. ✏️ A strong negative correlation between superplastic and water. ✏️ There is positive correlations between superplastic &amp; ash, superplastic &amp; fine_agg. . ## pairplot - plot density curve instead of histogram in diagonal print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8 ,&#39; t&#39; ,&#39;Pairplot&#39; ,&#39; t&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8,&#39; n&#39;) sns.pairplot(data, diag_kind = &#39;kde&#39;) plt.show() . . 📈📈📈📈📈📈📈📈 Pairplot 📈📈📈📈📈📈📈📈 . 9. Relationship b/w the Cement Components and the Target label . strength vs (cement,age,water) . print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8 ,&#39; t&#39; ,&#39;Scatterplot : strength vs (cement,age,water) &#39; ,&#39; t&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8,&#39; n&#39;) fig, ax = plt.subplots(figsize=(10,7)) sns.scatterplot(y=&quot;strength&quot;, x=&quot;cement&quot;, hue=&quot;water&quot;,size=&quot;age&quot;, data=data, ax=ax, sizes=(50, 300)) ax.set_title(&quot;strength vs (cement, age, water)&quot;) ax.legend(loc=&quot;upper left&quot;, bbox_to_anchor=(1,1)) plt.show() . . 📈📈📈📈📈📈📈📈 Scatterplot : strength vs (cement,age,water) 📈📈📈📈📈📈📈📈 . strength vs (fine_agg,ash,superplastic) . 📝📝 Essence of above : ✏️ Compressive strength increases with amount of cement. ✏️ Compressive strength increases with age. ✏️ Cement with low age requires more cement for higher strength. ✏️ The older the cement is the more water it requires. ✏️ Concrete strength increases when less water is used in preparing it. . print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8 ,&#39; t&#39; ,&#39;Scatterplot : strength vs (fine_agg,ash,superplastic) &#39; ,&#39; t&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8,&#39; n&#39;) fig, ax = plt.subplots(figsize=(10,7)) sns.scatterplot(y=&quot;strength&quot;, x=&quot;fine_agg&quot;, hue=&quot;ash&quot;, size=&quot;superplastic&quot;, data=data, ax=ax, sizes=(50, 300)) ax.set_title(&quot;strength vs (fine_agg, superplastic, ash)&quot;) ax.legend(loc=&quot;upper left&quot;, bbox_to_anchor=(1,1)) plt.show() . . 📈📈📈📈📈📈📈📈 Scatterplot : strength vs (fine_agg,ash,superplastic) 📈📈📈📈📈📈📈📈 . 📝📝 Essence of above : ✏️ As ash increases the strength decreases ✏️ Strength increases with superplastic&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; strength vs (fine_agg,water,superplastic) . print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8 ,&#39; t&#39; ,&#39;Scatterplot : strength vs (fine_agg,water,superplastic) &#39; ,&#39; t&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8,&#39; n&#39;) fig, ax = plt.subplots(figsize=(10,7)) sns.scatterplot(y=&quot;strength&quot;, x=&quot;fine_agg&quot;, hue=&quot;water&quot;, size=&quot;superplastic&quot;, data=data, ax=ax, sizes=(50, 300)) ax.set_title(&quot;strength vs (fine_aggr, superplastic, water)&quot;) ax.legend(loc=&quot;upper left&quot;, bbox_to_anchor=(1,1)) plt.show() . . 📈📈📈📈📈📈📈📈 Scatterplot : strength vs (fine_agg,water,superplastic) 📈📈📈📈📈📈📈📈 . 📝📝 Essence of above : ✏️ Strength decreases with increase in water, strength increases with increase in superplastic ✏️ More fine_agg is used when less water, more superplastic is used. . Data Preprocessing . Handling Outliers . Component : slag . #calculating the boundaries which differentiate the outliers Q1 = data.slag.quantile(0.25) Q3 = data.slag.quantile(0.75) IQR = Q3 - Q1 lower_bridge = Q1 - (IQR*1.5) upper_bridge = Q3 + (IQR*1.5) print(&#39; nLower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;lower boundary limit : {} nupper boundary limit : {}&quot;.format(lower_bridge,upper_bridge)) . . Lower &amp; Upper Limits 👇🏻👇🏻👇🏻 lower boundary limit : -213.75 upper boundary limit : 356.25 . #replace outliers with median data.loc[(data[&#39;slag&#39;] &gt; upper_bridge), &#39;slag&#39;] = data[&#39;slag&#39;].median() print(emoji.emojize(&quot;:plus:&quot;)*14 ,&quot; n nOutliers successfully replaced n n&quot; ,emoji.emojize(&quot;:plus:&quot;)*14) . . ➕➕➕➕➕➕➕➕➕➕➕➕➕➕ Outliers successfully replaced ➕➕➕➕➕➕➕➕➕➕➕➕➕➕ . num_boxplot(&#39;slag&#39;, data) . . 📈📈 Boxplot of &#39;slag&#39; 📈📈 . ✔✔ 👍🏻👍🏻 NO Outliers 👍🏻👍🏻 ✔✔ . Parameters Values . 1 Skewness | 0.83 | . 2 Kurtosis | -0.52 | . 3 Median | 20.00 | . 4 Count | 1005.00 | . 5 Mean | 71.37 | . 6 Stand. Dev. | 85.24 | . 7 Minimum | 0.00 | . 8 Q1 (25%) | 0.00 | . 9 Q2 (50%) | 20.00 | . 10 Q3 (75%) | 141.30 | . 11 Maximum | 342.10 | . 12 IQR | 141.30 | . 13 No.of observ. w/o outliers | 1005 (100.0%) | . Component : water . #calculating the boundaries which differentiate the outliers Q1 = data.water.quantile(0.25) Q3 = data.water.quantile(0.75) IQR = Q3 - Q1 lower_bridge = Q1 - (IQR*1.5) lower_bridge = lower_bridge.round(decimals=2) upper_bridge = Q3 + (IQR*1.5) upper_bridge = upper_bridge.round(decimals=2) print(&#39; nLower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;lower boundary limit : {} nupper boundary limit : {}&quot;.format(lower_bridge,upper_bridge)) . . Lower &amp; Upper Limits 👇🏻👇🏻👇🏻 lower boundary limit : 127.15 upper boundary limit : 232.35 . # Extreme outliers ext_lower_bridge = data.water.quantile(0.10) ext_upper_bridge = data.water.quantile(0.90) print(&#39; n Extreme Lower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; t&quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;Extreme lower boundary limit : {} nExtreme upper boundary limit : {}&quot;.format(ext_lower_bridge,ext_upper_bridge)) . . Extreme Lower &amp; Upper Limits 👇🏻👇🏻👇🏻 Extreme lower boundary limit : 155.6 Extreme upper boundary limit : 203.5 . #replace outliers with median data.loc[(data[&#39;water&#39;] &lt;= ext_lower_bridge) | (data[&#39;water&#39;] &gt; ext_upper_bridge), &#39;water&#39;] = data[&#39;water&#39;].median() print(emoji.emojize(&quot;:plus:&quot;)*18 ,&quot; n nOutliers successfully replaced with extreme limits n n&quot; ,emoji.emojize(&quot;:plus:&quot;)*18) . . ➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕ Outliers successfully replaced with extreme limits ➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕ . num_boxplot(&#39;water&#39;, data) . . 📈📈 Boxplot of &#39;water&#39; 📈📈 . ✔✔ 👍🏻👍🏻 NO Outliers 👍🏻👍🏻 ✔✔ . Parameters Values . 1 Skewness | -0.56 | . 2 Kurtosis | -0.42 | . 3 Median | 185.70 | . 4 Count | 1005.00 | . 5 Mean | 182.84 | . 6 Stand. Dev. | 12.21 | . 7 Minimum | 155.70 | . 8 Q1 (25%) | 175.10 | . 9 Q2 (50%) | 185.70 | . 10 Q3 (75%) | 192.00 | . 11 Maximum | 203.50 | . 12 IQR | 16.90 | . 13 No.of observ. w/o outliers | 1005 (100.0%) | . Component : superplastic . #calculating the boundaries which differentiate the outliers Q1 = data.superplastic.quantile(0.25) Q3 = data.superplastic.quantile(0.75) IQR = Q3 - Q1 lower_bridge = Q1 - (IQR*1.5) lower_bridge = lower_bridge.round(decimals=2) upper_bridge = Q3 + (IQR*1.5) upper_bridge = upper_bridge.round(decimals=2) print(&#39; nLower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;lower boundary limit : {} nupper boundary limit : {}&quot;.format(lower_bridge,upper_bridge)) . . Lower &amp; Upper Limits 👇🏻👇🏻👇🏻 lower boundary limit : -15.0 upper boundary limit : 25.0 . #replace outliers with median data.loc[(data[&#39;superplastic&#39;] &gt; upper_bridge), &#39;superplastic&#39;] = data[&#39;superplastic&#39;].median() print(emoji.emojize(&quot;:plus:&quot;)*14 ,&quot; n nOutliers successfully replaced n n&quot; ,emoji.emojize(&quot;:plus:&quot;)*14) . . ➕➕➕➕➕➕➕➕➕➕➕➕➕➕ Outliers successfully replaced ➕➕➕➕➕➕➕➕➕➕➕➕➕➕ . num_boxplot(&#39;superplastic&#39;, data) . . 📈📈 Boxplot of &#39;superplastic&#39; 📈📈 . ✔✔ 👍🏻👍🏻 NO Outliers 👍🏻👍🏻 ✔✔ . Parameters Values . 1 Skewness | 0.51 | . 2 Kurtosis | -0.31 | . 3 Median | 6.10 | . 4 Count | 1005.00 | . 5 Mean | 5.79 | . 6 Stand. Dev. | 5.40 | . 7 Minimum | 0.00 | . 8 Q1 (25%) | 0.00 | . 9 Q2 (50%) | 6.10 | . 10 Q3 (75%) | 9.90 | . 11 Maximum | 23.40 | . 12 IQR | 9.90 | . 13 No.of observ. w/o outliers | 1005 (100.0%) | . Component : age . #calculating the boundaries which differentiate the outliers Q1 = data.age.quantile(0.25) Q3 = data.age.quantile(0.75) IQR = Q3 - Q1 lower_bridge = Q1 - (IQR*1.5) lower_bridge = lower_bridge.round(decimals=2) upper_bridge = Q3 + (IQR*1.5) upper_bridge = upper_bridge.round(decimals=2) print(&#39; nLower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;lower boundary limit : {} nupper boundary limit : {}&quot;.format(lower_bridge,upper_bridge)) . . Lower &amp; Upper Limits 👇🏻👇🏻👇🏻 lower boundary limit : -66.5 upper boundary limit : 129.5 . # Extreme outliers ext_lower_bridge = data.age.quantile(0.20) ext_upper_bridge = data.age.quantile(0.80) print(&#39; n Extreme Lower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; t&quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;Extreme lower boundary limit : {} nExtreme upper boundary limit : {}&quot;.format(ext_lower_bridge,ext_upper_bridge)) . . Extreme Lower &amp; Upper Limits 👇🏻👇🏻👇🏻 Extreme lower boundary limit : 7.0 Extreme upper boundary limit : 56.0 . #replace outliers with median data.loc[(data[&#39;age&#39;] &gt; ext_upper_bridge), &#39;age&#39;] = data[&#39;age&#39;].median() print(emoji.emojize(&quot;:plus:&quot;)*18 ,&quot; n nOutliers successfully replaced with extreme limits n n&quot; ,emoji.emojize(&quot;:plus:&quot;)*18) . . ➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕ Outliers successfully replaced with extreme limits ➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕ . num_boxplot(&#39;age&#39;, data) . . 📈📈 Boxplot of &#39;age&#39; 📈📈 . ✔✔ 👍🏻👍🏻 NO Outliers 👍🏻👍🏻 ✔✔ . Parameters Values . 1 Skewness | 0.41 | . 2 Kurtosis | 0.32 | . 3 Median | 28.00 | . 4 Count | 1005.00 | . 5 Mean | 23.72 | . 6 Stand. Dev. | 14.01 | . 7 Minimum | 1.00 | . 8 Q1 (25%) | 7.00 | . 9 Q2 (50%) | 28.00 | . 10 Q3 (75%) | 28.00 | . 11 Maximum | 56.00 | . 12 IQR | 21.00 | . 13 No.of observ. w/o outliers | 1005 (100.0%) | . Splitting the data into train &amp; test . # splitting the data into independent &amp; dependent attributes X = data.drop(&#39;strength&#39;, axis=1) y = data[&#39;strength&#39;] X = pd.DataFrame(X) y = pd.DataFrame(y) # Splitting into train &amp; test data X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 54) print(emojis.encode(&quot;:scissors:&quot;)*22 ,&quot; n nData divided into Dependent &amp; Independent Variables nand Split into Train &amp; Test data n n&quot; ,emojis.encode(&quot;:scissors:&quot;)*22) . . ✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️ Data divided into Dependent &amp; Independent Variables and Split into Train &amp; Test data ✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️ . Feature Scaling . # Standardization in the data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) print(emoji.emojize(&quot;:triangular_flag:&quot;)*18 ,&quot; n nSuccessfully accomplished feature scaling n n&quot; ,emoji.emojize(&quot;:triangular_flag:&quot;)*18) . . 🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩 Successfully accomplished feature scaling 🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩 . #convert array into dataframe X_train = pd.DataFrame(X_train) X_test = pd.DataFrame(X_test) y_train = pd.DataFrame(y_train) y_test = pd.DataFrame(y_test) print(emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*25 ,&quot; n nX_train,X_test,y_train &amp; y_test converted into dataframe n n&quot; ,emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*25) . . 👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻 X_train,X_test,y_train &amp; y_test converted into dataframe 👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻 . Building Different Models . def regression_model(y_test,y_pred): &quot;&quot;&quot; Takes y_test &amp; y_pred of a model as input and return styler object of RMSE, MSE, MAE &amp; R3 Score for that particular model. &quot;&quot;&quot; rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred)) mse = metrics.mean_squared_error(y_test, y_pred) mae = metrics.mean_absolute_error(y_test, y_pred) r2score = metrics.r2_score(y_test, y_pred) v1 = pd.DataFrame({&#39;Metrics&#39;: &#39;RMSE&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(rmse)},index={&#39;1&#39;}) v2 = pd.DataFrame({&#39;Metrics&#39;: &#39;MSE&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(mse)},index={&#39;2&#39;}) v3 = pd.DataFrame({&#39;Metrics&#39;: &#39;MAE&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(mae)},index={&#39;3&#39;}) v4 = pd.DataFrame({&#39;Metrics&#39;: &#39;R2 Score&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(r2score)},index={&#39;4&#39;}) model_results = pd.concat([v1,v2,v3,v4]) model_results.columns = [&#39;Metrics&#39;,&#39;Values&#39;] model_results = model_results.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#34495E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).format({&#39;RMSE&#39; : &quot;{:.2f}&quot;,&#39;MSE&#39; : &quot;{:.2f}&quot;,&#39;MAE&#39; : &quot;{:.2f}&quot;,&#39;R2 Score&#39; : &quot;{:.2f}&quot;}) .set_properties(**{&#39;background-color&#39;: &#39;#FEF5E7&#39;}, subset=[&#39;Metrics&#39;]) return model_results . . def pred_plot(y_pred,title): &quot;&quot;&quot; Takes y_pred &amp; title as input and plots the prediction curve. &quot;&quot;&quot; print(&quot; n&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&quot;Prediction Plot&quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) plt.scatter(y_test, y_pred) plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], &#39;r--&#39;, lw=2) plt.xlabel(&quot;Predicted&quot;) plt.title(title) plt.ylabel(&quot;True&quot;) plt.show() . . def kf_score(k_result,name): &quot;&quot;&quot; Takes k_result &amp; name of the model as input and returns styler object of mean and standard deviation of accuracy of that particular model. &quot;&quot;&quot; acc = np.mean(abs(k_result)) result_kf = pd.DataFrame({&#39;Algorithm&#39;:[name] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(acc,np.std(abs(k_result)))} ,index={emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;)}) result_kf_final = result_kf[[&#39;Algorithm&#39;,&#39;Accuracy&#39;]] result_kf_final = result_kf_final.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A60B2E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#F2DBE0&#39;}, subset=[&#39;Algorithm&#39;]) return result_kf_final . . 1. Linear Regression . model1 = LinearRegression() model1.fit(X_train, y_train) y_pred_linear = model1.predict(X_test) # model performance on training data model1.score(X_train, y_train) # model performance on testing data model1.score(X_test, y_test) regression_model(y_test,y_pred_linear) . . Metrics Values . 1 RMSE | 8.54 | . 2 MSE | 72.87 | . 3 MAE | 6.43 | . 4 R2 Score | 0.74 | . Plotting Prediction . pred_plot(y_pred_linear,&quot;Linear Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Linear Regression with K-Fold Cross Validation . kfold = KFold(n_splits=10, random_state=70) k_result_linear = cross_val_score(model1,X,y,cv=kfold) kf_score(k_result_linear,&quot;Linear Regression (k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Linear Regression (k_fold) | 0.56(0.16) | . 2. Lasso Regression . model2 = Lasso() model2.fit(X_train, y_train) y_pred_lasso = model2.predict(X_test) # model performance on training data model2.score(X_train, y_train).round(decimals=2) # model performance on testing data model2.score(X_test, y_test).round(decimals=2) regression_model(y_test,y_pred_lasso) . . Metrics Values . 1 RMSE | 9.05 | . 2 MSE | 81.93 | . 3 MAE | 7.04 | . 4 R2 Score | 0.71 | . Plotting Prediction . pred_plot(y_pred_lasso,&quot;Lasso Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Lasso Regression with K-Fold Cross Validation . kfold = KFold(n_splits=10, random_state=70) k_result_lasso = cross_val_score(model2,X,y,cv=kfold) kf_score(k_result_lasso, &quot;Lasso Regression (k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Lasso Regression (k_fold) | 0.55(0.16) | . 3. Ridge Regression . model3 = Ridge() model3.fit(X_train, y_train) y_pred_ridge = model3.predict(X_test) # model performance on training data model3.score(X_train, y_train).round(decimals=2) # model performance on testing data model3.score(X_test, y_test).round(decimals=2) regression_model(y_test,y_pred_ridge) . . Metrics Values . 1 RMSE | 8.54 | . 2 MSE | 72.86 | . 3 MAE | 6.43 | . 4 R2 Score | 0.74 | . Plotting Prediction . pred_plot(y_pred_ridge,&quot;Ridge Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Ridge Regression with K-Fold Cross Validation . kfold = KFold(n_splits=10, random_state=70) k_result_ridge = cross_val_score(model3,X,y,cv=kfold) kf_score(k_result_ridge,&quot;Ridge Regression (k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Ridge Regression (k_fold) | 0.56(0.16) | . 4. Random Forest . model4 = RandomForestRegressor() model4.fit(X_train, y_train) y_pred_rf = model1.predict(X_test) # model performance on training data model4.score(X_train, y_train).round(decimals=2) # model performance on testing data model4.score(X_test, y_test).round(decimals=2) regression_model(y_test,y_pred_rf) . . Metrics Values . 1 RMSE | 8.54 | . 2 MSE | 72.87 | . 3 MAE | 6.43 | . 4 R2 Score | 0.74 | . Plotting Prediction . pred_plot(y_pred_rf,&quot;Random Forest Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Random Forest Regression with K-Fold Cross Validation . kfold = KFold(n_splits=10, random_state=70) k_result_rf = cross_val_score(model4,X,y,cv=kfold) kf_score(k_result_rf,&quot;Random Forest Regressor (k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Random Forest Regressor (k_fold) | 0.64(0.19) | . 5. Gradient Boosting Regressor . model5 = GradientBoostingRegressor() model5.fit(X_train, y_train) y_pred_gbr = model5.predict(X_test) # model performance on training data model5.score(X_train, y_train).round(decimals=2) # model performance on testing data model5.score(X_test, y_test).round(decimals=2) regression_model(y_test,y_pred_gbr) . . Metrics Values . 1 RMSE | 6.53 | . 2 MSE | 42.62 | . 3 MAE | 5.02 | . 4 R2 Score | 0.85 | . Plotting Prediction . pred_plot(y_pred_gbr,&quot;Gradient Boosting Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Gradient Boosting with K-Fold Cross Validation . kfold = KFold(n_splits=10, random_state=70) k_result_gbr = cross_val_score(model5,X,y,cv=kfold) kf_score(k_result_gbr,&quot;Gradient Boosting Regressor (k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Gradient Boosting Regressor (k_fold) | 0.62(0.17) | . 6. Ada Boosting Regressor . model6 = AdaBoostRegressor() model6.fit(X_train, y_train) y_pred_ada = model6.predict(X_test) # model performance on training data model6.score(X_train, y_train).round(decimals=2) # model performance on testing data model6.score(X_test, y_test).round(decimals=2) regression_model(y_test,y_pred_ada) . . Metrics Values . 1 RMSE | 8.84 | . 2 MSE | 78.06 | . 3 MAE | 7.25 | . 4 R2 Score | 0.72 | . Plotting Prediction . pred_plot(y_pred_ada,&quot;Ada Boosting Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Ada Boosting with K-Fold Cross Validation . kfold = KFold(n_splits=10, random_state=70) k_result_ada = cross_val_score(model6,X,y,cv=kfold) kf_score(k_result_ada,&quot;Ada Boosting Regressor (k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Ada Boosting Regressor (k_fold) | 0.43(0.15) | . 7. KNN Regressor . #checking for different values of neighbors to determine diff_k=[] for i in range(1,45): knn = KNeighborsRegressor(n_neighbors=i) knn.fit(X_train,y_train) pred_i = knn.predict(X_test) diff_k.append(np.mean(pred_i != y_test)) . . plt.figure(figsize=(12,6)) plt.plot(range(1,45), diff_k,color=&#39;white&#39;, linestyle=&#39;dashed&#39;,marker=&#39;o&#39;,markerfacecolor=&#39;red&#39;,markersize=10) plt.title(&#39;Different K Values&#39;) plt.xlabel(&#39;K Values&#39;) plt.ylabel(&#39;Mean Error&#39;) . . Text(0, 0.5, &#39;Mean Error&#39;) . model7 = KNeighborsRegressor(n_neighbors=3) model7.fit(X_train,y_train) y_pred_knn = model7.predict(X_test) # model performance on training data model7.score(X_train, y_train).round(decimals=2) # model performance on testing data model7.score(X_test, y_test).round(decimals=2) regression_model(y_test,y_pred_knn) . . Metrics Values . 1 RMSE | 7.86 | . 2 MSE | 61.71 | . 3 MAE | 5.88 | . 4 R2 Score | 0.78 | . Plotting Prediction . pred_plot(y_pred_knn,&quot;KNN Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . KNN Regressor with K-Fold Cross Validation . kfold = KFold(n_splits=10, random_state=70) k_result_knn = cross_val_score(model7,X,y,cv=kfold) kf_score(k_result_knn,&quot;K-NN Regressor (k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 K-NN Regressor (k_fold) | 0.42(0.37) | . 8. Bagging Regressor . model8 = BaggingRegressor() model8.fit(X_train, y_train) y_pred_bag = model8.predict(X_test) # model performance on training data model8.score(X_train, y_train).round(decimals=2) # model performance on testing data model8.score(X_test, y_test).round(decimals=2) regression_model(y_test,y_pred_bag) . . Metrics Values . 1 RMSE | 6.45 | . 2 MSE | 41.58 | . 3 MAE | 4.85 | . 4 R2 Score | 0.85 | . Plotting Prediction . pred_plot(y_pred_bag,&quot;Bagging Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Bagging Regressor with K-Fold Cross Validation . kfold = KFold(n_splits=10, random_state=70) k_result_bag = cross_val_score(model8,X,y,cv=kfold) kf_score(k_result_bag,&quot;Bagging Regressor (k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Bagging Regressor (k_fold) | 0.60(0.22) | . 9. Support Vector Regressor . model9 = SVR(kernel=&#39;linear&#39;) model9.fit(X_train, y_train) y_pred_svr = model9.predict(X_test) # model performance on training data model9.score(X_train, y_train).round(decimals=2) # model performance on testing data model9.score(X_test, y_test).round(decimals=2) regression_model(y_test,y_pred_svr) . . Metrics Values . 1 RMSE | 8.56 | . 2 MSE | 73.36 | . 3 MAE | 6.28 | . 4 R2 Score | 0.74 | . Plotting Prediction . pred_plot(y_pred_svr,&quot;Support Vector Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Support Vector Regressor with K-Fold Cross Validation . kfold = KFold(n_splits=10, random_state=70) k_result_svr = cross_val_score(model9,X,y,cv=kfold) kf_score(k_result_svr,&quot;Support Vector Regressor (k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Support Vector Regressor (k_fold) | 0.56(0.17) | . 10. Decision Tree Regression . model11 = DecisionTreeRegressor() model11.fit(X_train, y_train) # printing the feature importance print(&#39;Feature importances : n&#39;,pd.DataFrame(model11.feature_importances_,columns=[&#39;Importance&#39;],index=X_train.columns)) print(&quot;&quot;) y_pred_dtr = model11.predict(X_test) # model performance on training data model11.score(X_train, y_train).round(decimals=2) # model performance on testing data model11.score(X_test, y_test).round(decimals=2) regression_model(y_test,y_pred_dtr) . . Feature importances : Importance 0 0.371581 1 0.092883 2 0.006783 3 0.041728 4 0.105538 5 0.047598 6 0.037988 7 0.295900 . Metrics Values . 1 RMSE | 7.28 | . 2 MSE | 53.01 | . 3 MAE | 5.09 | . 4 R2 Score | 0.81 | . Plotting Prediction . pred_plot(y_pred_dtr,&quot;Decision Tree Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Decision Tree Regression with K-Fold Cross Validation . kfold = KFold(n_splits=10, random_state=70) k_result_dtr = cross_val_score(model11,X,y,cv=kfold) kf_score(k_result_dtr,&quot;Decision Tree Regressor (k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Decision Tree Regressor (k_fold) | 0.43(0.32) | . Feature Selection . data2= data.copy() X = data2.drop([&#39;strength&#39;,&#39;ash&#39;,&#39;coarse_agg&#39;,&#39;fine_agg&#39;], axis=1) y = data2[&#39;strength&#39;] X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 54) print(emoji.emojize(&quot;:bookmark_tabs:&quot;)*24 ,&quot; n nSome of the features are selected as feature selection. n n&quot; ,emoji.emojize(&quot;:bookmark_tabs:&quot;)*24) . . 📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑 Some of the features are selected as feature selection. 📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑 . Feature Scaling after feature selection . # Standardization in the data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) print(emoji.emojize(&quot;:triangular_flag:&quot;)*28 ,&quot; n nSuccessfully accomplished feature scaling after feature selection n n&quot; ,emoji.emojize(&quot;:triangular_flag:&quot;)*28) . . 🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩 Successfully accomplished feature scaling after feature selection 🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩 . #convert array into dataframe X_train = pd.DataFrame(X_train) X_test = pd.DataFrame(X_test) y_train = pd.DataFrame(y_train) y_test = pd.DataFrame(y_test) print(emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*25 ,&quot; n nX_train,X_test,y_train &amp; y_test converted into dataframe n n&quot; ,emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*25) . . 👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻 X_train,X_test,y_train &amp; y_test converted into dataframe 👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻 . def repeated_kf_score(new_k_result,name): &quot;&quot;&quot; Takes repeated k_result &amp; name of the model as input and returns styler object of mean and standard deviation of accuracy of that particular model. &quot;&quot;&quot; acc = np.mean(abs(new_k_result)) new_result_kf = pd.DataFrame({&#39;Algorithm&#39;:[name] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(acc,np.std(abs(new_k_result)))} ,index={emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;)}) new_result_kf_final = new_result_kf[[&#39;Algorithm&#39;,&#39;Accuracy&#39;]] new_result_kf_final = new_result_kf_final.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#035753&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#CAEEE5&#39;}, subset=[&#39;Algorithm&#39;]) return new_result_kf_final . . 1. Linear Regression After Feature Selection . new_model1 = LinearRegression() new_model1.fit(X_train, y_train) new_y_pred_linear = new_model1.predict(X_test) # new_model performance on training data new_model1.score(X_train, y_train).round(decimals=2) # new_model performance on testing data new_model1.score(X_test, y_test).round(decimals=2) regression_model(y_test,new_y_pred_linear) . . Metrics Values . 1 RMSE | 8.71 | . 2 MSE | 75.93 | . 3 MAE | 6.70 | . 4 R2 Score | 0.73 | . Plotting Prediction . pred_plot(new_y_pred_linear,&quot;Linear Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Linear Regression with Repeated K-Fold Cross Validation After Feature Selection . kfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) new_k_result_linear = cross_val_score(new_model1,X,y,cv=kfold) repeated_kf_score(new_k_result_linear,&quot;Linear Regression (Repeated k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Linear Regression (Repeated k_fold) | 0.68(0.06) | . 2. Lasso Regression After Feature Selection . new_model2 = Lasso() new_model2.fit(X_train, y_train) new_y_pred_lasso = new_model2.predict(X_test) # new_model performance on training data new_model2.score(X_train, y_train).round(decimals=2) # new_model performance on testing data new_model2.score(X_test, y_test).round(decimals=2) regression_model(y_test,new_y_pred_lasso) . . Metrics Values . 1 RMSE | 9.05 | . 2 MSE | 81.93 | . 3 MAE | 7.04 | . 4 R2 Score | 0.71 | . Plotting Prediction . pred_plot(new_y_pred_lasso,&quot;Lasso Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Lasso Regression with Repeated K-Fold Cross Validation After Feature Selection . kfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) new_k_result_lasso = cross_val_score(new_model2,X,y,cv=kfold) repeated_kf_score(new_k_result_lasso,&quot;Lasso Regression (Repeated k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Lasso Regression (Repeated k_fold) | 0.68(0.05) | . 3. Ridge Regression After Feature Selection . new_model3 = Ridge() new_model3.fit(X_train, y_train) new_y_pred_ridge = new_model3.predict(X_test) # new_model performance on training data new_model3.score(X_train, y_train).round(decimals=2) # new_model performance on testing data new_model3.score(X_test, y_test).round(decimals=2) regression_model(y_test,new_y_pred_ridge) . . Metrics Values . 1 RMSE | 8.71 | . 2 MSE | 75.92 | . 3 MAE | 6.70 | . 4 R2 Score | 0.73 | . Plotting Prediction . pred_plot(new_y_pred_ridge,&quot;Ridge Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Ridge Regression with Repeated K-Fold Cross Validation After Feature Selection . kfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) new_k_result_ridge = cross_val_score(new_model3,X,y,cv=kfold) repeated_kf_score(new_k_result_ridge,&quot;Ridge Regression (Repeated k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Ridge Regression (Repeated k_fold) | 0.68(0.06) | . 4. Random Forest After Feature Selection . new_model4 = RandomForestRegressor() new_model4.fit(X_train, y_train) new_y_pred_rf = new_model1.predict(X_test) # new_model performance on training data new_model4.score(X_train, y_train).round(decimals=2) # new_model performance on testing data new_model4.score(X_test, y_test).round(decimals=2) regression_model(y_test,new_y_pred_rf) . . Metrics Values . 1 RMSE | 8.71 | . 2 MSE | 75.93 | . 3 MAE | 6.70 | . 4 R2 Score | 0.73 | . Plotting Prediction . pred_plot(new_y_pred_rf,&quot;Random Forest Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Random Forest Regression with Repeated K-Fold Cross Validation After Feature Selection . kfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) new_k_result_rf = cross_val_score(new_model4,X,y,cv=kfold) repeated_kf_score(new_k_result_rf,&quot;Random Forest Regression (Repeated k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Random Forest Regression (Repeated k_fold) | 0.85(0.04) | . 5. Gradient Boosting Regressor After Feature Selection . new_model5 = GradientBoostingRegressor() new_model5.fit(X_train, y_train) new_y_pred_gbr = new_model5.predict(X_test) # new_model performance on training data new_model5.score(X_train, y_train).round(decimals=2) # new_model performance on testing data new_model5.score(X_test, y_test).round(decimals=2) regression_model(y_test,new_y_pred_gbr) . . Metrics Values . 1 RMSE | 6.93 | . 2 MSE | 48.05 | . 3 MAE | 5.26 | . 4 R2 Score | 0.83 | . Plotting Prediction . pred_plot(new_y_pred_gbr,&quot;Gradient Boosting Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Gradient Boosting with Repeated K-Fold Cross Validation After Feature Selection . kfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) new_k_result_gbr = cross_val_score(new_model5,X,y,cv=kfold) repeated_kf_score(new_k_result_gbr,&quot;Gradient Boosting Regressor (Repeated k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Gradient Boosting Regressor (Repeated k_fold) | 0.82(0.04) | . 6. Ada Boosting Regressor After Feature Selection . new_model6 = AdaBoostRegressor() new_model6.fit(X_train, y_train) new_y_pred_ada = new_model6.predict(X_test) # new_model performance on training data new_model6.score(X_train, y_train).round(decimals=2) # new_model performance on testing data new_model6.score(X_test, y_test).round(decimals=2) regression_model(y_test,new_y_pred_ada) . . Metrics Values . 1 RMSE | 8.67 | . 2 MSE | 75.22 | . 3 MAE | 7.23 | . 4 R2 Score | 0.73 | . Plotting Prediction . pred_plot(y_pred_linear,&quot;Ada Boosting Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Ada Boosting with Repeated K-Fold Cross Validation After Feature Selection . kfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) new_k_result_ada = cross_val_score(new_model6,X,y,cv=kfold) repeated_kf_score(new_k_result_ada,&quot;Ada Boosting Regressor (Repeated k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Ada Boosting Regressor (Repeated k_fold) | 0.69(0.06) | . 7. KNN Regressor After Feature Selection . #checking for different values of neighbors to determine diff_k=[] for i in range(1,45): knn = KNeighborsRegressor(n_neighbors=i) knn.fit(X_train,y_train) pred_i = knn.predict(X_test) diff_k.append(np.mean(pred_i != y_test)) . . plt.figure(figsize=(12,6)) plt.plot(range(1,45), diff_k,color=&#39;white&#39;, linestyle=&#39;dashed&#39;,marker=&#39;o&#39;,markerfacecolor=&#39;red&#39;,markersize=10) plt.title(&#39;Different K Values&#39;) plt.xlabel(&#39;K Values&#39;) plt.ylabel(&#39;Mean Error&#39;) . . Text(0, 0.5, &#39;Mean Error&#39;) . new_model7 = KNeighborsRegressor(n_neighbors=3) new_model7.fit(X_train,y_train) new_y_pred_knn = new_model7.predict(X_test) # new_model performance on training data new_model7.score(X_train, y_train).round(decimals=2) # new_model performance on testing data new_model7.score(X_test, y_test).round(decimals=2) regression_model(y_test,new_y_pred_knn) . . Metrics Values . 1 RMSE | 7.21 | . 2 MSE | 51.95 | . 3 MAE | 5.60 | . 4 R2 Score | 0.81 | . Plotting Prediction . pred_plot(new_y_pred_knn,&quot;K-NN Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . KNN Regressor with Repeated K-Fold Cross Validation After Feature Selection . kfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) new_k_result_knn = cross_val_score(new_model7,X,y,cv=kfold) repeated_kf_score(new_k_result_knn,&quot;K-NN Regression (Repeated k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 K-NN Regression (Repeated k_fold) | 0.72(0.06) | . 8. Bagging Regressor After Feature Selection . new_model8 = BaggingRegressor() new_model8.fit(X_train, y_train) new_y_pred_bag = new_model8.predict(X_test) # new_model performance on training data new_model8.score(X_train, y_train).round(decimals=2) # new_model performance on testing data new_model8.score(X_test, y_test).round(decimals=2) regression_model(y_test,new_y_pred_bag) . . Metrics Values . 1 RMSE | 6.40 | . 2 MSE | 40.90 | . 3 MAE | 4.90 | . 4 R2 Score | 0.85 | . Plotting Prediction . pred_plot(new_y_pred_bag,&quot;Bagging Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Bagging Regressor with Repeated K-Fold Cross Validation After Feature Selection . kfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) new_k_result_bag = cross_val_score(new_model8,X,y,cv=kfold) repeated_kf_score(new_k_result_bag,&quot;Bagging Regressor (Repeated k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Bagging Regressor (Repeated k_fold) | 0.84(0.04) | . 9. Support Vector Regressor After Feature Selection . new_model9 = SVR(kernel=&#39;linear&#39;) new_model9.fit(X_train, y_train) new_y_pred_svr = new_model9.predict(X_test) # new_model performance on training data new_model9.score(X_train, y_train).round(decimals=2) # new_model performance on testing data new_model9.score(X_test, y_test).round(decimals=2) regression_model(y_test,new_y_pred_svr) . . Metrics Values . 1 RMSE | 8.79 | . 2 MSE | 77.22 | . 3 MAE | 6.51 | . 4 R2 Score | 0.72 | . Plotting Prediction . pred_plot(new_y_pred_svr,&quot;Support Vector Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Support Vector Regressor with Repeated K-Fold Cross Validation After Feature Selection . kfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) new_k_result_svr = cross_val_score(new_model9,X,y,cv=kfold) repeated_kf_score(new_k_result_svr,&quot;Support Vector Regressor (Repeated k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Support Vector Regressor (Repeated k_fold) | 0.67(0.06) | . 10. Decision Tree Regression After Feature Selection . new_model11 = DecisionTreeRegressor() new_model11.fit(X_train, y_train) # printing the feature importance print(&#39;Feature importances : n&#39;,pd.DataFrame(new_model11.feature_importances_,columns=[&#39;Importance&#39;],index=X_train.columns)) print(&quot;&quot;) new_y_pred_dtr = new_model11.predict(X_test) # model performance on training data new_model11.score(X_train, y_train).round(decimals=2) # model performance on testing data new_model11.score(X_test, y_test).round(decimals=2) regression_model(y_test,new_y_pred_dtr) . . Feature importances : Importance 0 0.406734 1 0.114740 2 0.061161 3 0.119792 4 0.297573 . Metrics Values . 1 RMSE | 7.63 | . 2 MSE | 58.16 | . 3 MAE | 5.28 | . 4 R2 Score | 0.79 | . Plotting Prediction . pred_plot(new_y_pred_dtr,&quot;Decision Tree Regressor&quot;) . . 📈📈 Prediction Plot 📈📈 . Decision Tree Regressor with Repeated K-Fold Cross Validation After Feature Selection . kfold = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) new_k_result_dtr = cross_val_score(new_model11,X,y,cv=kfold) repeated_kf_score(new_k_result_dtr,&quot;Decision Tree Regressor (Repeated k_fold)&quot;) . . Algorithm Accuracy . 👉🏻 Decision Tree Regressor (Repeated k_fold) | 0.79(0.05) | . Conclusion . # store the accuracy results for each model in a dataframe for final comparison result_linear = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Linear Regression &#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred_linear))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, y_pred_linear)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, y_pred_linear)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, y_pred_linear))} , index={&#39;1&#39;}) result_lasso = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Lasso Regression&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred_lasso))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, y_pred_lasso)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, y_pred_lasso)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, y_pred_lasso))} , index={&#39;2&#39;}) result_ridge = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Ridge Regression&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred_ridge))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, y_pred_ridge)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, y_pred_ridge)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, y_pred_ridge))} , index={&#39;3&#39;}) result_rf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Random Forest Regression&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred_rf))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, y_pred_rf)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, y_pred_rf)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, y_pred_rf))} , index={&#39;4&#39;}) result_gbr = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Gradient Boosting Regressor&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred_gbr))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, y_pred_gbr)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, y_pred_gbr)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, y_pred_gbr))} , index={&#39;5&#39;}) result_ada = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Ada Boosting Regressor&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred_ada))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, y_pred_ada)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, y_pred_ada)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, y_pred_ada))} , index={&#39;6&#39;}) result_knn = pd.DataFrame({&#39;Algorithm&#39;:[&#39;KNN Regressor&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred_knn))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, y_pred_knn)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, y_pred_knn)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, y_pred_knn))} , index={&#39;7&#39;}) result_bag = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Bagging Regressor&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred_bag))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, y_pred_bag)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, y_pred_bag)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, y_pred_bag))} , index={&#39;8&#39;}) result_svr = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Support Vector Regressor&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred_svr))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, y_pred_svr)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, y_pred_svr)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, y_pred_svr))} , index={&#39;9&#39;}) result_dtr = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Decision Tree Regression&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred_dtr))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, y_pred_dtr)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, y_pred_dtr)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, y_pred_dtr))} , index={&#39;10&#39;}) model_results = pd.concat([result_linear,result_lasso,result_ridge,result_rf,result_gbr, result_ada,result_knn,result_bag,result_svr,result_dtr]) model_results = model_results[[&#39;Algorithm&#39;,&#39;RMSE&#39;,&#39;MSE&#39;,&#39;MAE&#39;,&#39;R2 SCORE&#39;]] print(&quot;Performance of models before feature selection&quot;) model_results = model_results.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#236665&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#9AC6CA&#39;}, subset=[&#39;Algorithm&#39;]) model_results . . Performance of models before feature selection . Algorithm RMSE MSE MAE R2 SCORE . 1 Linear Regression | 8.54 | 72.87 | 6.43 | 0.74 | . 2 Lasso Regression | 9.05 | 81.93 | 7.04 | 0.71 | . 3 Ridge Regression | 8.54 | 72.86 | 6.43 | 0.74 | . 4 Random Forest Regression | 8.54 | 72.87 | 6.43 | 0.74 | . 5 Gradient Boosting Regressor | 6.53 | 42.62 | 5.02 | 0.85 | . 6 Ada Boosting Regressor | 8.84 | 78.06 | 7.25 | 0.72 | . 7 KNN Regressor | 7.86 | 61.71 | 5.88 | 0.78 | . 8 Bagging Regressor | 6.45 | 41.58 | 4.85 | 0.85 | . 9 Support Vector Regressor | 8.56 | 73.36 | 6.28 | 0.74 | . 10 Decision Tree Regression | 7.28 | 53.01 | 5.09 | 0.81 | . result_linear_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Linear Regression (k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(k_result_linear)),np.std(abs(k_result_linear)))} , index={&#39;1&#39;}) result_lasso_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Lasso Regression (k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(k_result_lasso)),np.std(abs(k_result_lasso)))} , index={&#39;2&#39;}) result_ridge_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Ridge Regression (k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(k_result_ridge)),np.std(abs(k_result_ridge)))} , index={&#39;3&#39;}) result_rf_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Random Forest Regression (k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(k_result_rf)),np.std(abs(k_result_rf)))} , index={&#39;4&#39;}) result_gbr_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Gradient Boosting Regressor (k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(k_result_gbr)),np.std(abs(k_result_gbr)))} , index={&#39;5&#39;}) result_ada_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Ada Boosting Regressor (k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(k_result_ada)),np.std(abs(k_result_ada)))} , index={&#39;6&#39;}) result_knn_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;KNN Regressor (k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(k_result_knn)),np.std(abs(k_result_knn)))} , index={&#39;7&#39;}) result_bag_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Bagging Regressor (k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(k_result_bag)),np.std(abs(k_result_bag)))} , index={&#39;8&#39;}) result_svr_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Support Vector Regressor (k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(k_result_svr)),np.std(abs(k_result_svr)))} , index={&#39;9&#39;}) result_dtr_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Decision Tree Regression (k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(k_result_dtr)),np.std(abs(k_result_dtr)))} , index={&#39;10&#39;}) kf_results = pd.concat([result_linear_kf,result_lasso_kf,result_ridge_kf,result_rf_kf, result_gbr_kf,result_ada_kf,result_knn_kf,result_bag_kf,result_svr_kf,result_dtr_kf]) kf_results = kf_results[[&#39;Algorithm&#39;,&#39;Accuracy&#39;]] print(&quot; K-Fold Accuracy before feature selection for k=10 &quot;) kf_results = kf_results.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#233A66&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#A1ADC4&#39;}, subset=[&#39;Algorithm&#39;]) kf_results . . K-Fold Accuracy before feature selection for k=10 . Algorithm Accuracy . 1 Linear Regression (k_fold) | 0.56(0.16) | . 2 Lasso Regression (k_fold) | 0.55(0.16) | . 3 Ridge Regression (k_fold) | 0.56(0.16) | . 4 Random Forest Regression (k_fold) | 0.64(0.19) | . 5 Gradient Boosting Regressor (k_fold) | 0.62(0.17) | . 6 Ada Boosting Regressor (k_fold) | 0.43(0.15) | . 7 KNN Regressor (k_fold) | 0.42(0.37) | . 8 Bagging Regressor (k_fold) | 0.60(0.22) | . 9 Support Vector Regressor (k_fold) | 0.56(0.17) | . 10 Decision Tree Regression (k_fold) | 0.43(0.32) | . # store the accuracy results for each model in a dataframe for final comparison new_result_linear = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Linear Regression &#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, new_y_pred_linear))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, new_y_pred_linear)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, new_y_pred_linear)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, new_y_pred_linear))} , index={&#39;1&#39;}) new_result_lasso = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Lasso Regression&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, new_y_pred_lasso))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, new_y_pred_lasso)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, new_y_pred_lasso)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, new_y_pred_lasso))} , index={&#39;2&#39;}) new_result_ridge = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Ridge Regression&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, new_y_pred_ridge))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, new_y_pred_ridge)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, new_y_pred_ridge)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, new_y_pred_ridge))} , index={&#39;3&#39;}) new_result_rf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Random Forest Regression&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, new_y_pred_rf))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, new_y_pred_rf)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, new_y_pred_rf)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, new_y_pred_rf))} , index={&#39;4&#39;}) new_result_gbr = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Gradient Boosting Regressor&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, new_y_pred_gbr))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, new_y_pred_gbr)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, new_y_pred_gbr)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, new_y_pred_gbr))} , index={&#39;5&#39;}) new_result_ada = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Ada Boosting Regressor&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, new_y_pred_ada))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, new_y_pred_ada)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, new_y_pred_ada)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, new_y_pred_ada))} , index={&#39;6&#39;}) new_result_knn = pd.DataFrame({&#39;Algorithm&#39;:[&#39;KNN Regressor&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, new_y_pred_knn))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, new_y_pred_knn)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, new_y_pred_knn)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, new_y_pred_knn))} , index={&#39;7&#39;}) new_result_bag = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Bagging Regressor&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, new_y_pred_bag))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, new_y_pred_bag)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, new_y_pred_bag)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, new_y_pred_bag))} , index={&#39;8&#39;}) new_result_svr = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Support Vector Regressor&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, new_y_pred_svr))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, new_y_pred_svr)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, new_y_pred_svr)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, new_y_pred_svr))} , index={&#39;9&#39;}) new_result_dtr = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Decision Tree Regression&#39;] ,&#39;RMSE&#39;: &#39;{:.2f}&#39;.format(np.sqrt(metrics.mean_squared_error(y_test, new_y_pred_dtr))) ,&#39;MSE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_squared_error(y_test, new_y_pred_dtr)) ,&#39;MAE&#39;: &#39;{:.2f}&#39;.format(metrics.mean_absolute_error(y_test, new_y_pred_dtr)) ,&#39;R2 SCORE&#39;: &#39;{:.2f}&#39;.format(metrics.r2_score(y_test, new_y_pred_dtr))} , index={&#39;10&#39;}) new_model_results = pd.concat([new_result_linear,new_result_lasso,new_result_ridge,new_result_rf,new_result_gbr, new_result_ada,new_result_knn,new_result_bag,new_result_svr,new_result_dtr]) new_model_results = new_model_results[[&#39;Algorithm&#39;,&#39;RMSE&#39;,&#39;MSE&#39;,&#39;MAE&#39;,&#39;R2 SCORE&#39;]] print(&quot;Performance of models after feature selection&quot;) new_model_results = new_model_results.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#8B4060&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#BCA1AC&#39;}, subset=[&#39;Algorithm&#39;]) new_model_results . . Performance of models after feature selection . Algorithm RMSE MSE MAE R2 SCORE . 1 Linear Regression | 8.71 | 75.93 | 6.70 | 0.73 | . 2 Lasso Regression | 9.05 | 81.93 | 7.04 | 0.71 | . 3 Ridge Regression | 8.71 | 75.92 | 6.70 | 0.73 | . 4 Random Forest Regression | 8.71 | 75.93 | 6.70 | 0.73 | . 5 Gradient Boosting Regressor | 6.93 | 48.05 | 5.26 | 0.83 | . 6 Ada Boosting Regressor | 8.67 | 75.22 | 7.23 | 0.73 | . 7 KNN Regressor | 7.21 | 51.95 | 5.60 | 0.81 | . 8 Bagging Regressor | 6.40 | 40.90 | 4.90 | 0.85 | . 9 Support Vector Regressor | 8.79 | 77.22 | 6.51 | 0.72 | . 10 Decision Tree Regression | 7.63 | 58.16 | 5.28 | 0.79 | . new_result_linear_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Linear Regression (Repeated k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(new_k_result_linear)),np.std(abs(new_k_result_linear)))} , index={&#39;1&#39;}) new_result_lasso_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Lasso Regression (Repeated k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(new_k_result_lasso)),np.std(abs(new_k_result_lasso)))} , index={&#39;2&#39;}) new_result_ridge_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Ridge Regression (Repeated k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(new_k_result_ridge)),np.std(abs(new_k_result_ridge)))} , index={&#39;3&#39;}) new_result_rf_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Random Forest Regression (Repeated k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(new_k_result_rf)),np.std(abs(new_k_result_rf)))} , index={&#39;4&#39;}) new_result_gbr_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Gradient Boosting Regressor (Repeated k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(new_k_result_gbr)),np.std(abs(new_k_result_gbr)))} , index={&#39;5&#39;}) new_result_ada_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Ada Boosting Regressor (Repeated k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(new_k_result_ada)),np.std(abs(new_k_result_ada)))} , index={&#39;6&#39;}) new_result_knn_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;KNN Regressor (Repeated k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(new_k_result_knn)),np.std(abs(new_k_result_knn)))} , index={&#39;7&#39;}) new_result_bag_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Bagging Regressor (Repeated k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(new_k_result_bag)),np.std(abs(new_k_result_bag)))} , index={&#39;8&#39;}) new_result_svr_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Support Vector Regressor (Repeated k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(new_k_result_svr)),np.std(abs(new_k_result_svr)))} , index={&#39;9&#39;}) new_result_dtr_kf = pd.DataFrame({&#39;Algorithm&#39;:[&#39;Decision Tree Regression (Repeated k_fold)&#39;] ,&#39;Accuracy&#39;: &#39;{:.2f}({:.2f})&#39;.format(np.mean(abs(new_k_result_dtr)),np.std(abs(new_k_result_dtr)))} , index={&#39;10&#39;}) new_kf_results = pd.concat([new_result_linear_kf,new_result_lasso_kf,new_result_ridge_kf,new_result_rf_kf, new_result_gbr_kf,new_result_ada_kf,new_result_knn_kf,new_result_bag_kf,new_result_svr_kf,new_result_dtr_kf]) new_kf_results = new_kf_results[[&#39;Algorithm&#39;,&#39;Accuracy&#39;]] print(&quot;Repeated K-Fold Accuracy After feature selection for k=10 &quot;) new_kf_results = new_kf_results.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#436943&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#A7C9A7&#39;}, subset=[&#39;Algorithm&#39;]) new_kf_results . . Repeated K-Fold Accuracy After feature selection for k=10 . Algorithm Accuracy . 1 Linear Regression (Repeated k_fold) | 0.68(0.06) | . 2 Lasso Regression (Repeated k_fold) | 0.68(0.05) | . 3 Ridge Regression (Repeated k_fold) | 0.68(0.06) | . 4 Random Forest Regression (Repeated k_fold) | 0.85(0.04) | . 5 Gradient Boosting Regressor (Repeated k_fold) | 0.82(0.04) | . 6 Ada Boosting Regressor (Repeated k_fold) | 0.69(0.06) | . 7 KNN Regressor (Repeated k_fold) | 0.72(0.06) | . 8 Bagging Regressor (Repeated k_fold) | 0.84(0.04) | . 9 Support Vector Regressor (Repeated k_fold) | 0.67(0.06) | . 10 Decision Tree Regression (Repeated k_fold) | 0.79(0.05) | . Comparing the results of all the Models . df1_styler = model_results.set_table_attributes(&quot;style=&#39;display:inline&#39;&quot;).set_caption(&#39;Performance of Models&#39;) df2_styler = kf_results.set_table_attributes(&quot;style=&#39;display:inline&#39;&quot;).set_caption(&#39;K-Fold Accuracy&#39;) print(&quot;Before Feature Selection : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3,&quot; n&quot;) space = &quot; xa0&quot; * 10 display_html(df1_styler._repr_html_()+ space + df2_styler._repr_html_(), raw=True) . . Before Feature Selection : 👇🏻👇🏻👇🏻 . Performance of Models Algorithm RMSE MSE MAE R2 SCORE . 1 Linear Regression | 8.54 | 72.87 | 6.43 | 0.74 | . 2 Lasso Regression | 9.05 | 81.93 | 7.04 | 0.71 | . 3 Ridge Regression | 8.54 | 72.86 | 6.43 | 0.74 | . 4 Random Forest Regression | 8.54 | 72.87 | 6.43 | 0.74 | . 5 Gradient Boosting Regressor | 6.53 | 42.62 | 5.02 | 0.85 | . 6 Ada Boosting Regressor | 8.84 | 78.06 | 7.25 | 0.72 | . 7 KNN Regressor | 7.86 | 61.71 | 5.88 | 0.78 | . 8 Bagging Regressor | 6.45 | 41.58 | 4.85 | 0.85 | . 9 Support Vector Regressor | 8.56 | 73.36 | 6.28 | 0.74 | . 10 Decision Tree Regression | 7.28 | 53.01 | 5.09 | 0.81 | .           K-Fold Accuracy Algorithm Accuracy . 1 Linear Regression (k_fold) | 0.56(0.16) | . 2 Lasso Regression (k_fold) | 0.55(0.16) | . 3 Ridge Regression (k_fold) | 0.56(0.16) | . 4 Random Forest Regression (k_fold) | 0.64(0.19) | . 5 Gradient Boosting Regressor (k_fold) | 0.62(0.17) | . 6 Ada Boosting Regressor (k_fold) | 0.43(0.15) | . 7 KNN Regressor (k_fold) | 0.42(0.37) | . 8 Bagging Regressor (k_fold) | 0.60(0.22) | . 9 Support Vector Regressor (k_fold) | 0.56(0.17) | . 10 Decision Tree Regression (k_fold) | 0.43(0.32) | . df3_styler = new_model_results.set_table_attributes(&quot;style=&#39;display:inline&#39;&quot;).set_caption(&#39;Performance of Models&#39;) df4_styler = new_kf_results.set_table_attributes(&quot;style=&#39;display:inline&#39;&quot;).set_caption(&#39;Repeated K-Fold Accuracy&#39;) print(&quot;After Feature Selection : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3,&quot; n&quot;) space = &quot; xa0&quot; * 10 display_html(df3_styler._repr_html_()+ space + df4_styler._repr_html_(), raw=True) . . After Feature Selection : 👇🏻👇🏻👇🏻 . Performance of Models Algorithm RMSE MSE MAE R2 SCORE . 1 Linear Regression | 8.71 | 75.93 | 6.70 | 0.73 | . 2 Lasso Regression | 9.05 | 81.93 | 7.04 | 0.71 | . 3 Ridge Regression | 8.71 | 75.92 | 6.70 | 0.73 | . 4 Random Forest Regression | 8.71 | 75.93 | 6.70 | 0.73 | . 5 Gradient Boosting Regressor | 6.93 | 48.05 | 5.26 | 0.83 | . 6 Ada Boosting Regressor | 8.67 | 75.22 | 7.23 | 0.73 | . 7 KNN Regressor | 7.21 | 51.95 | 5.60 | 0.81 | . 8 Bagging Regressor | 6.40 | 40.90 | 4.90 | 0.85 | . 9 Support Vector Regressor | 8.79 | 77.22 | 6.51 | 0.72 | . 10 Decision Tree Regression | 7.63 | 58.16 | 5.28 | 0.79 | .           Repeated K-Fold Accuracy Algorithm Accuracy . 1 Linear Regression (Repeated k_fold) | 0.68(0.06) | . 2 Lasso Regression (Repeated k_fold) | 0.68(0.05) | . 3 Ridge Regression (Repeated k_fold) | 0.68(0.06) | . 4 Random Forest Regression (Repeated k_fold) | 0.85(0.04) | . 5 Gradient Boosting Regressor (Repeated k_fold) | 0.82(0.04) | . 6 Ada Boosting Regressor (Repeated k_fold) | 0.69(0.06) | . 7 KNN Regressor (Repeated k_fold) | 0.72(0.06) | . 8 Bagging Regressor (Repeated k_fold) | 0.84(0.04) | . 9 Support Vector Regressor (Repeated k_fold) | 0.67(0.06) | . 10 Decision Tree Regression (Repeated k_fold) | 0.79(0.05) | . 📝📝 We have analysed the Compressive Strength Data and used Machine Learning to Predict the Compressive Strength of Concrete. We have used Linear Regression and its variations, Decision Tree, Random Forests, Gradient Boosting, Ada Boosting, KNN, Bagging &amp; Support Vector Regressor to make predictions and compared their performance. Bagging Regressor has the lowest RMSE followed by Gradient Boosting Regressor, KNN Regressor and Decision Tree Regressor and are good choice for this problem. . &lt;/div&gt; .",
            "url": "https://swati5140.github.io/my_data_science_portfolio/2021/09/21/_08_31_concrete_strength.html",
            "relUrl": "/2021/09/21/_08_31_concrete_strength.html",
            "date": " • Sep 21, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Clustering players based on skills",
            "content": "Connection to the drive . # Mounting drive from google.colab import drive drive.mount(&#39;/content/drive&#39;) . . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . !pip install emoji . Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.4.2) . . !pip install emojis . Requirement already satisfied: emojis in /usr/local/lib/python3.7/dist-packages (0.6.0) . . #Packages related to general operating system &amp; warnings import os import warnings warnings.filterwarnings(&#39;ignore&#39;) # Importing required libraries and packages import emoji import emojis import pandas as pd pd.options.display.max_columns = 110 # None -&gt; No Restrictions pd.options.display.max_rows = 200 # None -&gt; Be careful with this pd.options.display.max_colwidth = 60 pd.options.display.precision = 1 pd.options.display.max_info_columns = 200 import numpy as np from numpy import unique from numpy import where import matplotlib.pyplot as plt import seaborn as sns from matplotlib.ticker import ScalarFormatter, FormatStrFormatter from sklearn import preprocessing from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score import scipy.cluster.hierarchy as sch from sklearn.cluster import DBSCAN from sklearn.neighbors import NearestNeighbors from sklearn.metrics import calinski_harabasz_score from sklearn.metrics import davies_bouldin_score from sklearn.cluster import AgglomerativeClustering print(emoji.emojize(&quot;:laptop:&quot;)*28 ,&quot; n nAll the required libraries and packages are imported successfully !!! n n&quot; ,emoji.emojize(&quot;:laptop:&quot;)*28) . . 💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻 All the required libraries and packages are imported successfully !!! 💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻 . #loading the dataset data = pd.read_csv(&#39;/content/drive/MyDrive/ColabNotebooks/1004_FIFA/players_20.csv&#39;) print(emoji.emojize(&quot;:file_folder:&quot;)*12 ,&quot; n nData loaded successfully !!! n n&quot; ,emoji.emojize(&quot;:file_folder:&quot;)*12) . . 📁📁📁📁📁📁📁📁📁📁📁📁 Data loaded successfully !!! 📁📁📁📁📁📁📁📁📁📁📁📁 . # To have a glimpse of the data print(&quot; nGlimpse of data : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A60B2E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).highlight_max(color=&#39;#0074FF&#39;,axis=0) .highlight_min(color=&#39;#00FFE5&#39;,axis=0) .highlight_null(null_color=&#39;#CCB3C&#39;) . . Glimpse of data : 👇🏻👇🏻👇🏻 . sofifa_id player_url short_name long_name age dob height_cm weight_kg nationality club overall potential value_eur wage_eur player_positions preferred_foot international_reputation weak_foot skill_moves work_rate body_type real_face release_clause_eur player_tags team_position team_jersey_number loaned_from joined contract_valid_until nation_position nation_jersey_number pace shooting passing dribbling defending physic gk_diving gk_handling gk_kicking gk_reflexes gk_speed gk_positioning player_traits attacking_crossing attacking_finishing attacking_heading_accuracy attacking_short_passing attacking_volleys skill_dribbling skill_curve skill_fk_accuracy skill_long_passing skill_ball_control movement_acceleration movement_sprint_speed movement_agility movement_reactions movement_balance power_shot_power power_jumping power_stamina power_strength power_long_shots mentality_aggression mentality_interceptions mentality_positioning mentality_vision mentality_penalties mentality_composure defending_marking defending_standing_tackle defending_sliding_tackle goalkeeping_diving goalkeeping_handling goalkeeping_kicking goalkeeping_positioning goalkeeping_reflexes ls st rs lw lf cf rf rw lam cam ram lm lcm cm rcm rm lwb ldm cdm rdm rwb lb lcb cb rcb rb . 0 158023 | https://sofifa.com/player/158023/lionel-messi/20/159586 | L. Messi | Lionel Andrés Messi Cuccittini | 32 | 1987-06-24 | 170 | 72 | Argentina | FC Barcelona | 94 | 94 | 95500000 | 565000 | RW, CF, ST | Left | 5 | 4 | 4 | Medium/Low | Messi | Yes | 195800000.0 | #Dribbler, #Distance Shooter, #Crosser, #FK Specialist, #Acrobat, #Clinical Finisher, #Complete Forward | RW | 10.0 | nan | 2004-07-01 | 2021.0 | nan | nan | 87.0 | 92.0 | 92.0 | 96.0 | 39.0 | 66.0 | nan | nan | nan | nan | nan | nan | Beat Offside Trap, Argues with Officials, Early Crosser, Finesse Shot, Speed Dribbler (CPU AI Only), 1-on-1 Rush, Giant Throw-in, Outside Foot Shot | 88 | 95 | 70 | 92 | 88 | 97 | 93 | 94 | 92 | 96 | 91 | 84 | 93 | 95 | 95 | 86 | 68 | 75 | 68 | 94 | 48 | 40 | 94 | 94 | 75 | 96 | 33 | 37 | 26 | 6 | 11 | 15 | 14 | 8 | 89+2 | 89+2 | 89+2 | 93+2 | 93+2 | 93+2 | 93+2 | 93+2 | 93+2 | 93+2 | 93+2 | 92+2 | 87+2 | 87+2 | 87+2 | 92+2 | 68+2 | 66+2 | 66+2 | 66+2 | 68+2 | 63+2 | 52+2 | 52+2 | 52+2 | 63+2 | . 1 20801 | https://sofifa.com/player/20801/c-ronaldo-dos-santos-aveiro/20/159586 | Cristiano Ronaldo | Cristiano Ronaldo dos Santos Aveiro | 34 | 1985-02-05 | 187 | 83 | Portugal | Juventus | 93 | 93 | 58500000 | 405000 | ST, LW | Right | 5 | 4 | 5 | High/Low | C. Ronaldo | Yes | 96500000.0 | #Speedster, #Dribbler, #Distance Shooter, #Acrobat, #Clinical Finisher, #Complete Forward | LW | 7.0 | nan | 2018-07-10 | 2022.0 | LS | 7.0 | 90.0 | 93.0 | 82.0 | 89.0 | 35.0 | 78.0 | nan | nan | nan | nan | nan | nan | Long Throw-in, Selfish, Argues with Officials, Early Crosser, Speed Dribbler (CPU AI Only), Skilled Dribbling | 84 | 94 | 89 | 83 | 87 | 89 | 81 | 76 | 77 | 92 | 89 | 91 | 87 | 96 | 71 | 95 | 95 | 85 | 78 | 93 | 63 | 29 | 95 | 82 | 85 | 95 | 28 | 32 | 24 | 7 | 11 | 15 | 14 | 11 | 91+3 | 91+3 | 91+3 | 89+3 | 90+3 | 90+3 | 90+3 | 89+3 | 88+3 | 88+3 | 88+3 | 88+3 | 81+3 | 81+3 | 81+3 | 88+3 | 65+3 | 61+3 | 61+3 | 61+3 | 65+3 | 61+3 | 53+3 | 53+3 | 53+3 | 61+3 | . 2 190871 | https://sofifa.com/player/190871/neymar-da-silva-santos-jr/20/159586 | Neymar Jr | Neymar da Silva Santos Junior | 27 | 1992-02-05 | 175 | 68 | Brazil | Paris Saint-Germain | 92 | 92 | 105500000 | 290000 | LW, CAM | Right | 5 | 5 | 5 | High/Medium | Neymar | Yes | 195200000.0 | #Speedster, #Dribbler, #Playmaker  , #Crosser, #FK Specialist, #Acrobat, #Clinical Finisher, #Complete Midfielder, #Complete Forward | CAM | 10.0 | nan | 2017-08-03 | 2022.0 | LW | 10.0 | 91.0 | 85.0 | 87.0 | 95.0 | 32.0 | 58.0 | nan | nan | nan | nan | nan | nan | Power Free-Kick, Injury Free, Selfish, Early Crosser, Speed Dribbler (CPU AI Only), Crowd Favourite | 87 | 87 | 62 | 87 | 87 | 96 | 88 | 87 | 81 | 95 | 94 | 89 | 96 | 92 | 84 | 80 | 61 | 81 | 49 | 84 | 51 | 36 | 87 | 90 | 90 | 94 | 27 | 26 | 29 | 9 | 9 | 15 | 15 | 11 | 84+3 | 84+3 | 84+3 | 90+3 | 89+3 | 89+3 | 89+3 | 90+3 | 90+3 | 90+3 | 90+3 | 89+3 | 82+3 | 82+3 | 82+3 | 89+3 | 66+3 | 61+3 | 61+3 | 61+3 | 66+3 | 61+3 | 46+3 | 46+3 | 46+3 | 61+3 | . 3 200389 | https://sofifa.com/player/200389/jan-oblak/20/159586 | J. Oblak | Jan Oblak | 26 | 1993-01-07 | 188 | 87 | Slovenia | Atlético Madrid | 91 | 93 | 77500000 | 125000 | GK | Right | 3 | 3 | 1 | Medium/Medium | Normal | Yes | 164700000.0 | nan | GK | 13.0 | nan | 2014-07-16 | 2023.0 | GK | 1.0 | nan | nan | nan | nan | nan | nan | 87.0 | 92.0 | 78.0 | 89.0 | 52.0 | 90.0 | Flair, Acrobatic Clearance | 13 | 11 | 15 | 43 | 13 | 12 | 13 | 14 | 40 | 30 | 43 | 60 | 67 | 88 | 49 | 59 | 78 | 41 | 78 | 12 | 34 | 19 | 11 | 65 | 11 | 68 | 27 | 12 | 18 | 87 | 92 | 78 | 90 | 89 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . 4 183277 | https://sofifa.com/player/183277/eden-hazard/20/159586 | E. Hazard | Eden Hazard | 28 | 1991-01-07 | 175 | 74 | Belgium | Real Madrid | 91 | 91 | 90000000 | 470000 | LW, CF | Right | 4 | 4 | 4 | High/Medium | Normal | Yes | 184500000.0 | #Speedster, #Dribbler, #Acrobat | LW | 7.0 | nan | 2019-07-01 | 2024.0 | LF | 10.0 | 91.0 | 83.0 | 86.0 | 94.0 | 35.0 | 66.0 | nan | nan | nan | nan | nan | nan | Beat Offside Trap, Selfish, Finesse Shot, Speed Dribbler (CPU AI Only), Crowd Favourite | 81 | 84 | 61 | 89 | 83 | 95 | 83 | 79 | 83 | 94 | 94 | 88 | 95 | 90 | 94 | 82 | 56 | 84 | 63 | 80 | 54 | 41 | 87 | 89 | 88 | 91 | 34 | 27 | 22 | 11 | 12 | 6 | 8 | 8 | 83+3 | 83+3 | 83+3 | 89+3 | 88+3 | 88+3 | 88+3 | 89+3 | 89+3 | 89+3 | 89+3 | 89+3 | 83+3 | 83+3 | 83+3 | 89+3 | 66+3 | 63+3 | 63+3 | 63+3 | 66+3 | 61+3 | 49+3 | 49+3 | 49+3 | 61+3 | . 5 192985 | https://sofifa.com/player/192985/kevin-de-bruyne/20/159586 | K. De Bruyne | Kevin De Bruyne | 28 | 1991-06-28 | 181 | 70 | Belgium | Manchester City | 91 | 91 | 90000000 | 370000 | CAM, CM | Right | 4 | 5 | 4 | High/High | Normal | Yes | 166500000.0 | #Dribbler, #Playmaker  , #Engine, #Distance Shooter, #Crosser, #Complete Midfielder | RCM | 17.0 | nan | 2015-08-30 | 2023.0 | RCM | 7.0 | 76.0 | 86.0 | 92.0 | 86.0 | 61.0 | 78.0 | nan | nan | nan | nan | nan | nan | Power Free-Kick, Avoids Using Weaker Foot, Dives Into Tackles (CPU AI Only), Leadership, Argues with Officials, Finesse Shot | 93 | 82 | 55 | 92 | 82 | 86 | 85 | 83 | 91 | 91 | 77 | 76 | 78 | 91 | 76 | 91 | 63 | 89 | 74 | 90 | 76 | 61 | 88 | 94 | 79 | 91 | 68 | 58 | 51 | 15 | 13 | 5 | 10 | 13 | 82+3 | 82+3 | 82+3 | 87+3 | 87+3 | 87+3 | 87+3 | 87+3 | 88+3 | 88+3 | 88+3 | 88+3 | 87+3 | 87+3 | 87+3 | 88+3 | 77+3 | 77+3 | 77+3 | 77+3 | 77+3 | 73+3 | 66+3 | 66+3 | 66+3 | 73+3 | . 6 192448 | https://sofifa.com/player/192448/marc-andre-ter-stegen/20/159586 | M. ter Stegen | Marc-André ter Stegen | 27 | 1992-04-30 | 187 | 85 | Germany | FC Barcelona | 90 | 93 | 67500000 | 250000 | GK | Right | 3 | 4 | 1 | Medium/Medium | Normal | Yes | 143400000.0 | nan | GK | 1.0 | nan | 2014-07-01 | 2022.0 | SUB | 22.0 | nan | nan | nan | nan | nan | nan | 88.0 | 85.0 | 88.0 | 90.0 | 45.0 | 88.0 | Swerve Pass, Acrobatic Clearance, Flair Passes | 18 | 14 | 11 | 61 | 14 | 21 | 18 | 12 | 63 | 30 | 38 | 50 | 37 | 86 | 43 | 66 | 79 | 35 | 78 | 10 | 43 | 22 | 11 | 70 | 25 | 70 | 25 | 13 | 10 | 88 | 85 | 88 | 88 | 90 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . 7 203376 | https://sofifa.com/player/203376/virgil-van-dijk/20/159586 | V. van Dijk | Virgil van Dijk | 27 | 1991-07-08 | 193 | 92 | Netherlands | Liverpool | 90 | 91 | 78000000 | 200000 | CB | Right | 3 | 3 | 2 | Medium/Medium | Normal | Yes | 150200000.0 | #Tackling , #Tactician , #Strength, #Complete Defender | LCB | 4.0 | nan | 2018-01-01 | 2023.0 | LCB | 4.0 | 77.0 | 60.0 | 70.0 | 71.0 | 90.0 | 86.0 | nan | nan | nan | nan | nan | nan | Diver, Avoids Using Weaker Foot, Leadership, Long Passer (CPU AI Only) | 53 | 52 | 86 | 78 | 45 | 70 | 60 | 70 | 81 | 76 | 74 | 79 | 61 | 88 | 53 | 81 | 90 | 75 | 92 | 64 | 82 | 89 | 47 | 65 | 62 | 89 | 91 | 92 | 85 | 13 | 10 | 13 | 11 | 11 | 69+3 | 69+3 | 69+3 | 67+3 | 69+3 | 69+3 | 69+3 | 67+3 | 69+3 | 69+3 | 69+3 | 69+3 | 74+3 | 74+3 | 74+3 | 69+3 | 79+3 | 83+3 | 83+3 | 83+3 | 79+3 | 81+3 | 87+3 | 87+3 | 87+3 | 81+3 | . 8 177003 | https://sofifa.com/player/177003/luka-modric/20/159586 | L. Modrić | Luka Modrić | 33 | 1985-09-09 | 172 | 66 | Croatia | Real Madrid | 90 | 90 | 45000000 | 340000 | CM | Right | 4 | 4 | 4 | High/High | Lean | Yes | 92300000.0 | #Dribbler, #Playmaker  , #Crosser, #Acrobat, #Complete Midfielder | RCM | 10.0 | nan | 2012-08-01 | 2020.0 | nan | nan | 74.0 | 76.0 | 89.0 | 89.0 | 72.0 | 66.0 | nan | nan | nan | nan | nan | nan | Argues with Officials, Finesse Shot, Speed Dribbler (CPU AI Only), Crowd Favourite | 86 | 72 | 55 | 92 | 76 | 87 | 85 | 78 | 88 | 92 | 77 | 71 | 92 | 89 | 93 | 79 | 68 | 85 | 58 | 82 | 62 | 82 | 79 | 91 | 82 | 92 | 68 | 76 | 71 | 13 | 9 | 7 | 14 | 9 | 77+3 | 77+3 | 77+3 | 84+3 | 83+3 | 83+3 | 83+3 | 84+3 | 86+3 | 86+3 | 86+3 | 85+3 | 87+3 | 87+3 | 87+3 | 85+3 | 81+3 | 81+3 | 81+3 | 81+3 | 81+3 | 79+3 | 72+3 | 72+3 | 72+3 | 79+3 | . 9 209331 | https://sofifa.com/player/209331/mohamed-salah/20/159586 | M. Salah | Mohamed Salah Ghaly | 27 | 1992-06-15 | 175 | 71 | Egypt | Liverpool | 90 | 90 | 80500000 | 240000 | RW, ST | Left | 3 | 3 | 4 | High/Medium | PLAYER_BODY_TYPE_25 | Yes | 148900000.0 | #Speedster, #Dribbler, #Acrobat, #Clinical Finisher, #Complete Forward | RW | 11.0 | nan | 2017-07-01 | 2023.0 | RW | 10.0 | 93.0 | 86.0 | 81.0 | 89.0 | 45.0 | 74.0 | nan | nan | nan | nan | nan | nan | Beat Offside Trap, Argues with Officials, Early Crosser, Speed Dribbler (CPU AI Only), Outside Foot Shot | 79 | 90 | 59 | 84 | 79 | 89 | 83 | 69 | 75 | 89 | 94 | 92 | 91 | 92 | 88 | 80 | 69 | 85 | 73 | 84 | 63 | 55 | 92 | 84 | 77 | 91 | 38 | 43 | 41 | 14 | 14 | 9 | 11 | 14 | 84+3 | 84+3 | 84+3 | 88+3 | 88+3 | 88+3 | 88+3 | 88+3 | 87+3 | 87+3 | 87+3 | 87+3 | 81+3 | 81+3 | 81+3 | 87+3 | 70+3 | 67+3 | 67+3 | 67+3 | 70+3 | 66+3 | 57+3 | 57+3 | 57+3 | 66+3 | . Basic Info of the dataset . #finding the no. of rows and cols print(&quot; nFinding the no. of rows and cols in the dataset : n n&quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;No. of Players : {}&quot;.format(data.shape[0])) print(&quot;No. of features : {} &quot;.format(data.shape[1])) . . Finding the no. of rows and cols in the dataset : 👇🏻👇🏻👇🏻 No. of Players : 18278 No. of features : 104 . # Overview of shape, attributes, types and missing values print(&quot; nOverview of shape, attributes, types and missing values : n n&quot; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) overview = data.info(verbose=True) . . Overview of shape, attributes, types and missing values : 👇🏻👇🏻👇🏻 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 18278 entries, 0 to 18277 Data columns (total 104 columns): # Column Non-Null Count Dtype -- -- 0 sofifa_id 18278 non-null int64 1 player_url 18278 non-null object 2 short_name 18278 non-null object 3 long_name 18278 non-null object 4 age 18278 non-null int64 5 dob 18278 non-null object 6 height_cm 18278 non-null int64 7 weight_kg 18278 non-null int64 8 nationality 18278 non-null object 9 club 18278 non-null object 10 overall 18278 non-null int64 11 potential 18278 non-null int64 12 value_eur 18278 non-null int64 13 wage_eur 18278 non-null int64 14 player_positions 18278 non-null object 15 preferred_foot 18278 non-null object 16 international_reputation 18278 non-null int64 17 weak_foot 18278 non-null int64 18 skill_moves 18278 non-null int64 19 work_rate 18278 non-null object 20 body_type 18278 non-null object 21 real_face 18278 non-null object 22 release_clause_eur 16980 non-null float64 23 player_tags 1499 non-null object 24 team_position 18038 non-null object 25 team_jersey_number 18038 non-null float64 26 loaned_from 1048 non-null object 27 joined 16990 non-null object 28 contract_valid_until 18038 non-null float64 29 nation_position 1126 non-null object 30 nation_jersey_number 1126 non-null float64 31 pace 16242 non-null float64 32 shooting 16242 non-null float64 33 passing 16242 non-null float64 34 dribbling 16242 non-null float64 35 defending 16242 non-null float64 36 physic 16242 non-null float64 37 gk_diving 2036 non-null float64 38 gk_handling 2036 non-null float64 39 gk_kicking 2036 non-null float64 40 gk_reflexes 2036 non-null float64 41 gk_speed 2036 non-null float64 42 gk_positioning 2036 non-null float64 43 player_traits 7566 non-null object 44 attacking_crossing 18278 non-null int64 45 attacking_finishing 18278 non-null int64 46 attacking_heading_accuracy 18278 non-null int64 47 attacking_short_passing 18278 non-null int64 48 attacking_volleys 18278 non-null int64 49 skill_dribbling 18278 non-null int64 50 skill_curve 18278 non-null int64 51 skill_fk_accuracy 18278 non-null int64 52 skill_long_passing 18278 non-null int64 53 skill_ball_control 18278 non-null int64 54 movement_acceleration 18278 non-null int64 55 movement_sprint_speed 18278 non-null int64 56 movement_agility 18278 non-null int64 57 movement_reactions 18278 non-null int64 58 movement_balance 18278 non-null int64 59 power_shot_power 18278 non-null int64 60 power_jumping 18278 non-null int64 61 power_stamina 18278 non-null int64 62 power_strength 18278 non-null int64 63 power_long_shots 18278 non-null int64 64 mentality_aggression 18278 non-null int64 65 mentality_interceptions 18278 non-null int64 66 mentality_positioning 18278 non-null int64 67 mentality_vision 18278 non-null int64 68 mentality_penalties 18278 non-null int64 69 mentality_composure 18278 non-null int64 70 defending_marking 18278 non-null int64 71 defending_standing_tackle 18278 non-null int64 72 defending_sliding_tackle 18278 non-null int64 73 goalkeeping_diving 18278 non-null int64 74 goalkeeping_handling 18278 non-null int64 75 goalkeeping_kicking 18278 non-null int64 76 goalkeeping_positioning 18278 non-null int64 77 goalkeeping_reflexes 18278 non-null int64 78 ls 16242 non-null object 79 st 16242 non-null object 80 rs 16242 non-null object 81 lw 16242 non-null object 82 lf 16242 non-null object 83 cf 16242 non-null object 84 rf 16242 non-null object 85 rw 16242 non-null object 86 lam 16242 non-null object 87 cam 16242 non-null object 88 ram 16242 non-null object 89 lm 16242 non-null object 90 lcm 16242 non-null object 91 cm 16242 non-null object 92 rcm 16242 non-null object 93 rm 16242 non-null object 94 lwb 16242 non-null object 95 ldm 16242 non-null object 96 cdm 16242 non-null object 97 rdm 16242 non-null object 98 rwb 16242 non-null object 99 lb 16242 non-null object 100 lcb 16242 non-null object 101 cb 16242 non-null object 102 rcb 16242 non-null object 103 rb 16242 non-null object dtypes: float64(16), int64(45), object(43) memory usage: 14.5+ MB . 📝📝 There are 45 int variables, 16 float variables and 43 object variables in the dataset. . # General stats of the numerical variables print(&quot; nGeneral stats of the numerical variables : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.describe().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#753976&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . General stats of the numerical variables : 👇🏻👇🏻👇🏻 . sofifa_id age height_cm weight_kg overall potential value_eur wage_eur international_reputation weak_foot skill_moves release_clause_eur team_jersey_number contract_valid_until nation_jersey_number pace shooting passing dribbling defending physic gk_diving gk_handling gk_kicking gk_reflexes gk_speed gk_positioning attacking_crossing attacking_finishing attacking_heading_accuracy attacking_short_passing attacking_volleys skill_dribbling skill_curve skill_fk_accuracy skill_long_passing skill_ball_control movement_acceleration movement_sprint_speed movement_agility movement_reactions movement_balance power_shot_power power_jumping power_stamina power_strength power_long_shots mentality_aggression mentality_interceptions mentality_positioning mentality_vision mentality_penalties mentality_composure defending_marking defending_standing_tackle defending_sliding_tackle goalkeeping_diving goalkeeping_handling goalkeeping_kicking goalkeeping_positioning goalkeeping_reflexes . count 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 16980.0 | 18038.0 | 18038.0 | 1126.0 | 16242.0 | 16242.0 | 16242.0 | 16242.0 | 16242.0 | 16242.0 | 2036.0 | 2036.0 | 2036.0 | 2036.0 | 2036.0 | 2036.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | 18278.0 | . mean 219738.9 | 25.3 | 181.4 | 75.3 | 66.2 | 71.5 | 2484037.6 | 9456.9 | 1.1 | 2.9 | 2.4 | 4740717.4 | 20.1 | 2021.1 | 12.1 | 67.7 | 52.3 | 57.2 | 62.5 | 51.6 | 64.9 | 65.4 | 63.1 | 61.8 | 66.4 | 37.8 | 63.4 | 49.7 | 45.6 | 52.2 | 58.7 | 42.8 | 55.6 | 47.3 | 42.7 | 52.8 | 58.5 | 64.3 | 64.4 | 63.5 | 61.8 | 63.9 | 58.2 | 64.9 | 62.9 | 65.2 | 46.8 | 55.7 | 46.4 | 50.1 | 53.6 | 48.4 | 58.5 | 46.8 | 47.6 | 45.6 | 16.6 | 16.4 | 16.2 | 16.4 | 16.7 | . std 27960.2 | 4.7 | 6.8 | 7.0 | 6.9 | 6.1 | 5585481.1 | 21351.7 | 0.4 | 0.7 | 0.8 | 11030016.3 | 16.6 | 1.3 | 6.7 | 11.3 | 14.0 | 10.4 | 10.3 | 16.4 | 9.8 | 7.7 | 7.2 | 7.5 | 8.2 | 10.6 | 8.4 | 18.3 | 19.6 | 17.4 | 14.7 | 17.7 | 18.9 | 18.4 | 17.4 | 15.2 | 16.7 | 15.0 | 14.8 | 14.8 | 9.1 | 14.2 | 13.3 | 11.9 | 16.0 | 12.5 | 19.3 | 17.3 | 20.8 | 19.6 | 14.0 | 15.7 | 11.9 | 20.1 | 21.6 | 21.2 | 17.7 | 17.0 | 16.6 | 17.1 | 18.0 | . min 768.0 | 16.0 | 156.0 | 50.0 | 48.0 | 49.0 | 0.0 | 0.0 | 1.0 | 1.0 | 1.0 | 13000.0 | 1.0 | 2019.0 | 1.0 | 24.0 | 15.0 | 24.0 | 23.0 | 15.0 | 27.0 | 44.0 | 42.0 | 35.0 | 45.0 | 12.0 | 41.0 | 5.0 | 2.0 | 5.0 | 7.0 | 3.0 | 4.0 | 6.0 | 4.0 | 8.0 | 5.0 | 12.0 | 11.0 | 11.0 | 21.0 | 12.0 | 14.0 | 19.0 | 12.0 | 20.0 | 4.0 | 9.0 | 3.0 | 2.0 | 9.0 | 7.0 | 12.0 | 1.0 | 5.0 | 3.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | . 25% 204445.5 | 22.0 | 177.0 | 70.0 | 62.0 | 67.0 | 325000.0 | 1000.0 | 1.0 | 3.0 | 2.0 | 563000.0 | 9.0 | 2020.0 | 6.0 | 61.0 | 42.0 | 50.0 | 57.0 | 36.0 | 59.0 | 60.0 | 58.0 | 57.0 | 60.8 | 29.0 | 58.0 | 38.0 | 30.0 | 44.0 | 54.0 | 30.0 | 50.0 | 34.0 | 31.0 | 43.0 | 54.0 | 56.0 | 57.0 | 55.0 | 56.0 | 56.0 | 48.0 | 58.0 | 56.0 | 58.0 | 32.0 | 44.0 | 25.0 | 39.0 | 44.0 | 39.0 | 51.0 | 29.0 | 27.0 | 24.0 | 8.0 | 8.0 | 8.0 | 8.0 | 8.0 | . 50% 226165.0 | 25.0 | 181.0 | 75.0 | 66.0 | 71.0 | 700000.0 | 3000.0 | 1.0 | 3.0 | 2.0 | 1200000.0 | 17.0 | 2021.0 | 12.0 | 69.0 | 54.0 | 58.0 | 64.0 | 56.0 | 66.0 | 65.0 | 63.0 | 61.0 | 66.0 | 39.0 | 64.0 | 54.0 | 49.0 | 56.0 | 62.0 | 44.0 | 61.0 | 49.0 | 41.0 | 56.0 | 63.0 | 67.0 | 67.0 | 66.0 | 62.0 | 66.0 | 59.0 | 66.0 | 66.0 | 66.0 | 51.0 | 58.0 | 52.0 | 55.0 | 55.0 | 49.0 | 60.0 | 52.0 | 55.0 | 52.0 | 11.0 | 11.0 | 11.0 | 11.0 | 11.0 | . 75% 240795.8 | 29.0 | 186.0 | 80.0 | 71.0 | 75.0 | 2100000.0 | 8000.0 | 1.0 | 3.0 | 3.0 | 3700000.0 | 27.0 | 2022.0 | 18.0 | 75.0 | 63.0 | 64.0 | 69.0 | 65.0 | 72.0 | 70.0 | 68.0 | 66.0 | 72.0 | 46.0 | 69.0 | 64.0 | 62.0 | 64.0 | 68.0 | 56.0 | 68.0 | 62.0 | 56.0 | 64.0 | 69.0 | 75.0 | 75.0 | 74.0 | 68.0 | 74.0 | 68.0 | 73.0 | 74.0 | 74.0 | 62.0 | 69.0 | 64.0 | 64.0 | 64.0 | 60.0 | 67.0 | 64.0 | 66.0 | 64.0 | 14.0 | 14.0 | 14.0 | 14.0 | 14.0 | . max 252905.0 | 42.0 | 205.0 | 110.0 | 94.0 | 95.0 | 105500000.0 | 565000.0 | 5.0 | 5.0 | 5.0 | 195800000.0 | 99.0 | 2026.0 | 30.0 | 96.0 | 93.0 | 92.0 | 96.0 | 90.0 | 90.0 | 90.0 | 92.0 | 93.0 | 92.0 | 65.0 | 91.0 | 93.0 | 95.0 | 93.0 | 92.0 | 90.0 | 97.0 | 94.0 | 94.0 | 92.0 | 96.0 | 97.0 | 96.0 | 96.0 | 96.0 | 97.0 | 95.0 | 95.0 | 97.0 | 97.0 | 94.0 | 95.0 | 92.0 | 95.0 | 94.0 | 92.0 | 96.0 | 94.0 | 92.0 | 90.0 | 90.0 | 92.0 | 93.0 | 91.0 | 92.0 | . 📝📝 Essence of above dataframe : ✏️ The min. age of the players is 16, avg. age is 25 and the max. age upto which they can play is 42 years. ✏️ The min. height of the players is 156cm, avg. height is 181cm and the max. height is 205cm. ✏️ The min. weight of the players should be 50Kg, avg. weight should be 75Kg and the max. weight should not exceed than 110Kg. ✏️ The min. overall score of the players is 48, avg. overall score is 66 and the max. overall score is 94 out of 100. ✏️ The min. potential score of the players is 49, avg. potential score is 71.5 and the max. overall score is 95 out of 100. ✏️ The avg. valuation of the players is around 2.5 million Euros and the max. valuation is 105 million Euros. ✏️ The avg. wage of the players is around 9.5k Euros and the max. wage is around 0.56 million Euros. . # General stats of the categorical variables print(&quot; nGeneral stats of the categorical variables : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.describe(include=[&#39;object&#39;]).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#117A65&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . General stats of the categorical variables : 👇🏻👇🏻👇🏻 . player_url short_name long_name dob nationality club player_positions preferred_foot work_rate body_type real_face player_tags team_position loaned_from joined nation_position player_traits ls st rs lw lf cf rf rw lam cam ram lm lcm cm rcm rm lwb ldm cdm rdm rwb lb lcb cb rcb rb . count 18278 | 18278 | 18278 | 18278 | 18278 | 18278 | 18278 | 18278 | 18278 | 18278 | 18278 | 1499 | 18038 | 1048 | 16990 | 1126 | 7566 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | 16242 | . unique 18278 | 17354 | 18218 | 6142 | 162 | 698 | 643 | 2 | 9 | 10 | 2 | 83 | 29 | 316 | 1760 | 26 | 921 | 94 | 94 | 94 | 107 | 103 | 103 | 103 | 107 | 103 | 103 | 103 | 100 | 88 | 88 | 88 | 100 | 98 | 98 | 98 | 98 | 98 | 96 | 110 | 110 | 110 | 96 | . top https://sofifa.com/player/208507/daniel-barrio-alvarez/20/159586 | J. Rodríguez | Liam Kelly | 1992-02-29 | England | Manchester United | CB | Right | Medium/Medium | Normal | No | #Strength | SUB | Sassuolo | 2019-07-01 | SUB | Early Crosser | 61+2 | 61+2 | 61+2 | 63+2 | 63+2 | 63+2 | 63+2 | 63+2 | 61+2 | 61+2 | 61+2 | 61+2 | 58+2 | 58+2 | 58+2 | 61+2 | 59+2 | 59+2 | 59+2 | 59+2 | 59+2 | 61+2 | 63+2 | 63+2 | 63+2 | 61+2 | . freq 1 | 11 | 3 | 113 | 1667 | 33 | 2322 | 13960 | 9875 | 10750 | 16310 | 514 | 7820 | 17 | 1465 | 587 | 501 | 725 | 725 | 725 | 736 | 727 | 727 | 727 | 736 | 748 | 748 | 748 | 809 | 775 | 775 | 775 | 809 | 682 | 636 | 636 | 636 | 682 | 667 | 621 | 621 | 621 | 667 | . 📝📝 Essence of above dataframe : ✏️ There were 162 different countries participated in FIFA 2020 and most players were from England . ✏️ There were 698 different clubs and the top clubs was CD Leganés. ✏️ The most preferred position of the players was CB. ✏️ The most preferred foot by the players was Right. ✏️ The work rate of most of the players is Medium/Medium. ✏️ The body type of most of the players is Normal. . Data Preparation&lt;/h1&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; 1. Finding duplicate values . # checking for duplicate values if present in the dataframe print(&quot;Duplicate Data&quot; ,emoji.emojize(&quot;:red_question_mark:&quot;)*2,&quot; n&quot;) print(emoji.emojize(&quot;:check_mark_button:&quot;)*3 ,&quot; n n&quot;,data.duplicated().any() ,&quot; n n&quot;,emoji.emojize(&quot;:check_mark_button:&quot;)*3) . . Duplicate Data ❓❓ ✅✅✅ False ✅✅✅ . 📝📝 There are no duplicate values in the dataset. . 2. Finding Missing Values . #checking for missing values print(&quot;Missing values&quot; ,emoji.emojize(&quot;:red_question_mark:&quot;)*2,&quot; n&quot;) print(emoji.emojize(&quot;:cross_mark_button::&quot;)*3 ,&quot; n n&quot;,data.isnull().values.any() ,&quot; n n&quot;,emoji.emojize(&quot;:cross_mark_button:&quot;)*3) . . Missing values ❓❓ ❎:❎:❎: True ❎❎❎ . # missing values in the dataset print(&#39; nMissing Values n&#39; ,&quot; n&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.isnull().values.sum() . . Missing Values 👇🏻👇🏻👇🏻 . 244935 . print(&#39; nMissing Values in the dataset : &#39; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#610646&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).highlight_null(null_color=&#39;#CCB3C5&#39;) . . Missing Values in the dataset : 👇🏻👇🏻👇🏻 . sofifa_id player_url short_name long_name age dob height_cm weight_kg nationality club overall potential value_eur wage_eur player_positions preferred_foot international_reputation weak_foot skill_moves work_rate body_type real_face release_clause_eur player_tags team_position team_jersey_number loaned_from joined contract_valid_until nation_position nation_jersey_number pace shooting passing dribbling defending physic gk_diving gk_handling gk_kicking gk_reflexes gk_speed gk_positioning player_traits attacking_crossing attacking_finishing attacking_heading_accuracy attacking_short_passing attacking_volleys skill_dribbling skill_curve skill_fk_accuracy skill_long_passing skill_ball_control movement_acceleration movement_sprint_speed movement_agility movement_reactions movement_balance power_shot_power power_jumping power_stamina power_strength power_long_shots mentality_aggression mentality_interceptions mentality_positioning mentality_vision mentality_penalties mentality_composure defending_marking defending_standing_tackle defending_sliding_tackle goalkeeping_diving goalkeeping_handling goalkeeping_kicking goalkeeping_positioning goalkeeping_reflexes ls st rs lw lf cf rf rw lam cam ram lm lcm cm rcm rm lwb ldm cdm rdm rwb lb lcb cb rcb rb . 0 158023 | https://sofifa.com/player/158023/lionel-messi/20/159586 | L. Messi | Lionel Andrés Messi Cuccittini | 32 | 1987-06-24 | 170 | 72 | Argentina | FC Barcelona | 94 | 94 | 95500000 | 565000 | RW, CF, ST | Left | 5 | 4 | 4 | Medium/Low | Messi | Yes | 195800000.0 | #Dribbler, #Distance Shooter, #Crosser, #FK Specialist, #Acrobat, #Clinical Finisher, #Complete Forward | RW | 10.0 | nan | 2004-07-01 | 2021.0 | nan | nan | 87.0 | 92.0 | 92.0 | 96.0 | 39.0 | 66.0 | nan | nan | nan | nan | nan | nan | Beat Offside Trap, Argues with Officials, Early Crosser, Finesse Shot, Speed Dribbler (CPU AI Only), 1-on-1 Rush, Giant Throw-in, Outside Foot Shot | 88 | 95 | 70 | 92 | 88 | 97 | 93 | 94 | 92 | 96 | 91 | 84 | 93 | 95 | 95 | 86 | 68 | 75 | 68 | 94 | 48 | 40 | 94 | 94 | 75 | 96 | 33 | 37 | 26 | 6 | 11 | 15 | 14 | 8 | 89+2 | 89+2 | 89+2 | 93+2 | 93+2 | 93+2 | 93+2 | 93+2 | 93+2 | 93+2 | 93+2 | 92+2 | 87+2 | 87+2 | 87+2 | 92+2 | 68+2 | 66+2 | 66+2 | 66+2 | 68+2 | 63+2 | 52+2 | 52+2 | 52+2 | 63+2 | . 1 20801 | https://sofifa.com/player/20801/c-ronaldo-dos-santos-aveiro/20/159586 | Cristiano Ronaldo | Cristiano Ronaldo dos Santos Aveiro | 34 | 1985-02-05 | 187 | 83 | Portugal | Juventus | 93 | 93 | 58500000 | 405000 | ST, LW | Right | 5 | 4 | 5 | High/Low | C. Ronaldo | Yes | 96500000.0 | #Speedster, #Dribbler, #Distance Shooter, #Acrobat, #Clinical Finisher, #Complete Forward | LW | 7.0 | nan | 2018-07-10 | 2022.0 | LS | 7.0 | 90.0 | 93.0 | 82.0 | 89.0 | 35.0 | 78.0 | nan | nan | nan | nan | nan | nan | Long Throw-in, Selfish, Argues with Officials, Early Crosser, Speed Dribbler (CPU AI Only), Skilled Dribbling | 84 | 94 | 89 | 83 | 87 | 89 | 81 | 76 | 77 | 92 | 89 | 91 | 87 | 96 | 71 | 95 | 95 | 85 | 78 | 93 | 63 | 29 | 95 | 82 | 85 | 95 | 28 | 32 | 24 | 7 | 11 | 15 | 14 | 11 | 91+3 | 91+3 | 91+3 | 89+3 | 90+3 | 90+3 | 90+3 | 89+3 | 88+3 | 88+3 | 88+3 | 88+3 | 81+3 | 81+3 | 81+3 | 88+3 | 65+3 | 61+3 | 61+3 | 61+3 | 65+3 | 61+3 | 53+3 | 53+3 | 53+3 | 61+3 | . 2 190871 | https://sofifa.com/player/190871/neymar-da-silva-santos-jr/20/159586 | Neymar Jr | Neymar da Silva Santos Junior | 27 | 1992-02-05 | 175 | 68 | Brazil | Paris Saint-Germain | 92 | 92 | 105500000 | 290000 | LW, CAM | Right | 5 | 5 | 5 | High/Medium | Neymar | Yes | 195200000.0 | #Speedster, #Dribbler, #Playmaker  , #Crosser, #FK Specialist, #Acrobat, #Clinical Finisher, #Complete Midfielder, #Complete Forward | CAM | 10.0 | nan | 2017-08-03 | 2022.0 | LW | 10.0 | 91.0 | 85.0 | 87.0 | 95.0 | 32.0 | 58.0 | nan | nan | nan | nan | nan | nan | Power Free-Kick, Injury Free, Selfish, Early Crosser, Speed Dribbler (CPU AI Only), Crowd Favourite | 87 | 87 | 62 | 87 | 87 | 96 | 88 | 87 | 81 | 95 | 94 | 89 | 96 | 92 | 84 | 80 | 61 | 81 | 49 | 84 | 51 | 36 | 87 | 90 | 90 | 94 | 27 | 26 | 29 | 9 | 9 | 15 | 15 | 11 | 84+3 | 84+3 | 84+3 | 90+3 | 89+3 | 89+3 | 89+3 | 90+3 | 90+3 | 90+3 | 90+3 | 89+3 | 82+3 | 82+3 | 82+3 | 89+3 | 66+3 | 61+3 | 61+3 | 61+3 | 66+3 | 61+3 | 46+3 | 46+3 | 46+3 | 61+3 | . 3 200389 | https://sofifa.com/player/200389/jan-oblak/20/159586 | J. Oblak | Jan Oblak | 26 | 1993-01-07 | 188 | 87 | Slovenia | Atlético Madrid | 91 | 93 | 77500000 | 125000 | GK | Right | 3 | 3 | 1 | Medium/Medium | Normal | Yes | 164700000.0 | nan | GK | 13.0 | nan | 2014-07-16 | 2023.0 | GK | 1.0 | nan | nan | nan | nan | nan | nan | 87.0 | 92.0 | 78.0 | 89.0 | 52.0 | 90.0 | Flair, Acrobatic Clearance | 13 | 11 | 15 | 43 | 13 | 12 | 13 | 14 | 40 | 30 | 43 | 60 | 67 | 88 | 49 | 59 | 78 | 41 | 78 | 12 | 34 | 19 | 11 | 65 | 11 | 68 | 27 | 12 | 18 | 87 | 92 | 78 | 90 | 89 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . 4 183277 | https://sofifa.com/player/183277/eden-hazard/20/159586 | E. Hazard | Eden Hazard | 28 | 1991-01-07 | 175 | 74 | Belgium | Real Madrid | 91 | 91 | 90000000 | 470000 | LW, CF | Right | 4 | 4 | 4 | High/Medium | Normal | Yes | 184500000.0 | #Speedster, #Dribbler, #Acrobat | LW | 7.0 | nan | 2019-07-01 | 2024.0 | LF | 10.0 | 91.0 | 83.0 | 86.0 | 94.0 | 35.0 | 66.0 | nan | nan | nan | nan | nan | nan | Beat Offside Trap, Selfish, Finesse Shot, Speed Dribbler (CPU AI Only), Crowd Favourite | 81 | 84 | 61 | 89 | 83 | 95 | 83 | 79 | 83 | 94 | 94 | 88 | 95 | 90 | 94 | 82 | 56 | 84 | 63 | 80 | 54 | 41 | 87 | 89 | 88 | 91 | 34 | 27 | 22 | 11 | 12 | 6 | 8 | 8 | 83+3 | 83+3 | 83+3 | 89+3 | 88+3 | 88+3 | 88+3 | 89+3 | 89+3 | 89+3 | 89+3 | 89+3 | 83+3 | 83+3 | 83+3 | 89+3 | 66+3 | 63+3 | 63+3 | 63+3 | 66+3 | 61+3 | 49+3 | 49+3 | 49+3 | 61+3 | . 5 192985 | https://sofifa.com/player/192985/kevin-de-bruyne/20/159586 | K. De Bruyne | Kevin De Bruyne | 28 | 1991-06-28 | 181 | 70 | Belgium | Manchester City | 91 | 91 | 90000000 | 370000 | CAM, CM | Right | 4 | 5 | 4 | High/High | Normal | Yes | 166500000.0 | #Dribbler, #Playmaker  , #Engine, #Distance Shooter, #Crosser, #Complete Midfielder | RCM | 17.0 | nan | 2015-08-30 | 2023.0 | RCM | 7.0 | 76.0 | 86.0 | 92.0 | 86.0 | 61.0 | 78.0 | nan | nan | nan | nan | nan | nan | Power Free-Kick, Avoids Using Weaker Foot, Dives Into Tackles (CPU AI Only), Leadership, Argues with Officials, Finesse Shot | 93 | 82 | 55 | 92 | 82 | 86 | 85 | 83 | 91 | 91 | 77 | 76 | 78 | 91 | 76 | 91 | 63 | 89 | 74 | 90 | 76 | 61 | 88 | 94 | 79 | 91 | 68 | 58 | 51 | 15 | 13 | 5 | 10 | 13 | 82+3 | 82+3 | 82+3 | 87+3 | 87+3 | 87+3 | 87+3 | 87+3 | 88+3 | 88+3 | 88+3 | 88+3 | 87+3 | 87+3 | 87+3 | 88+3 | 77+3 | 77+3 | 77+3 | 77+3 | 77+3 | 73+3 | 66+3 | 66+3 | 66+3 | 73+3 | . 6 192448 | https://sofifa.com/player/192448/marc-andre-ter-stegen/20/159586 | M. ter Stegen | Marc-André ter Stegen | 27 | 1992-04-30 | 187 | 85 | Germany | FC Barcelona | 90 | 93 | 67500000 | 250000 | GK | Right | 3 | 4 | 1 | Medium/Medium | Normal | Yes | 143400000.0 | nan | GK | 1.0 | nan | 2014-07-01 | 2022.0 | SUB | 22.0 | nan | nan | nan | nan | nan | nan | 88.0 | 85.0 | 88.0 | 90.0 | 45.0 | 88.0 | Swerve Pass, Acrobatic Clearance, Flair Passes | 18 | 14 | 11 | 61 | 14 | 21 | 18 | 12 | 63 | 30 | 38 | 50 | 37 | 86 | 43 | 66 | 79 | 35 | 78 | 10 | 43 | 22 | 11 | 70 | 25 | 70 | 25 | 13 | 10 | 88 | 85 | 88 | 88 | 90 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . 7 203376 | https://sofifa.com/player/203376/virgil-van-dijk/20/159586 | V. van Dijk | Virgil van Dijk | 27 | 1991-07-08 | 193 | 92 | Netherlands | Liverpool | 90 | 91 | 78000000 | 200000 | CB | Right | 3 | 3 | 2 | Medium/Medium | Normal | Yes | 150200000.0 | #Tackling , #Tactician , #Strength, #Complete Defender | LCB | 4.0 | nan | 2018-01-01 | 2023.0 | LCB | 4.0 | 77.0 | 60.0 | 70.0 | 71.0 | 90.0 | 86.0 | nan | nan | nan | nan | nan | nan | Diver, Avoids Using Weaker Foot, Leadership, Long Passer (CPU AI Only) | 53 | 52 | 86 | 78 | 45 | 70 | 60 | 70 | 81 | 76 | 74 | 79 | 61 | 88 | 53 | 81 | 90 | 75 | 92 | 64 | 82 | 89 | 47 | 65 | 62 | 89 | 91 | 92 | 85 | 13 | 10 | 13 | 11 | 11 | 69+3 | 69+3 | 69+3 | 67+3 | 69+3 | 69+3 | 69+3 | 67+3 | 69+3 | 69+3 | 69+3 | 69+3 | 74+3 | 74+3 | 74+3 | 69+3 | 79+3 | 83+3 | 83+3 | 83+3 | 79+3 | 81+3 | 87+3 | 87+3 | 87+3 | 81+3 | . 8 177003 | https://sofifa.com/player/177003/luka-modric/20/159586 | L. Modrić | Luka Modrić | 33 | 1985-09-09 | 172 | 66 | Croatia | Real Madrid | 90 | 90 | 45000000 | 340000 | CM | Right | 4 | 4 | 4 | High/High | Lean | Yes | 92300000.0 | #Dribbler, #Playmaker  , #Crosser, #Acrobat, #Complete Midfielder | RCM | 10.0 | nan | 2012-08-01 | 2020.0 | nan | nan | 74.0 | 76.0 | 89.0 | 89.0 | 72.0 | 66.0 | nan | nan | nan | nan | nan | nan | Argues with Officials, Finesse Shot, Speed Dribbler (CPU AI Only), Crowd Favourite | 86 | 72 | 55 | 92 | 76 | 87 | 85 | 78 | 88 | 92 | 77 | 71 | 92 | 89 | 93 | 79 | 68 | 85 | 58 | 82 | 62 | 82 | 79 | 91 | 82 | 92 | 68 | 76 | 71 | 13 | 9 | 7 | 14 | 9 | 77+3 | 77+3 | 77+3 | 84+3 | 83+3 | 83+3 | 83+3 | 84+3 | 86+3 | 86+3 | 86+3 | 85+3 | 87+3 | 87+3 | 87+3 | 85+3 | 81+3 | 81+3 | 81+3 | 81+3 | 81+3 | 79+3 | 72+3 | 72+3 | 72+3 | 79+3 | . 9 209331 | https://sofifa.com/player/209331/mohamed-salah/20/159586 | M. Salah | Mohamed Salah Ghaly | 27 | 1992-06-15 | 175 | 71 | Egypt | Liverpool | 90 | 90 | 80500000 | 240000 | RW, ST | Left | 3 | 3 | 4 | High/Medium | PLAYER_BODY_TYPE_25 | Yes | 148900000.0 | #Speedster, #Dribbler, #Acrobat, #Clinical Finisher, #Complete Forward | RW | 11.0 | nan | 2017-07-01 | 2023.0 | RW | 10.0 | 93.0 | 86.0 | 81.0 | 89.0 | 45.0 | 74.0 | nan | nan | nan | nan | nan | nan | Beat Offside Trap, Argues with Officials, Early Crosser, Speed Dribbler (CPU AI Only), Outside Foot Shot | 79 | 90 | 59 | 84 | 79 | 89 | 83 | 69 | 75 | 89 | 94 | 92 | 91 | 92 | 88 | 80 | 69 | 85 | 73 | 84 | 63 | 55 | 92 | 84 | 77 | 91 | 38 | 43 | 41 | 14 | 14 | 9 | 11 | 14 | 84+3 | 84+3 | 84+3 | 88+3 | 88+3 | 88+3 | 88+3 | 88+3 | 87+3 | 87+3 | 87+3 | 87+3 | 81+3 | 81+3 | 81+3 | 87+3 | 70+3 | 67+3 | 67+3 | 67+3 | 70+3 | 66+3 | 57+3 | 57+3 | 57+3 | 66+3 | . 📝📝 There are 244935 missing values in the dataset. . # missing values in every column of the dataset print(&#39; nMissing Values in every column of the data : n n&#39; ,&quot; &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) missing = data.isnull().sum().sort_values(ascending=False) missing = missing.head(50) missing = missing.to_frame() missing.columns = [&#39;missing_values&#39;] missing.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A15F86&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Missing Values in every column of the data : 👇🏻👇🏻👇🏻 . missing_values . loaned_from 17230 | . nation_jersey_number 17152 | . nation_position 17152 | . player_tags 16779 | . gk_diving 16242 | . gk_handling 16242 | . gk_kicking 16242 | . gk_reflexes 16242 | . gk_speed 16242 | . gk_positioning 16242 | . player_traits 10712 | . rb 2036 | . st 2036 | . ls 2036 | . dribbling 2036 | . shooting 2036 | . rcb 2036 | . pace 2036 | . lw 2036 | . passing 2036 | . physic 2036 | . rs 2036 | . defending 2036 | . lf 2036 | . rw 2036 | . cf 2036 | . cam 2036 | . ram 2036 | . lm 2036 | . lcm 2036 | . cm 2036 | . rcm 2036 | . lam 2036 | . rm 2036 | . lwb 2036 | . ldm 2036 | . cdm 2036 | . rdm 2036 | . rwb 2036 | . lb 2036 | . lcb 2036 | . cb 2036 | . rf 2036 | . release_clause_eur 1298 | . joined 1288 | . contract_valid_until 240 | . team_jersey_number 240 | . team_position 240 | . real_face 0 | . body_type 0 | . 📝📝 There are missing values in 48 columns of the dataset and no. of missing values in those 48 columns can be seen from the above. . 3. Finding Features with one value . # All the features with their unique values print(&#39; nUnique Values in each column of the data : n n&#39; ,&quot; t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) for column in data.columns: print(emoji.emojize(&quot;:arrow_right:&quot;, use_aliases=True) ,column ,emoji.emojize(&quot;:1234:&quot;, use_aliases=True) ,data[column].nunique()) . . Unique Values in each column of the data : 👇🏻👇🏻👇🏻 ➡ sofifa_id 🔢 18278 ➡ player_url 🔢 18278 ➡ short_name 🔢 17354 ➡ long_name 🔢 18218 ➡ age 🔢 27 ➡ dob 🔢 6142 ➡ height_cm 🔢 49 ➡ weight_kg 🔢 56 ➡ nationality 🔢 162 ➡ club 🔢 698 ➡ overall 🔢 47 ➡ potential 🔢 47 ➡ value_eur 🔢 214 ➡ wage_eur 🔢 141 ➡ player_positions 🔢 643 ➡ preferred_foot 🔢 2 ➡ international_reputation 🔢 5 ➡ weak_foot 🔢 5 ➡ skill_moves 🔢 5 ➡ work_rate 🔢 9 ➡ body_type 🔢 10 ➡ real_face 🔢 2 ➡ release_clause_eur 🔢 1224 ➡ player_tags 🔢 83 ➡ team_position 🔢 29 ➡ team_jersey_number 🔢 99 ➡ loaned_from 🔢 316 ➡ joined 🔢 1760 ➡ contract_valid_until 🔢 8 ➡ nation_position 🔢 26 ➡ nation_jersey_number 🔢 30 ➡ pace 🔢 70 ➡ shooting 🔢 79 ➡ passing 🔢 68 ➡ dribbling 🔢 71 ➡ defending 🔢 76 ➡ physic 🔢 61 ➡ gk_diving 🔢 47 ➡ gk_handling 🔢 47 ➡ gk_kicking 🔢 52 ➡ gk_reflexes 🔢 48 ➡ gk_speed 🔢 53 ➡ gk_positioning 🔢 50 ➡ player_traits 🔢 921 ➡ attacking_crossing 🔢 88 ➡ attacking_finishing 🔢 93 ➡ attacking_heading_accuracy 🔢 89 ➡ attacking_short_passing 🔢 84 ➡ attacking_volleys 🔢 87 ➡ skill_dribbling 🔢 92 ➡ skill_curve 🔢 89 ➡ skill_fk_accuracy 🔢 89 ➡ skill_long_passing 🔢 85 ➡ skill_ball_control 🔢 90 ➡ movement_acceleration 🔢 86 ➡ movement_sprint_speed 🔢 86 ➡ movement_agility 🔢 83 ➡ movement_reactions 🔢 72 ➡ movement_balance 🔢 81 ➡ power_shot_power 🔢 80 ➡ power_jumping 🔢 74 ➡ power_stamina 🔢 86 ➡ power_strength 🔢 75 ➡ power_long_shots 🔢 90 ➡ mentality_aggression 🔢 87 ➡ mentality_interceptions 🔢 89 ➡ mentality_positioning 🔢 94 ➡ mentality_vision 🔢 85 ➡ mentality_penalties 🔢 86 ➡ mentality_composure 🔢 85 ➡ defending_marking 🔢 92 ➡ defending_standing_tackle 🔢 88 ➡ defending_sliding_tackle 🔢 88 ➡ goalkeeping_diving 🔢 73 ➡ goalkeeping_handling 🔢 71 ➡ goalkeeping_kicking 🔢 81 ➡ goalkeeping_positioning 🔢 76 ➡ goalkeeping_reflexes 🔢 75 ➡ ls 🔢 94 ➡ st 🔢 94 ➡ rs 🔢 94 ➡ lw 🔢 107 ➡ lf 🔢 103 ➡ cf 🔢 103 ➡ rf 🔢 103 ➡ rw 🔢 107 ➡ lam 🔢 103 ➡ cam 🔢 103 ➡ ram 🔢 103 ➡ lm 🔢 100 ➡ lcm 🔢 88 ➡ cm 🔢 88 ➡ rcm 🔢 88 ➡ rm 🔢 100 ➡ lwb 🔢 98 ➡ ldm 🔢 98 ➡ cdm 🔢 98 ➡ rdm 🔢 98 ➡ rwb 🔢 98 ➡ lb 🔢 96 ➡ lcb 🔢 110 ➡ cb 🔢 110 ➡ rcb 🔢 110 ➡ rb 🔢 96 . 📝📝 There is no feature having only 1 value. . 4. Inserting a new column &#39;positions&#39; based on the &#39;player_positions&#39; . # function for dividing the player positions into 4 main categories of positions def player_pos(row): positions = row[&#39;player_positions&#39;].split(&#39;, &#39;) N = len(positions) if N &lt; 3: pos = positions[0] if pos in [&#39;ST&#39;, &#39;LW&#39;, &#39;RW&#39;,&#39;CF&#39;]: return 0 # Attacker elif pos in [&#39;CAM&#39;, &#39;LM&#39;, &#39;CM&#39;, &#39;RM&#39;, &#39;CDM&#39;]: return 1 # Midfielder elif pos in [&#39;LWB&#39;, &#39;RWB&#39;, &#39;LB&#39;, &#39;CB&#39;, &#39;RB&#39;]: return 2 # Defender elif pos in [&#39;GK&#39;]: return 3 # Goalkeeper else: position_counter = [0, 0, 0, 0] for pos in positions: if pos in [&#39;ST&#39;, &#39;LW&#39;, &#39;RW&#39;,&#39;CF&#39;]: index = 0 # Attacker elif pos in [&#39;CAM&#39;, &#39;LM&#39;, &#39;CM&#39;, &#39;RM&#39;, &#39;CDM&#39;]: index = 1 # Midfielder elif pos in [&#39;LWB&#39;, &#39;RWB&#39;, &#39;LB&#39;, &#39;CB&#39;, &#39;RB&#39;]: index = 2 # Defender elif pos in [&#39;GK&#39;]: index = 3 # Goalkeeper else: continue position_counter[index] += 1 return position_counter.index(max(position_counter)) . . # creating a new column and applying the above function on it data[&#39;positions&#39;] = data.apply(player_pos, axis=1) # replacing the int values with corresponding positions data.replace({&#39;positions&#39; : { 0 : &#39;Attacker&#39; , 1 : &#39;Midfielder&#39; , 2 : &#39;Defender&#39; , 3 : &#39;Goalkeeper&#39; }},inplace=True) def color_red(val): if val==&#39;Attacker&#39;: color = &#39;brown&#39; elif val==&#39;Defender&#39;: color = &#39;darkgreen&#39; elif val==&#39;Midfielder&#39;: color = &#39;orange&#39; else: color = &#39;darkblue&#39; return &#39;color: %s&#39; % color print(&#39; nNew Column added as &quot;postions&quot; n n&#39; ,&quot; &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data[[&#39;positions&#39;]].head(50).style.applymap(color_red).set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#035753&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . New Column added as &#34;postions&#34; 👇🏻👇🏻👇🏻 . positions . 0 Attacker | . 1 Attacker | . 2 Attacker | . 3 Goalkeeper | . 4 Attacker | . 5 Midfielder | . 6 Goalkeeper | . 7 Defender | . 8 Midfielder | . 9 Attacker | . 10 Attacker | . 11 Defender | . 12 Attacker | . 13 Goalkeeper | . 14 Goalkeeper | . 15 Midfielder | . 16 Defender | . 17 Attacker | . 18 Defender | . 19 Attacker | . 20 Attacker | . 21 Midfielder | . 22 Attacker | . 23 Midfielder | . 24 Midfielder | . 25 Goalkeeper | . 26 Attacker | . 27 Midfielder | . 28 Goalkeeper | . 29 Defender | . 30 Goalkeeper | . 31 Goalkeeper | . 32 Goalkeeper | . 33 Midfielder | . 34 Attacker | . 35 Defender | . 36 Midfielder | . 37 Midfielder | . 38 Attacker | . 39 Attacker | . 40 Defender | . 41 Midfielder | . 42 Midfielder | . 43 Attacker | . 44 Midfielder | . 45 Defender | . 46 Attacker | . 47 Defender | . 48 Attacker | . 49 Defender | . Exploratory Data Analysis . Exploring numerical features . #list of numerical variables numerical_features = [feature for feature in data.columns if (data[feature].dtypes!=&#39;O&#39;)] print(&#39;No. of numerical variables&#39; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;)*2 ,len(numerical_features)) print(&quot;&quot;) #all the numerical variables for feature in numerical_features: print(&quot;The variable&quot; ,emoji.emojize(&quot;:memo:&quot;) ,&quot;{}&quot;.format(feature) ,&quot;has datatype&quot; ,emoji.emojize(&quot;:1234:&quot;, use_aliases=True) ,&quot;{}&quot;.format(data[feature].dtypes) ,&quot;and&quot; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;) ,&quot;{}&quot;.format(len(data[feature].unique())) ,&quot;unique values&quot;) . . No. of numerical variables 👉🏻👉🏻 61 The variable 📝 sofifa_id has datatype 🔢 int64 and 👉🏻 18278 unique values The variable 📝 age has datatype 🔢 int64 and 👉🏻 27 unique values The variable 📝 height_cm has datatype 🔢 int64 and 👉🏻 49 unique values The variable 📝 weight_kg has datatype 🔢 int64 and 👉🏻 56 unique values The variable 📝 overall has datatype 🔢 int64 and 👉🏻 47 unique values The variable 📝 potential has datatype 🔢 int64 and 👉🏻 47 unique values The variable 📝 value_eur has datatype 🔢 int64 and 👉🏻 214 unique values The variable 📝 wage_eur has datatype 🔢 int64 and 👉🏻 141 unique values The variable 📝 international_reputation has datatype 🔢 int64 and 👉🏻 5 unique values The variable 📝 weak_foot has datatype 🔢 int64 and 👉🏻 5 unique values The variable 📝 skill_moves has datatype 🔢 int64 and 👉🏻 5 unique values The variable 📝 release_clause_eur has datatype 🔢 float64 and 👉🏻 1225 unique values The variable 📝 team_jersey_number has datatype 🔢 float64 and 👉🏻 100 unique values The variable 📝 contract_valid_until has datatype 🔢 float64 and 👉🏻 9 unique values The variable 📝 nation_jersey_number has datatype 🔢 float64 and 👉🏻 31 unique values The variable 📝 pace has datatype 🔢 float64 and 👉🏻 71 unique values The variable 📝 shooting has datatype 🔢 float64 and 👉🏻 80 unique values The variable 📝 passing has datatype 🔢 float64 and 👉🏻 69 unique values The variable 📝 dribbling has datatype 🔢 float64 and 👉🏻 72 unique values The variable 📝 defending has datatype 🔢 float64 and 👉🏻 77 unique values The variable 📝 physic has datatype 🔢 float64 and 👉🏻 62 unique values The variable 📝 gk_diving has datatype 🔢 float64 and 👉🏻 48 unique values The variable 📝 gk_handling has datatype 🔢 float64 and 👉🏻 48 unique values The variable 📝 gk_kicking has datatype 🔢 float64 and 👉🏻 53 unique values The variable 📝 gk_reflexes has datatype 🔢 float64 and 👉🏻 49 unique values The variable 📝 gk_speed has datatype 🔢 float64 and 👉🏻 54 unique values The variable 📝 gk_positioning has datatype 🔢 float64 and 👉🏻 51 unique values The variable 📝 attacking_crossing has datatype 🔢 int64 and 👉🏻 88 unique values The variable 📝 attacking_finishing has datatype 🔢 int64 and 👉🏻 93 unique values The variable 📝 attacking_heading_accuracy has datatype 🔢 int64 and 👉🏻 89 unique values The variable 📝 attacking_short_passing has datatype 🔢 int64 and 👉🏻 84 unique values The variable 📝 attacking_volleys has datatype 🔢 int64 and 👉🏻 87 unique values The variable 📝 skill_dribbling has datatype 🔢 int64 and 👉🏻 92 unique values The variable 📝 skill_curve has datatype 🔢 int64 and 👉🏻 89 unique values The variable 📝 skill_fk_accuracy has datatype 🔢 int64 and 👉🏻 89 unique values The variable 📝 skill_long_passing has datatype 🔢 int64 and 👉🏻 85 unique values The variable 📝 skill_ball_control has datatype 🔢 int64 and 👉🏻 90 unique values The variable 📝 movement_acceleration has datatype 🔢 int64 and 👉🏻 86 unique values The variable 📝 movement_sprint_speed has datatype 🔢 int64 and 👉🏻 86 unique values The variable 📝 movement_agility has datatype 🔢 int64 and 👉🏻 83 unique values The variable 📝 movement_reactions has datatype 🔢 int64 and 👉🏻 72 unique values The variable 📝 movement_balance has datatype 🔢 int64 and 👉🏻 81 unique values The variable 📝 power_shot_power has datatype 🔢 int64 and 👉🏻 80 unique values The variable 📝 power_jumping has datatype 🔢 int64 and 👉🏻 74 unique values The variable 📝 power_stamina has datatype 🔢 int64 and 👉🏻 86 unique values The variable 📝 power_strength has datatype 🔢 int64 and 👉🏻 75 unique values The variable 📝 power_long_shots has datatype 🔢 int64 and 👉🏻 90 unique values The variable 📝 mentality_aggression has datatype 🔢 int64 and 👉🏻 87 unique values The variable 📝 mentality_interceptions has datatype 🔢 int64 and 👉🏻 89 unique values The variable 📝 mentality_positioning has datatype 🔢 int64 and 👉🏻 94 unique values The variable 📝 mentality_vision has datatype 🔢 int64 and 👉🏻 85 unique values The variable 📝 mentality_penalties has datatype 🔢 int64 and 👉🏻 86 unique values The variable 📝 mentality_composure has datatype 🔢 int64 and 👉🏻 85 unique values The variable 📝 defending_marking has datatype 🔢 int64 and 👉🏻 92 unique values The variable 📝 defending_standing_tackle has datatype 🔢 int64 and 👉🏻 88 unique values The variable 📝 defending_sliding_tackle has datatype 🔢 int64 and 👉🏻 88 unique values The variable 📝 goalkeeping_diving has datatype 🔢 int64 and 👉🏻 73 unique values The variable 📝 goalkeeping_handling has datatype 🔢 int64 and 👉🏻 71 unique values The variable 📝 goalkeeping_kicking has datatype 🔢 int64 and 👉🏻 81 unique values The variable 📝 goalkeeping_positioning has datatype 🔢 int64 and 👉🏻 76 unique values The variable 📝 goalkeeping_reflexes has datatype 🔢 int64 and 👉🏻 75 unique values . Exploring categorical features. . # displaying each categorical feature with its unique no. of categories categorical_features = [feature for feature in data.columns if (data[feature].dtypes==&#39;O&#39;)] print(&#39;No. of categorical variables&#39; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;)*2 ,len(categorical_features)) print(&quot;&quot;) for feature in categorical_features: print(&quot;The variable&quot; ,emoji.emojize(&quot;:abcd:&quot;, use_aliases=True) ,&quot;&#39;{}&#39;&quot;.format(feature) ,&quot;has&quot; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;) ,&quot;{}&quot;.format(len(data[feature].unique())) ,&quot;unique values&quot;) . . No. of categorical variables 👉🏻👉🏻 44 The variable 🔡 &#39;player_url&#39; has 👉🏻 18278 unique values The variable 🔡 &#39;short_name&#39; has 👉🏻 17354 unique values The variable 🔡 &#39;long_name&#39; has 👉🏻 18218 unique values The variable 🔡 &#39;dob&#39; has 👉🏻 6142 unique values The variable 🔡 &#39;nationality&#39; has 👉🏻 162 unique values The variable 🔡 &#39;club&#39; has 👉🏻 698 unique values The variable 🔡 &#39;player_positions&#39; has 👉🏻 643 unique values The variable 🔡 &#39;preferred_foot&#39; has 👉🏻 2 unique values The variable 🔡 &#39;work_rate&#39; has 👉🏻 9 unique values The variable 🔡 &#39;body_type&#39; has 👉🏻 10 unique values The variable 🔡 &#39;real_face&#39; has 👉🏻 2 unique values The variable 🔡 &#39;player_tags&#39; has 👉🏻 84 unique values The variable 🔡 &#39;team_position&#39; has 👉🏻 30 unique values The variable 🔡 &#39;loaned_from&#39; has 👉🏻 317 unique values The variable 🔡 &#39;joined&#39; has 👉🏻 1761 unique values The variable 🔡 &#39;nation_position&#39; has 👉🏻 27 unique values The variable 🔡 &#39;player_traits&#39; has 👉🏻 922 unique values The variable 🔡 &#39;ls&#39; has 👉🏻 95 unique values The variable 🔡 &#39;st&#39; has 👉🏻 95 unique values The variable 🔡 &#39;rs&#39; has 👉🏻 95 unique values The variable 🔡 &#39;lw&#39; has 👉🏻 108 unique values The variable 🔡 &#39;lf&#39; has 👉🏻 104 unique values The variable 🔡 &#39;cf&#39; has 👉🏻 104 unique values The variable 🔡 &#39;rf&#39; has 👉🏻 104 unique values The variable 🔡 &#39;rw&#39; has 👉🏻 108 unique values The variable 🔡 &#39;lam&#39; has 👉🏻 104 unique values The variable 🔡 &#39;cam&#39; has 👉🏻 104 unique values The variable 🔡 &#39;ram&#39; has 👉🏻 104 unique values The variable 🔡 &#39;lm&#39; has 👉🏻 101 unique values The variable 🔡 &#39;lcm&#39; has 👉🏻 89 unique values The variable 🔡 &#39;cm&#39; has 👉🏻 89 unique values The variable 🔡 &#39;rcm&#39; has 👉🏻 89 unique values The variable 🔡 &#39;rm&#39; has 👉🏻 101 unique values The variable 🔡 &#39;lwb&#39; has 👉🏻 99 unique values The variable 🔡 &#39;ldm&#39; has 👉🏻 99 unique values The variable 🔡 &#39;cdm&#39; has 👉🏻 99 unique values The variable 🔡 &#39;rdm&#39; has 👉🏻 99 unique values The variable 🔡 &#39;rwb&#39; has 👉🏻 99 unique values The variable 🔡 &#39;lb&#39; has 👉🏻 97 unique values The variable 🔡 &#39;lcb&#39; has 👉🏻 111 unique values The variable 🔡 &#39;cb&#39; has 👉🏻 111 unique values The variable 🔡 &#39;rcb&#39; has 👉🏻 111 unique values The variable 🔡 &#39;rb&#39; has 👉🏻 97 unique values The variable 🔡 &#39;positions&#39; has 👉🏻 4 unique values . # for annotating patches on the bars of the plots def patches(plot, feature,r): &quot;&quot;&quot; Takes plot, feature &amp; rotation as input and plots annotation for the plot &quot;&quot;&quot; total = len(feature) for p in plot.patches: percentage = &#39;{:.1f}%&#39;.format(100 * p.get_height()/total) x = p.get_x() + p.get_width() / 2. y = p.get_y() + p.get_height() + 20 ax.annotate(percentage, (x, y),ha=&#39;center&#39;, size = 12,rotation=r) plt.show() . . Comparison of preferred foot over the different players . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Preferred Foot vs Players &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) plt.figure(figsize = (18, 8)) ax = sns.countplot(data[&#39;positions&#39;],hue = data[&#39;preferred_foot&#39;], palette = &#39;pink&#39;) plt.title(&#39;Most Preferred Foot of the Players&#39;, fontsize = 20) plt.xlabel(&#39;Preferred Foot&#39;, fontsize = 16) plt.ylabel(&#39;Count of the Players&#39;, fontsize = 16) patches(ax, data.positions,0) . . 📈📈📈📈📈 Distribution of Preferred Foot vs Players 📈📈📈📈📈 . 📝📝 The preffered foot by the most players is Right. . Representation of share of international reputation . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of International Reputation vs Players &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize = (18, 8)) ax = sns.countplot(data[&#39;international_reputation&#39;], palette = &#39;Greens&#39;) plt.title(&#39;International Repuatation for the Football Players&#39;, fontsize = 20) plt.xlabel(&#39;International Reputation&#39;, fontsize = 16) plt.ylabel(&#39;Count of the Players&#39;, fontsize = 16) patches(ax, data.international_reputation,0) . . 📈📈📈📈📈 Distribution of International Reputation vs Players 📈📈📈📈📈 . 📝📝 Around 92% of the players have international reputation as 1 and 6.2% have 2, making it around 98% of the total no. of players. . Different positions acquired by the players . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Positions vs Players &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize = (18, 8)) ax = sns.countplot(&#39;positions&#39;, data = data, palette = &#39;pastel&#39;) plt.xlabel(&#39;Different Positions in Football&#39;, fontsize = 16) plt.ylabel(&#39;Count of Players&#39;, fontsize = 16) plt.title(&#39;Comparison of Positions and Players&#39;, fontsize = 20) patches(ax, data.positions,0) . . 📈📈📈📈📈 Distribution of Positions vs Players 📈📈📈📈📈 . 📝📝 Our dataset is divided into 4 major categories of player postions in which 19% of the players are Attackers, around 38% of the players are Midfielders, 32% are Defenders and 11% are Goalkeepers. . Comparing the players&#39; Wages . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Wage vs Players &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize = (18, 8)) ax = sns.histplot(data[&#39;wage_eur&#39;], color = &#39;skyblue&#39;,bins=10) plt.xlabel(&#39;Wage Range for Players&#39;, fontsize = 16) plt.ylabel(&#39;Count of the Players&#39;, fontsize = 16) plt.title(&#39;Distribution of Wages of Players&#39;, fontsize = 20) plt.xticks(rotation = 90) patches(ax, data.wage_eur,0) . . 📈📈📈📈📈 Distribution of Wage vs Players 📈📈📈📈📈 . 📝📝 98% of the players have wages less than 0.1 million Euros . Player with highest Wage . top_wage_player = data.loc[data.wage_eur == data[&#39;wage_eur&#39;].max(),&#39;short_name&#39;] print(&#39;The player with the highest wage is&#39;, top_wage_player.values[0]) . . The player with the highest wage is L. Messi . Comparing the players&#39; Valuation . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Valuation vs Players &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize = (18, 8)) ax = sns.histplot(data[&#39;value_eur&#39;], color = &#39;pink&#39;,bins=10) plt.xlabel(&#39;Value Range for Players&#39;, fontsize = 16) plt.ylabel(&#39;Count of the Players&#39;, fontsize = 16) plt.title(&#39;Distribution of Value of Players&#39;, fontsize = 20) plt.ticklabel_format(axis=&quot;x&quot;, style=&#39;plain&#39;) patches(ax, data.value_eur,0) . . 📈📈📈📈📈 Distribution of Valuation vs Players 📈📈📈📈📈 . 📝📝 95% of the players have valuation under 10 million Euros and 3% players have more valuation between 10-20 million Euros . Most expensive player . most_expensive_player = data.loc[data.value_eur == data[&#39;value_eur&#39;].max(),&#39;short_name&#39;] print(&#39;The most expensive player is&#39;, most_expensive_player.values[0]) . . The most expensive player is Neymar Jr . Age vs Valuations . # grouping the players on the basis of age data[&#39;age_group&#39;] = pd.cut(data[&#39;age&#39;], bins = [data[&#39;age&#39;].min(), 20, 25,30,35,40, data[&#39;age&#39;].max()], labels=[&#39;20 and Under&#39;, &#39;21 to 25&#39;, &#39;26 to 30&#39;,&#39;31 to 35&#39;,&#39;36 to 40&#39;,&#39;Over 40&#39;]) . . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Age vs Players &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize = (18, 8)) ax = sns.stripplot(x = data[&#39;age_group&#39;], y = data[&#39;value_eur&#39;], palette = &#39;Reds&#39;) plt.xlabel(&#39;Age Group of Players&#39;, fontsize = 16) plt.ylabel(&#39;Valuation in EUR&#39;, fontsize = 16) plt.title(&#39;Distribution of Valuation of players from different Age Groups&#39;, fontsize = 20) plt.ticklabel_format(axis=&quot;y&quot;, style=&#39;plain&#39;) plt.show() . . 📈📈📈📈📈 Distribution of Age vs Players 📈📈📈📈📈 . # grouping the players on the basis of age print(&#39; nGrouping the players on the basis of age : n n&#39; ,&quot; t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) players_count = data.groupby([&#39;age_group&#39;])[&#39;short_name&#39;].count() players_count.to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#139BB4&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Grouping the players on the basis of age : 👇🏻👇🏻👇🏻 . short_name . age_group . 20 and Under 3127 | . 21 to 25 6794 | . 26 to 30 5562 | . 31 to 35 2490 | . 36 to 40 287 | . Over 40 6 | . 📝📝 The valuation of the players increases with their age upto 30 years but after 30 years of age it starts decreasing. There are 5562 players who belong to the age group of 26-30 have the highest valuation. . Positions vs Valuations . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Valuation vs Positions &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize = (18, 8)) ax = sns.stripplot(x = data[&#39;positions&#39;], y = data[&#39;value_eur&#39;], palette = &#39;Blues&#39;) plt.xlabel(&#39;Positions of Players&#39;, fontsize = 16) plt.ylabel(&#39;Valuation in EUR&#39;, fontsize = 16) plt.title(&#39;Distribution of Valuation of players for different Positions&#39;, fontsize = 20) plt.ticklabel_format(axis=&quot;y&quot;, style=&#39;plain&#39;) plt.show() . . 📈📈📈📈📈 Distribution of Valuation vs Positions 📈📈📈📈📈 . # grouping the players on the basis of positions print(&#39; nGrouping the players on the basis of positions : n n&#39; ,&quot; t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) positions_count = data.groupby([&#39;positions&#39;])[&#39;short_name&#39;].count() positions_count.to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#436C3A&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Grouping the players on the basis of positions : 👇🏻👇🏻👇🏻 . short_name . positions . Attacker 3489 | . Defender 5878 | . Goalkeeper 2036 | . Midfielder 6875 | . 📝📝 Some of the Attackers and Midfielders are highly valued. . Different skill moves of players . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Skill Moves vs Players &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize = (18, 8)) ax = sns.countplot(x = &#39;skill_moves&#39;, data = data, palette = &#39;Oranges&#39;) plt.title(&#39;Count of players on Basis of their skill moves&#39;, fontsize = 20) plt.xlabel(&#39;Number of Skill Moves&#39;, fontsize = 16) plt.ylabel(&#39;Count of Players&#39;, fontsize = 16) patches(ax, data.skill_moves,0) . . 📈📈📈📈📈 Distribution of Skill Moves vs Players 📈📈📈📈📈 . 📝📝 47% of the players have 2 skill moves , around 36% of players have 3 skill moves. . Height of players . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Height vs Players &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize = (18, 8)) ax = sns.countplot(x = &#39;height_cm&#39;, data = data, palette = &#39;dark&#39;) plt.title(&#39;Count of players on Basis of Height&#39;, fontsize = 20) plt.xlabel(&#39;Height in cm&#39;, fontsize = 16) plt.ylabel(&#39;Count of Players&#39;, fontsize = 16) plt.xticks(rotation = 90) patches(ax, data.height_cm,90) . . 📈📈📈📈📈 Distribution of Height vs Players 📈📈📈📈📈 . 📝📝 Almost 70% of players have height between 175cm to 188 cm . . Body weight of players . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Weight vs Players &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize = (18, 8)) ax = sns.histplot(data[&#39;weight_kg&#39;],kde=True, color = &#39;pink&#39;) plt.title(&#39;Different Weights of the Players Participating in FIFA 2020&#39;, fontsize = 20) plt.xlabel(&#39;Heights associated with the players&#39;, fontsize = 16) plt.ylabel(&#39;Count of Players&#39;, fontsize = 16) patches(ax, data.weight_kg,90) . . 📈📈📈📈📈 Distribution of Weight vs Players 📈📈📈📈📈 . 📝📝 60% of players have weight in the bracket of 70-80kg. . Work Rate of players . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Work Rate vs Players &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize = (18, 8)) ax = sns.countplot(x = &#39;work_rate&#39;, data = data, palette = &#39;hls&#39;) plt.title(&#39;Different work rates of the Players Participating in the FIFA 2020&#39;, fontsize = 20) plt.xlabel(&#39;Work rates associated with the players&#39;, fontsize = 16) plt.ylabel(&#39;Count of Players&#39;, fontsize = 16) patches(ax, data.work_rate,0) . . 📈📈📈📈📈 Distribution of Work Rate vs Players 📈📈📈📈📈 . 📝📝 90% of the players have work rates as Medium/Medium. . Different potential scores of the players . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Potential vs Players &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize = (18, 8)) ax = sns.histplot(data.potential, kde=True,bins = 50) plt.xlabel(&quot;Player &#39;s Potential Scores&quot;, fontsize = 16) plt.ylabel(&#39;Count of Players&#39;, fontsize = 16) plt.title(&#39;Histogram of players Potential Scores&#39;, fontsize = 20) patches(ax, data.potential,90) . . 📈📈📈📈📈 Distribution of Potential vs Players 📈📈📈📈📈 . 📝📝 Between 60-80 potential scores, 90% of players data lie. . Different overall scores of the players . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Overall vs Players &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize = (18, 8)) ax = sns.histplot(data.overall,kde=True,bins = 50, color = &#39;m&#39;) plt.xlabel(&#39;Overall score range&#39;, fontsize = 16) plt.ylabel(&#39;Count of the Players&#39;,fontsize = 16) plt.title(&#39;Histogram for the Overall Scores of the Players&#39;, fontsize = 20) patches(ax, data.overall,90) . . 📈📈📈📈📈 Distribution of Overall vs Players 📈📈📈📈📈 . 📝📝 Between 50-80 overall scores, 90% of players data lie. . Age vs Overall Scores . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Overall vs Age &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) fig = plt.figure(figsize = (20, 10)) ax = sns.catplot(x = &#39;age&#39;, y = &#39;overall&#39;,col=&#39;positions&#39;, data = data,kind=&quot;box&quot;, palette = &#39;Reds&#39;) ax.set_axis_labels(x_var=&quot;Age of Players&quot;, y_var=&quot;Overall Scores&quot;) ax.set_xticklabels(rotation=90) #ax.fig.suptitle(&#39;Distribution of player Overall ratings vs Age &#39;,fontsize = 20) plt.show() . . 📈📈📈📈📈 Distribution of Overall vs Age 📈📈📈📈📈 . &lt;Figure size 1440x720 with 0 Axes&gt; . 📝📝 With increase in age , overall scores usually increase . . Different nations participating in FIFA 2020 . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Nations vs Players &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize = (18, 8)) ax = data[&#39;nationality&#39;].value_counts().head(100).plot.bar(color = &#39;orange&#39;, figsize = (20, 7)) plt.title(&#39;Different Nations Participating in FIFA 2020&#39;, fontsize = 30, fontweight = 20) plt.xlabel(&#39;Name of The Countries&#39;,fontsize = 16) plt.ylabel(&#39;Count of Players&#39;,fontsize = 16) patches(ax, data.nationality,90) . . 📈📈📈📈📈 Distribution of Nations vs Players 📈📈📈📈📈 . 📝📝 England , Germany , Spain , France , Argentina , Brazil , Italy , Colombia – these 8 countries consist of 45% of overall data. . Countries with most players . print(&#39; nNations with most players n n&#39; ,&quot; t &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) nationality = data[&#39;nationality&#39;].value_counts().head(50) nationality.to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#636A92&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Nations with most players 👇🏻👇🏻👇🏻 . nationality . England 1667 | . Germany 1216 | . Spain 1035 | . France 984 | . Argentina 886 | . Brazil 824 | . Italy 732 | . Colombia 591 | . Japan 453 | . Netherlands 416 | . China PR 373 | . Chile 370 | . Sweden 358 | . Norway 350 | . Republic of Ireland 348 | . United States 347 | . Denmark 345 | . Portugal 344 | . Mexico 340 | . Poland 324 | . Korea Republic 322 | . Austria 319 | . Saudi Arabia 310 | . Turkey 294 | . Romania 287 | . Scotland 277 | . Belgium 268 | . Switzerland 229 | . Australia 196 | . Uruguay 164 | . Serbia 139 | . Ghana 130 | . Senegal 127 | . Croatia 126 | . Nigeria 126 | . Wales 117 | . Ivory Coast 105 | . Czech Republic 102 | . Greece 96 | . Morocco 94 | . Russia 81 | . Northern Ireland 81 | . Paraguay 80 | . Cameroon 78 | . South Africa 72 | . Finland 72 | . Ukraine 69 | . Venezuela 66 | . Bosnia Herzegovina 66 | . Canada 61 | . 📝📝 Most of the players are from England, then Germany and then followed by Spain. . Every nation&#39;s player and their weights . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Nations vs Weight &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) top_10_countries = (&#39;England&#39;, &#39;Germany&#39;, &#39;Spain&#39;, &#39;Argentina&#39;, &#39;France&#39;, &#39;Brazil&#39;, &#39;Italy&#39;, &#39;Columbia&#39;,&#39;Colombia&#39;,&#39;Japan&#39;,&#39;Netherlands&#39;) data_countries = data.loc[data[&#39;nationality&#39;].isin(top_10_countries) &amp; data[&#39;weight_kg&#39;]] plt.figure(figsize = (18, 8)) ax = sns.boxplot(x = data_countries[&#39;nationality&#39;], y = data_countries[&#39;weight_kg&#39;], palette = &#39;Reds&#39;) ax.set_xlabel(xlabel = &#39;Countries&#39;, fontsize = 16) ax.set_ylabel(ylabel = &#39;Weight in kg&#39;, fontsize = 16) ax.set_title(label = &#39;Distribution of Weight of players from different countries&#39;, fontsize = 20) plt.show() . . 📈📈📈📈📈 Distribution of Nations vs Weight 📈📈📈📈📈 . 📝📝 Mostly European countries have avg player weight at around 77 Kg , however for Asian countries like Japan it is close to 72 Kg. . Different Clubs associated with players . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Clubs vs Players &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize = (18, 8)) ax = data[&#39;club&#39;].value_counts().head(100).plot.bar(color = &#39;white&#39;) plt.title(&#39;Different clubs associated with players&#39;, fontsize = 30, fontweight = 20) plt.xlabel(&#39;Name of The Clubs&#39;,fontsize = 16) plt.ylabel(&#39;Count of Players&#39;,fontsize = 16) plt.show() . . 📈📈📈📈📈 Distribution of Clubs vs Players 📈📈📈📈📈 . 📝📝 Some of the clubs have 33 players, few have 32 players, few have 31 players and most of the clubs have 30 players. . Popular clubs . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Distribution of Overall vs Popular Clubs &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) some_clubs = (&#39;CD Leganés&#39;, &#39;Southampton&#39;, &#39;RC Celta&#39;, &#39;Empoli&#39;, &#39;Fortuna Düsseldorf&#39;, &#39;Manchestar City&#39;, &#39;Tottenham Hotspur&#39;, &#39;FC Barcelona&#39;, &#39;Valencia CF&#39;, &#39;Chelsea&#39;, &#39;Real Madrid&#39;) data_clubs = data.loc[data[&#39;club&#39;].isin(some_clubs) &amp; data[&#39;overall&#39;]] plt.figure(figsize = (18, 8)) ax = sns.boxplot(x = data_clubs[&#39;club&#39;], y = data_clubs[&#39;overall&#39;], palette = &#39;inferno&#39;) ax.set_xlabel(xlabel = &#39;Some Popular Clubs&#39;, fontsize = 16) ax.set_ylabel(ylabel = &#39;Overall Score&#39;, fontsize = 16) ax.set_title(label = &#39;Distribution of Overall Score in Different popular Clubs&#39;, fontsize = 20) plt.xticks(rotation = 90) plt.show() . . 📈📈📈📈📈 Distribution of Overall vs Popular Clubs 📈📈📈📈📈 . 📝📝 The average overall score of the popular clubs is between 70-85. . Clubs which pay highest wage . print(&#39; nNo. of Clubs&#39; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;) ,data.club.nunique()) print(&quot;&quot;) print(&#39; nClubs n&#39; ,&quot; n&quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3 ,&quot; n&quot; ,&quot; n &quot;,data.club.unique()) . . No. of Clubs 👉🏻 698 Clubs 👇🏻👇🏻👇🏻 [&#39;FC Barcelona&#39; &#39;Juventus&#39; &#39;Paris Saint-Germain&#39; &#39;Atlético Madrid&#39; &#39;Real Madrid&#39; &#39;Manchester City&#39; &#39;Liverpool&#39; &#39;Napoli&#39; &#39;Tottenham Hotspur&#39; &#39;Manchester United&#39; &#39;Chelsea&#39; &#39;FC Bayern München&#39; &#39;Inter&#39; &#39;Borussia Dortmund&#39; &#39;Arsenal&#39; &#39;Valencia CF&#39; &#39;Lazio&#39; &#39;Milan&#39; &#39;Sporting CP&#39; &#39;Olympique Lyonnais&#39; &#39;RB Leipzig&#39; &#39;Ajax&#39; &#39;LA Galaxy&#39; &#39;Atalanta&#39; &#39;RC Celta&#39; &#39;Bayer 04 Leverkusen&#39; &#39;Real Betis&#39; &#39;FC Porto&#39; &#39;SV Werder Bremen&#39; &#39;West Ham United&#39; &#39;Wolverhampton Wanderers&#39; &#39;AS Saint-Étienne&#39; &#39;Torino&#39; &#39;Dalian YiFang FC&#39; &#39;Borussia Mönchengladbach&#39; &#39;Roma&#39; &#39;Guangzhou Evergrande Taobao FC&#39; &#39;SL Benfica&#39; &#39;Medipol Başakşehir FK&#39; &#39;Everton&#39; &#39;VfL Wolfsburg&#39; &#39;Crystal Palace&#39; &#39;Getafe CF&#39; &#39;Shanghai SIPG FC&#39; &#39;Eintracht Frankfurt&#39; &#39;Olympique de Marseille&#39; &#39;Hertha BSC&#39; &#39;RSC Anderlecht&#39; &#39;Villarreal CF&#39; &#39;Sampdoria&#39; &#39;Leicester City&#39; &#39;AS Monaco&#39; &#39;Jiangsu Suning FC&#39; &#39;Los Angeles FC&#39; &#39;Cagliari&#39; &#39;Sevilla FC&#39; &#39;Fenerbahçe SK&#39; &#39;Real Sociedad&#39; &#39;TSG 1899 Hoffenheim&#39; &#39;Atlético Mineiro&#39; &#39;Grêmio&#39; &#39;PSV&#39; &#39;Athletic Club de Bilbao&#39; &#39;Deportivo Alavés&#39; &#39;Boca Juniors&#39; &#39;Lokomotiv Moscow&#39; &#39;Al Nassr&#39; &#39;Brescia&#39; &#39;Shakhtar Donetsk&#39; &#39;Shanghai Greenland Shenhua FC&#39; &#39;PFC CSKA Moscow&#39; &#39;Beijing Sinobo Guoan FC&#39; &#39;Levante UD&#39; &#39;Cruzeiro&#39; &#39;Uruguay&#39; &#39;Montpellier HSC&#39; &#39;Atlanta United&#39; &#39;Watford&#39; &#39;1. FC Köln&#39; &#39;Bournemouth&#39; &#39;Beşiktaş JK&#39; &#39;Real Valladolid CF&#39; &#39;Racing Club&#39; &#39;Al Hilal&#39; &#39;Guangzhou R&amp;F FC&#39; &#39;Sassuolo&#39; &#39;FC Girondins de Bordeaux&#39; &#39;LOSC Lille&#39; &#39;Galatasaray SK&#39; &#39;Chicago Fire&#39; &#39;Fluminense&#39; &#39;Ecuador&#39; &#39;RCD Espanyol&#39; &#39;Dinamo Zagreb&#39; &#39;FC Nantes&#39; &#39;River Plate&#39; &#39;OGC Nice&#39; &#39;Newcastle United&#39; &#39;Brighton &amp; Hove Albion&#39; &#39;Club Brugge KV&#39; &#39;FC Schalke 04&#39; &#39;SD Eibar&#39; &#39;DC United&#39; &#39;Orlando City SC&#39; &#39;Hebei China Fortune FC&#39; &#39;Tigres U.A.N.L.&#39; &#39;Aston Villa&#39; &#39;Montreal Impact&#39; &#39;Olympiacos CFP&#39; &#39;Norwich City&#39; &#39;Feyenoord&#39; &#39;Toronto FC&#39; &#39;KRC Genk&#39; &#39;Fiorentina&#39; &#39;Spartak Moscow&#39; &#39;Dynamo Kyiv&#39; &#39;SK Slavia Praha&#39; &#39;Southampton&#39; &#39;Burnley&#39; &#39;SC Braga&#39; &#39;Russia&#39; &#39;RC Strasbourg Alsace&#39; &#39;Wuhan Zall&#39; &#39;Vissel Kobe&#39; &#39;Portland Timbers&#39; &#39;Genoa&#39; &#39;Beijing Renhe FC&#39; &#39;Toulouse Football Club&#39; &#39;Girona FC&#39; &#39;Real Zaragoza&#39; &#39;CD Leganés&#39; &#39;Shenzhen FC&#39; &#39;Internacional&#39; &#39;CSA - AL&#39; &#39;Santos&#39; &#39;1. FSV Mainz 05&#39; &#39;Stoke City&#39; &#39;Udinese&#39; &#39;Colombia&#39; &#39;Angers SCO&#39; &#39;FC Augsburg&#39; &#39;Netherlands&#39; &#39;Fulham&#39; &#39;FC København&#39; &#39;KAA Gent&#39; &#39;SC Freiburg&#39; &#39;Stade Rennais FC&#39; &#39;Club América&#39; &#39;Trabzonspor&#39; &#39;BSC Young Boys&#39; &#39;Helsingborgs IF&#39; &#39;Kaizer Chiefs&#39; &#39;Parma&#39; &#39;Mexico&#39; &#39;Royal Antwerp FC&#39; &#39;Tianjin TEDA FC&#39; &#39;Hannover 96&#39; &#39;Tianjin Quanjian FC&#39; &#39;Al Ahli&#39; &#39;Bologna&#39; &#39;VfB Stuttgart&#39; &#39;Seattle Sounders FC&#39; &#39;Godoy Cruz&#39; &#39;Sparta Praha&#39; &#39;Independiente Medellín&#39; &#39;Sivasspor&#39; &#39;Independiente&#39; &#39;Nîmes Olympique&#39; &#39;Club Tijuana&#39; &#39;SPAL&#39; &#39;Monterrey&#39; &#39;CD Tondela&#39; &#39;Fortuna Düsseldorf&#39; &#39;Vitória Guimarães&#39; &#39;AZ Alkmaar&#39; &#39;Atlético de San Luis&#39; &#39;PAOK&#39; &#39;Nagoya Grampus&#39; &#39;Club Atlético Banfield&#39; &#39;Shandong Luneng TaiShan FC&#39; &#39;New York Red Bulls&#39; &#39;FC Red Bull Salzburg&#39; &#39;Sweden&#39; &#39;Amiens SC&#39; &#39;Hellas Verona&#39; &#39;Cruz Azul&#39; &#39;Gençlerbirliği SK&#39; &#39;1. FC Union Berlin&#39; &#39;Standard de Liège&#39; &#39;Chongqing Dangdai Lifan FC SWM Team&#39; &#39;New England Revolution&#39; &#39;Club Atlético Colón&#39; &#39;Celtic&#39; &#39;Club Atlas&#39; &#39;Botafogo&#39; &#39;En Avant de Guingamp&#39; &#39;West Bromwich Albion&#39; &#39;Pachuca&#39; &#39;AEK Athens&#39; &#39;Portimonense SC&#39; &#39;Real Salt Lake&#39; &#39;FC Utrecht&#39; &#39;Sheffield United&#39; &#34;Newell&#39;s Old Boys&#34; &#39;Club Atlético Talleres&#39; &#39;Philadelphia Union&#39; &#39;Rosenborg BK&#39; &#39;FC Basel 1893&#39; &#39;Brentford&#39; &#39;Club León&#39; &#39;Unión de Santa Fe&#39; &#39;Deportivo de La Coruña&#39; &#39;Rangers FC&#39; &#39;Turkey&#39; &#39;Atiker Konyaspor&#39; &#39;Granada CF&#39; &#39;Perth Glory&#39; &#39;Club Atlético Lanús&#39; &#39;Hamburger SV&#39; &#39;Al Ittihad&#39; &#39;Santos Laguna&#39; &#39;Western United FC&#39; &#39;Columbus Crew SC&#39; &#39;Deportivo Toluca&#39; &#39;Cardiff City&#39; &#39;CA Osasuna&#39; &#39;Swansea City&#39; &#39;Melbourne Victory&#39; &#39;Leeds United&#39; &#39;Göztepe SK&#39; &#39;Hungary&#39; &#39;New York City FC&#39; &#39;Bulgaria&#39; &#39;Kayserispor&#39; &#39;Minnesota United FC&#39; &#39;Guadalajara&#39; &#39;FC Groningen&#39; &#39;Paraguay&#39; &#39;Junior FC&#39; &#39;Al Taawoun&#39; &#39;Huddersfield Town&#39; &#39;Ettifaq FC&#39; &#39;Stade de Reims&#39; &#39;Rayo Vallecano&#39; &#39;San Lorenzo de Almagro&#39; &#39;Bahia&#39; &#39;Atlético Paranaense&#39; &#39;Goiás&#39; &#39;Avaí FC&#39; &#39;Fortaleza&#39; &#39;Kawasaki Frontale&#39; &#39;Vélez Sarsfield&#39; &#39;Hull City&#39; &#39;Houston Dynamo&#39; &#39;Birmingham City&#39; &#39;Al Wehda&#39; &#39;Nottingham Forest&#39; &#39;CD Tenerife&#39; &#39;Aalborg BK&#39; &#39;Preston North End&#39; &#39;Bristol City&#39; &#39;Lecce&#39; &#39;Gimnasia y Esgrima La Plata&#39; &#39;CD Aves&#39; &#39;Viktoria Plzeň&#39; &#39;1. FC Nürnberg&#39; &#39;Slovenia&#39; &#39;FC Metz&#39; &#39;FC Midtjylland&#39; &#39;Molde FK&#39; &#39;Colo-Colo&#39; &#39;RCD Mallorca&#39; &#39;FC Sion&#39; &#39;Wisła Kraków&#39; &#39;Denizlispor&#39; &#39;Middlesbrough&#39; &#39;Universidad Católica&#39; &#39;Alanyaspor&#39; &#39;Moreirense FC&#39; &#34;Côte d&#39;Ivoire&#34; &#39;Stade Brestois 29&#39; &#39;Chievo Verona&#39; &#39;Boavista FC&#39; &#39;MKE Ankaragücü&#39; &#39;Sydney FC&#39; &#39;Al Ain FC&#39; &#39;Yeni Malatyaspor&#39; &#39;Urawa Red Diamonds&#39; &#39;Querétaro&#39; &#39;Sporting Kansas City&#39; &#39;Málaga CF&#39; &#39;1. FC Heidenheim 1846&#39; &#39;Atlético Tucumán&#39; &#39;Gazişehir Gaziantep F.K.&#39; &#39;Clube Sport Marítimo&#39; &#39;Chapecoense&#39; &#39;Atlético Nacional&#39; &#39;Vitesse&#39; &#39;Australia&#39; &#39;Henan Jianye FC&#39; &#39;Panathinaikos FC&#39; &#39;Blackburn Rovers&#39; &#39;Santa Clara&#39; &#39;Cameroon&#39; &#39;Puebla FC&#39; &#39;U.N.A.M.&#39; &#39;SD Huesca&#39; &#39;Estudiantes de La Plata&#39; &#39;Austria&#39; &#39;KAS Eupen&#39; &#39;LASK Linz&#39; &#39;Sporting de Charleroi&#39; &#39;Daegu FC&#39; &#39;Real Sporting de Gijón&#39; &#39;Derby County&#39; &#39;Rio Ave FC&#39; &#39;South Africa&#39; &#39;Famalicão&#39; &#39;Neuchâtel Xamax&#39; &#39;Benevento&#39; &#39;Cádiz CF&#39; &#39;AIK&#39; &#39;Sheffield Wednesday&#39; &#39;Empoli&#39; &#39;Colorado Rapids&#39; &#39;Os Belenenses&#39; &#39;Unión Magdalena&#39; &#39;Real Oviedo&#39; &#39;Peru&#39; &#39;Antalyaspor&#39; &#39;Dijon FCO&#39; &#39;SV Sandhausen&#39; &#39;Rosario Central&#39; &#39;Reading&#39; &#39;DSC Arminia Bielefeld&#39; &#39;Malmö FF&#39; &#39;Jeonbuk Hyundai Motors&#39; &#39;Frosinone&#39; &#39;FC Tokyo&#39; &#39;Canada&#39; &#39;Çaykur Rizespor&#39; &#39;FCSB (Steaua)&#39; &#39;Defensa y Justicia&#39; &#39;Monarcas Morelia&#39; &#39;Club Atlético Huracán&#39; &#39;Ceará Sporting Club&#39; &#39;Argentinos Juniors&#39; &#39;Al Shabab&#39; &#39;Legia Warszawa&#39; &#39;Shimizu S-Pulse&#39; &#39;Millonarios FC&#39; &#39;Lechia Gdańsk&#39; &#39;Brøndby IF&#39; &#39;Albacete BP&#39; &#39;FC Lorient&#39; &#39;Universitatea Craiova&#39; &#39;Deportivo Cali&#39; &#39;SK Rapid Wien&#39; &#39;Kashima Antlers&#39; &#39;Poland&#39; &#39;Elche CF&#39; &#39;Club Atlético Aldosivi&#39; &#39;Deportes Tolima&#39; &#39;Cúcuta Deportivo&#39; &#39;Club Necaxa&#39; &#39;Piast Gliwice&#39; &#39;Pescara&#39; &#39;Kasimpaşa SK&#39; &#39;Egypt&#39; &#39;Holstein Kiel&#39; &#39;Livorno&#39; &#39;Coquimbo Unido&#39; &#39;UD Las Palmas&#39; &#39;Górnik Zabrze&#39; &#39;FC Twente&#39; &#39;Paris FC&#39; &#39;Racing Club de Lens&#39; &#39;VfL Bochum 1848&#39; &#39;Sunderland&#39; &#39;Aberdeen&#39; &#39;Heart of Midlothian&#39; &#39;Romania&#39; &#39;Crotone&#39; &#39;Iceland&#39; &#39;CFR Cluj&#39; &#39;FK Austria Wien&#39; &#39;UD Almería&#39; &#39;SK Brann&#39; &#39;BK Häcken&#39; &#39;Al Fateh&#39; &#39;CD Everton de Viña del Mar&#39; &#39;FC St. Pauli&#39; &#39;Gamba Osaka&#39; &#39;Yokohama F. Marinos&#39; &#39;Djurgårdens IF&#39; &#39;FC Dallas&#39; &#39;Sagan Tosu&#39; &#39;Al Fayha&#39; &#39;Chile&#39; &#39;Once Caldas&#39; &#39;Atlético Bucaramanga&#39; &#39;FC Cincinnati&#39; &#39;San Jose Earthquakes&#39; &#39;América de Cali&#39; &#39;Perugia&#39; &#39;La Equidad&#39; &#39;SV Darmstadt 98&#39; &#39;FC Zürich&#39; &#39;SC Paderborn 07&#39; &#39;SV Zulte-Waregem&#39; &#39;IFK Norrköping&#39; &#39;FC Viitorul&#39; &#39;Pordenone&#39; &#39;AD Alcorcón&#39; &#39;Sint-Truidense VV&#39; &#39;KV Kortrijk&#39; &#39;Royal Excel Mouscron&#39; &#39;FC Lugano&#39; &#39;Hokkaido Consadole Sapporo&#39; &#39;Spezia&#39; &#39;FC Paços de Ferreira&#39; &#39;Servette FC&#39; &#39;US Salernitana 1919&#39; &#39;Lech Poznań&#39; &#39;ESTAC Troyes&#39; &#39;Fortuna Sittard&#39; &#39;KV Oostende&#39; &#39;VVV-Venlo&#39; &#39;KSV Cercle Brugge&#39; &#39;FC Seoul&#39; &#39;Cerezo Osaka&#39; &#39;Stade Malherbe Caen&#39; &#39;Universidad de Chile&#39; &#39;Kalmar FF&#39; &#39;KV Mechelen&#39; &#39;Deportes Iquique&#39; &#39;US Cremonese&#39; &#39;Bolivia&#39; &#39;Valenciennes FC&#39; &#39;Ulsan Hyundai FC&#39; &#39;Vegalta Sendai&#39; &#39;ADO Den Haag&#39; &#39;CD Palestino&#39; &#39;Cittadella&#39; &#39;Vitória de Setúbal&#39; &#39;FC Nordsjælland&#39; &#39;Charlton Athletic&#39; &#39;Al Raed&#39; &#39;Jagiellonia Białystok&#39; &#39;Odense Boldklub&#39; &#39;FC Thun&#39; &#39;SG Dynamo Dresden&#39; &#39;Hammarby IF&#39; &#39;Central Córdoba&#39; &#39;Queens Park Rangers&#39; &#39; SSV Jahn Regensburg&#39; &#39;Tiburones Rojos de Veracruz&#39; &#39;Patronato&#39; &#39;Virtus Entella&#39; &#39;SK Sturm Graz&#39; &#39;Kilmarnock&#39; &#39;Extremadura UD&#39; &#39;Willem II&#39; &#39;Gil Vicente FC&#39; &#39;Randers FC&#39; &#39;Ascoli&#39; &#39;Vancouver Whitecaps FC&#39; &#39;Millwall&#39; &#39;Pohang Steelers&#39; &#39;Suwon Samsung Bluewings&#39; &#39;Heracles Almelo&#39; &#39;Al Hazem&#39; &#39;FC Juárez&#39; &#39;SC Heerenveen&#39; &#39;Dinamo Bucureşti&#39; &#39;Gyeongnam FC&#39; &#39;KFC Uerdingen 05&#39; &#39;PEC Zwolle&#39; &#39;Al Faisaly&#39; &#39;Arsenal de Sarandí&#39; &#39;FC Ingolstadt 04&#39; &#39;Wolfsberger AC&#39; &#39;Wigan Athletic&#39; &#39;Júbilo Iwata&#39; &#39;CD Lugo&#39; &#39;SpVgg Greuther Fürth&#39; &#39;FC Emmen&#39; &#39;Cosenza&#39; &#39;AJ Auxerre&#39; &#39;Cracovia&#39; &#39;FC Erzgebirge Aue&#39; &#39;Grenoble Foot 38&#39; &#39;Korona Kielce&#39; &#39;Alianza Petrolera&#39; &#39;CD Universidad de Concepción&#39; &#39;Melbourne City FC&#39; &#39;CD Antofagasta&#39; &#39;Independiente Santa Fe&#39; &#39;Sanfrecce Hiroshima&#39; &#39;Damac FC&#39; &#39;Chamois Niortais Football Club&#39; &#39;Atlético Huila&#39; &#39;CD Numancia&#39; &#39;Luton Town&#39; &#39;Unión La Calera&#39; &#39;Le Havre AC&#39; &#39;AS Nancy Lorraine&#39; &#39;Audax Italiano&#39; &#39;Hibernian&#39; &#39;Barnsley&#39; &#39;Pogoń Szczecin&#39; &#39;Sarpsborg 08 FF&#39; &#39;Kristiansund BK&#39; &#39;FC Luzern&#39; &#39;Unión Española&#39; &#39;IFK Göteborg&#39; &#39;Envigado FC&#39; &#39;Jeju United FC&#39; &#39;Patriotas Boyacá FC&#39; &#39;Peterborough United&#39; &#39;Deportivo Pasto&#39; &#39;Wales&#39; &#39;Clermont Foot 63&#39; &#39;Northern Ireland&#39; &#39;Astra Giurgiu&#39; &#39;Racing Santander&#39; &#39;Pisa&#39; &#39;Newcastle Jets&#39; &#39;Castellammare di Stabia&#39; &#39;Rionegro Águilas&#39; &#39;Vålerenga Fotball&#39; &#34;CD O&#39;Higgins&#34; &#39;AC Ajaccio&#39; &#39;CF Fuenlabrada&#39; &#39;VfL Osnabrück&#39; &#39;Arka Gdynia&#39; &#39;FC Sochaux-Montbéliard&#39; &#39;SpVgg Unterhaching&#39; &#39;Central Coast Mariners&#39; &#39;SV Wehen Wiesbaden&#39; &#39;Fleetwood Town&#39; &#39;Venezia FC&#39; &#39;Karlsruher SC&#39; &#39;Wisła Płock&#39; &#39;Gangwon FC&#39; &#39;IF Elfsborg&#39; &#39;Sangju Sangmu FC&#39; &#39;Zagłębie Lubin&#39; &#39;Abha Club&#39; &#39;Jaguares de Córdoba&#39; &#39;FC Botoşani&#39; &#39;Motherwell&#39; &#39;FC St. Gallen&#39; &#39;Bayern München II&#39; &#39;CD Mirandés&#39; &#39;St. Johnstone FC&#39; &#39;Lillestrøm SK&#39; &#39;Odds BK&#39; &#39;SD Ponferradina&#39; &#39;Doncaster Rovers&#39; &#39;Örebro SK&#39; &#39;Shamrock Rovers&#39; &#39;FSV Zwickau&#39; &#39;1. FC Kaiserslautern&#39; &#39;Portsmouth&#39; &#39;Waasland-Beveren&#39; &#39;Ipswich Town&#39; &#39;Dundalk&#39; &#39;CD Huachipato&#39; &#39;Le Mans FC&#39; &#39;Strømsgodset IF&#39; &#39;Sepsi OSK&#39; &#39;Sparta Rotterdam&#39; &#39;FC Admira Wacker Mödling&#39; &#39;SønderjyskE&#39; &#39;Hallescher FC&#39; &#39;SKN St. Pölten&#39; &#39;MSV Duisburg&#39; &#39;La Berrichonne de Châteauroux&#39; &#39;FC Hansa Rostock&#39; &#39;Incheon United FC&#39; &#39;Shonan Bellmare&#39; &#39;Orlando Pirates&#39; &#39;FK Bodø/Glimt&#39; &#39;Lincoln City&#39; &#39;Brisbane Roar&#39; &#39;Eintracht Braunschweig&#39; &#39;Curicó Unido&#39; &#39;Falkenbergs FF&#39; &#39;Venezuela&#39; &#39;1. FC Magdeburg&#39; &#39;CD Cobresal&#39; &#39;Aarhus GF&#39; &#39;Salford City&#39; &#39;HJK Helsinki&#39; &#39;Oxford United&#39; &#39;Hobro IK&#39; &#39;Esbjerg fB&#39; &#39;Bolton Wanderers&#39; &#39;SV Mattersburg&#39; &#39;Rotherham United&#39; &#39;SCR Altach&#39; &#39;Gaz Metan Mediaş&#39; &#39;Tromsø IL&#39; &#39;WSG Tirol&#39; &#39;IK Sirius&#39; &#39;Shrewsbury&#39; &#39;Oldham Athletic&#39; &#39;Southend United&#39; &#39;Blackpool&#39; &#39;Coventry City&#39; &#39;Adelaide United&#39; &#39;Ranheim Fotball&#39; &#39;SG Sonnenhof Großaspach&#39; &#39;Matsumoto Yamaga&#39; &#39;Burton Albion&#39; &#39;FC Hermannstadt&#39; &#39;TSV 1860 München&#39; &#39;LKS Lodz&#39; &#39;FC Würzburger Kickers&#39; &#39;TSV Hartberg&#39; &#39;Politehnica Iaşi&#39; &#39;Wellington Phoenix&#39; &#39;Milton Keynes Dons&#39; &#39;Stabæk Fotball&#39; &#39;Wycombe Wanderers&#39; &#39;SV Waldhof Mannheim&#39; &#39;AC Horsens&#39; &#39;Scunthorpe United&#39; &#39;RKC Waalwijk&#39; &#39;Oita Trinita&#39; &#39;St. Mirren&#39; &#39;Bristol Rovers&#39; &#39;Rodez Aveyron Football&#39; &#39;SV Meppen&#39; &#39;Viking FK&#39; &#39;Östersunds FK&#39; &#39;FK Haugesund&#39; &#39;Rochdale&#39; &#39;Colchester United&#39; &#39;Trapani&#39; &#39;Stevenage&#39; &#39;Bradford City&#39; &#39;Livingston FC&#39; &#39;FC Chambly Oise&#39; &#39;Mansfield Town&#39; &#39;SC Preußen Münster&#39; &#39;FC Voluntari&#39; &#39;Accrington Stanley&#39; &#39;Al Adalah&#39; &#39;Lyngby BK&#39; &#39;FC Carl Zeiss Jena&#39; &#39;Viktoria Köln&#39; &#39;Tranmere Rovers&#39; &#39;Silkeborg IF&#39; &#39;Gillingham&#39; &#39;Plymouth Argyle&#39; &#39;Chemnitzer FC&#39; &#39;Mjøndalen IF&#39; &#39;Walsall&#39; &#39;Northampton Town&#39; &#39;Hamilton Academical FC&#39; &#39;Grimsby Town&#39; &#39;Exeter City&#39; &#39;Swindon Town&#39; &#39;Raków Częstochowa&#39; &#39;Chindia Târgovişte&#39; &#39;US Orléans Loiret Football&#39; &#39;Forest Green Rovers&#39; &#39;AFC Wimbledon&#39; &#39;Carlisle United&#39; &#39;Morecambe&#39; &#39;Port Vale&#39; &#39;Cheltenham Town&#39; &#39;Academica Clinceni&#39; &#39;Crawley Town&#39; &#39;Ross County FC&#39; &#39;AFC Eskilstuna&#39; &#39;Macclesfield Town&#39; &#39;Cork City&#39; &#39;Newport County&#39; &#39;Crewe Alexandra&#39; &#39;Leyton Orient&#39; &#39;Cambridge United&#39; &#34;St. Patrick&#39;s Athletic&#34; &#39;Bohemian FC&#39; &#39;India&#39; &#39;Finland&#39; &#39;Waterford FC&#39; &#39;Derry City&#39; &#39;Bury&#39; &#39;New Zealand&#39; &#39;GIF Sundsvall&#39; &#39;Sligo Rovers&#39; &#39;Finn Harps&#39; &#39;Seongnam FC&#39; &#39;UCD AFC&#39; &#39;Śląsk Wrocław&#39;] . 📝📝 There are 698 different clubs. . clubs = data.groupby([&#39;club&#39;]) . . print(&quot; nNations with their maximum wage n n&quot; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) max_wage = clubs[[&#39;wage_eur&#39;,&#39;nationality&#39;]].max() max_wage.head(100).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#053975&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Nations with their maximum wage 👇🏻👇🏻👇🏻 . wage_eur nationality . club . SSV Jahn Regensburg 12000 | Portugal | . 1. FC Heidenheim 1846 15000 | Germany | . 1. FC Kaiserslautern 3000 | Namibia | . 1. FC Köln 36000 | United States | . 1. FC Magdeburg 4000 | Ghana | . 1. FC Nürnberg 15000 | Sweden | . 1. FC Union Berlin 35000 | Turkey | . 1. FSV Mainz 05 30000 | Switzerland | . AC Ajaccio 4000 | United States | . AC Horsens 4000 | United States | . AD Alcorcón 8000 | Venezuela | . ADO Den Haag 8000 | Suriname | . AEK Athens 1000 | Ukraine | . AFC Eskilstuna 1000 | Ukraine | . AFC Wimbledon 3000 | Wales | . AIK 10000 | Sweden | . AJ Auxerre 6000 | Serbia | . AS Monaco 67000 | Switzerland | . AS Nancy Lorraine 4000 | Senegal | . AS Saint-Étienne 43000 | Tunisia | . AZ Alkmaar 12000 | Norway | . Aalborg BK 13000 | Sweden | . Aarhus GF 7000 | Sweden | . Aberdeen 6000 | Wales | . Abha Club 11000 | Tunisia | . Academica Clinceni 2000 | Slovakia | . Accrington Stanley 3000 | Republic of Ireland | . Adelaide United 2000 | Norway | . Ajax 39000 | United States | . Al Adalah 8000 | Tunisia | . Al Ahli 47000 | Syria | . Al Ain FC 1000 | United Arab Emirates | . Al Faisaly 17000 | Trinidad &amp; Tobago | . Al Fateh 21000 | Uruguay | . Al Fayha 20000 | Saudi Arabia | . Al Hazem 10000 | Saudi Arabia | . Al Hilal 56000 | Syria | . Al Ittihad 39000 | Serbia | . Al Nassr 59000 | Saudi Arabia | . Al Raed 18000 | Saudi Arabia | . Al Shabab 29000 | Tunisia | . Al Taawoun 27000 | Saudi Arabia | . Al Wehda 25000 | Turkey | . Alanyaspor 16000 | Turkey | . Albacete BP 9000 | Uruguay | . Alianza Petrolera 2000 | Guatemala | . Amiens SC 20000 | Sweden | . América de Cali 3000 | Colombia | . Angers SCO 23000 | Spain | . Antalyaspor 16000 | Turkey | . Argentinos Juniors 12000 | Uruguay | . Arka Gdynia 3000 | Spain | . Arsenal 205000 | Wales | . Arsenal de Sarandí 8000 | United States | . Ascoli 4000 | Tanzania | . Aston Villa 61000 | Zimbabwe | . Astra Giurgiu 6000 | Romania | . Atalanta 92000 | Ukraine | . Athletic Club de Bilbao 36000 | Spain | . Atiker Konyaspor 18000 | Ukraine | . Atlanta United 14000 | Venezuela | . Atlético Bucaramanga 2000 | Uruguay | . Atlético Huila 1000 | Venezuela | . Atlético Madrid 125000 | Venezuela | . Atlético Mineiro 60000 | Brazil | . Atlético Nacional 5000 | Uruguay | . Atlético Paranaense 15000 | Brazil | . Atlético Tucumán 13000 | Switzerland | . Atlético de San Luis 16000 | Uruguay | . Audax Italiano 4000 | Venezuela | . Australia 0 | Australia | . Austria 0 | Austria | . Avaí FC 16000 | Brazil | . BK Häcken 6000 | Sweden | . BSC Young Boys 26000 | Switzerland | . Bahia 13000 | Brazil | . Barnsley 7000 | Wales | . Bayer 04 Leverkusen 89000 | Poland | . Bayern München II 3000 | United States | . Beijing Renhe FC 19000 | Senegal | . Beijing Sinobo Guoan FC 34000 | Korea Republic | . Benevento 5000 | Sweden | . Beşiktaş JK 80000 | United States | . Birmingham City 13000 | Sweden | . Blackburn Rovers 20000 | Republic of Ireland | . Blackpool 4000 | Tanzania | . Boavista FC 7000 | Spain | . Boca Juniors 31000 | Venezuela | . Bohemian FC 1000 | Scotland | . Bolivia 0 | Bolivia | . Bologna 36000 | Uruguay | . Bolton Wanderers 3000 | Romania | . Borussia Dortmund 170000 | Switzerland | . Borussia Mönchengladbach 41000 | United States | . Botafogo 26000 | Brazil | . Bournemouth 81000 | Wales | . Bradford City 8000 | Wales | . Brentford 40000 | Sweden | . Brescia 27000 | Venezuela | . Brighton &amp; Hove Albion 53000 | Spain | . print(&quot; nClubs with their maximum wage n n&quot; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) high_wage_clubs = max_wage[&#39;wage_eur&#39;].sort_values(ascending=False).head(50) high_wage_clubs.to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#261655&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Clubs with their maximum wage 👇🏻👇🏻👇🏻 . wage_eur . club . FC Barcelona 565000 | . Real Madrid 470000 | . Juventus 405000 | . Manchester City 370000 | . Paris Saint-Germain 290000 | . Manchester United 250000 | . Liverpool 240000 | . Chelsea 235000 | . FC Bayern München 235000 | . Tottenham Hotspur 220000 | . Arsenal 205000 | . Borussia Dortmund 170000 | . Napoli 150000 | . Inter 135000 | . Atlético Madrid 125000 | . West Ham United 125000 | . Wolverhampton Wanderers 125000 | . Everton 120000 | . Leicester City 115000 | . Olympique Lyonnais 110000 | . Lazio 105000 | . Fenerbahçe SK 105000 | . Atalanta 92000 | . Crystal Palace 89000 | . Bayer 04 Leverkusen 89000 | . Tigres U.A.N.L. 88000 | . Bournemouth 81000 | . Beşiktaş JK 80000 | . Watford 78000 | . Galatasaray SK 77000 | . RB Leipzig 77000 | . Roma 75000 | . Valencia CF 69000 | . Medipol Başakşehir FK 67000 | . AS Monaco 67000 | . Fiorentina 63000 | . Torino 63000 | . VfL Wolfsburg 62000 | . TSG 1899 Hoffenheim 61000 | . Aston Villa 61000 | . Atlético Mineiro 60000 | . Olympique de Marseille 60000 | . Al Nassr 59000 | . Leeds United 57000 | . Al Hilal 56000 | . Milan 56000 | . Brighton &amp; Hove Albion 53000 | . Norwich City 53000 | . Southampton 53000 | . Celtic 52000 | . 📝📝 The highest wage is given by FC Barcelona followed by Real Madrid and Juventus. . Data Preprocessing . # selecting only player&#39;s name and all numeric features data = data[[&#39;short_name&#39;,&#39;positions&#39;,&#39;nationality&#39;,&#39;club&#39;,&#39;age&#39;, &#39;height_cm&#39;, &#39;weight_kg&#39;, &#39;overall&#39;, &#39;potential&#39;, &#39;value_eur&#39;, &#39;wage_eur&#39;, &#39;international_reputation&#39;, &#39;weak_foot&#39;, &#39;skill_moves&#39;, &#39;release_clause_eur&#39;, &#39;contract_valid_until&#39;, &#39;pace&#39;, &#39;shooting&#39;, &#39;passing&#39;, &#39;dribbling&#39;, &#39;defending&#39;, &#39;physic&#39;, &#39;gk_diving&#39;, &#39;gk_handling&#39;, &#39;gk_kicking&#39;, &#39;gk_reflexes&#39;, &#39;gk_speed&#39;, &#39;gk_positioning&#39;, &#39;attacking_crossing&#39;, &#39;attacking_finishing&#39;, &#39;attacking_heading_accuracy&#39;, &#39;attacking_short_passing&#39;, &#39;attacking_volleys&#39;, &#39;skill_dribbling&#39;, &#39;skill_curve&#39;, &#39;skill_fk_accuracy&#39;, &#39;skill_long_passing&#39;, &#39;skill_ball_control&#39;, &#39;movement_acceleration&#39;, &#39;movement_sprint_speed&#39;, &#39;movement_agility&#39;, &#39;movement_reactions&#39;, &#39;movement_balance&#39;, &#39;power_shot_power&#39;, &#39;power_jumping&#39;, &#39;power_stamina&#39;, &#39;power_strength&#39;, &#39;power_long_shots&#39;, &#39;mentality_aggression&#39;, &#39;mentality_interceptions&#39;, &#39;mentality_positioning&#39;, &#39;mentality_vision&#39;, &#39;mentality_penalties&#39;, &#39;mentality_composure&#39;, &#39;defending_marking&#39;, &#39;defending_standing_tackle&#39;, &#39;defending_sliding_tackle&#39;, &#39;goalkeeping_diving&#39;, &#39;goalkeeping_handling&#39;, &#39;goalkeeping_kicking&#39;, &#39;goalkeeping_positioning&#39;, &#39;goalkeeping_reflexes&#39;]] . . 📝📝 We have selected 62 columns out of which 58 are numerical and 4 are categorical. . # Exctracting players whose overall&gt;80 data = data[data.overall &gt; 80] data.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#C70039&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).highlight_null(null_color=&#39;#CCB3C5&#39;).highlight_max(color=&#39;#0074FF&#39;,axis=0).highlight_min(color=&#39;#00FFE5&#39;,axis=0) . . short_name positions nationality club age height_cm weight_kg overall potential value_eur wage_eur international_reputation weak_foot skill_moves release_clause_eur contract_valid_until pace shooting passing dribbling defending physic gk_diving gk_handling gk_kicking gk_reflexes gk_speed gk_positioning attacking_crossing attacking_finishing attacking_heading_accuracy attacking_short_passing attacking_volleys skill_dribbling skill_curve skill_fk_accuracy skill_long_passing skill_ball_control movement_acceleration movement_sprint_speed movement_agility movement_reactions movement_balance power_shot_power power_jumping power_stamina power_strength power_long_shots mentality_aggression mentality_interceptions mentality_positioning mentality_vision mentality_penalties mentality_composure defending_marking defending_standing_tackle defending_sliding_tackle goalkeeping_diving goalkeeping_handling goalkeeping_kicking goalkeeping_positioning goalkeeping_reflexes . 0 L. Messi | Attacker | Argentina | FC Barcelona | 32 | 170 | 72 | 94 | 94 | 95500000 | 565000 | 5 | 4 | 4 | 195800000.0 | 2021.0 | 87.0 | 92.0 | 92.0 | 96.0 | 39.0 | 66.0 | nan | nan | nan | nan | nan | nan | 88 | 95 | 70 | 92 | 88 | 97 | 93 | 94 | 92 | 96 | 91 | 84 | 93 | 95 | 95 | 86 | 68 | 75 | 68 | 94 | 48 | 40 | 94 | 94 | 75 | 96 | 33 | 37 | 26 | 6 | 11 | 15 | 14 | 8 | . 1 Cristiano Ronaldo | Attacker | Portugal | Juventus | 34 | 187 | 83 | 93 | 93 | 58500000 | 405000 | 5 | 4 | 5 | 96500000.0 | 2022.0 | 90.0 | 93.0 | 82.0 | 89.0 | 35.0 | 78.0 | nan | nan | nan | nan | nan | nan | 84 | 94 | 89 | 83 | 87 | 89 | 81 | 76 | 77 | 92 | 89 | 91 | 87 | 96 | 71 | 95 | 95 | 85 | 78 | 93 | 63 | 29 | 95 | 82 | 85 | 95 | 28 | 32 | 24 | 7 | 11 | 15 | 14 | 11 | . 2 Neymar Jr | Attacker | Brazil | Paris Saint-Germain | 27 | 175 | 68 | 92 | 92 | 105500000 | 290000 | 5 | 5 | 5 | 195200000.0 | 2022.0 | 91.0 | 85.0 | 87.0 | 95.0 | 32.0 | 58.0 | nan | nan | nan | nan | nan | nan | 87 | 87 | 62 | 87 | 87 | 96 | 88 | 87 | 81 | 95 | 94 | 89 | 96 | 92 | 84 | 80 | 61 | 81 | 49 | 84 | 51 | 36 | 87 | 90 | 90 | 94 | 27 | 26 | 29 | 9 | 9 | 15 | 15 | 11 | . 3 J. Oblak | Goalkeeper | Slovenia | Atlético Madrid | 26 | 188 | 87 | 91 | 93 | 77500000 | 125000 | 3 | 3 | 1 | 164700000.0 | 2023.0 | nan | nan | nan | nan | nan | nan | 87.0 | 92.0 | 78.0 | 89.0 | 52.0 | 90.0 | 13 | 11 | 15 | 43 | 13 | 12 | 13 | 14 | 40 | 30 | 43 | 60 | 67 | 88 | 49 | 59 | 78 | 41 | 78 | 12 | 34 | 19 | 11 | 65 | 11 | 68 | 27 | 12 | 18 | 87 | 92 | 78 | 90 | 89 | . 4 E. Hazard | Attacker | Belgium | Real Madrid | 28 | 175 | 74 | 91 | 91 | 90000000 | 470000 | 4 | 4 | 4 | 184500000.0 | 2024.0 | 91.0 | 83.0 | 86.0 | 94.0 | 35.0 | 66.0 | nan | nan | nan | nan | nan | nan | 81 | 84 | 61 | 89 | 83 | 95 | 83 | 79 | 83 | 94 | 94 | 88 | 95 | 90 | 94 | 82 | 56 | 84 | 63 | 80 | 54 | 41 | 87 | 89 | 88 | 91 | 34 | 27 | 22 | 11 | 12 | 6 | 8 | 8 | . 5 K. De Bruyne | Midfielder | Belgium | Manchester City | 28 | 181 | 70 | 91 | 91 | 90000000 | 370000 | 4 | 5 | 4 | 166500000.0 | 2023.0 | 76.0 | 86.0 | 92.0 | 86.0 | 61.0 | 78.0 | nan | nan | nan | nan | nan | nan | 93 | 82 | 55 | 92 | 82 | 86 | 85 | 83 | 91 | 91 | 77 | 76 | 78 | 91 | 76 | 91 | 63 | 89 | 74 | 90 | 76 | 61 | 88 | 94 | 79 | 91 | 68 | 58 | 51 | 15 | 13 | 5 | 10 | 13 | . 6 M. ter Stegen | Goalkeeper | Germany | FC Barcelona | 27 | 187 | 85 | 90 | 93 | 67500000 | 250000 | 3 | 4 | 1 | 143400000.0 | 2022.0 | nan | nan | nan | nan | nan | nan | 88.0 | 85.0 | 88.0 | 90.0 | 45.0 | 88.0 | 18 | 14 | 11 | 61 | 14 | 21 | 18 | 12 | 63 | 30 | 38 | 50 | 37 | 86 | 43 | 66 | 79 | 35 | 78 | 10 | 43 | 22 | 11 | 70 | 25 | 70 | 25 | 13 | 10 | 88 | 85 | 88 | 88 | 90 | . 7 V. van Dijk | Defender | Netherlands | Liverpool | 27 | 193 | 92 | 90 | 91 | 78000000 | 200000 | 3 | 3 | 2 | 150200000.0 | 2023.0 | 77.0 | 60.0 | 70.0 | 71.0 | 90.0 | 86.0 | nan | nan | nan | nan | nan | nan | 53 | 52 | 86 | 78 | 45 | 70 | 60 | 70 | 81 | 76 | 74 | 79 | 61 | 88 | 53 | 81 | 90 | 75 | 92 | 64 | 82 | 89 | 47 | 65 | 62 | 89 | 91 | 92 | 85 | 13 | 10 | 13 | 11 | 11 | . 8 L. Modrić | Midfielder | Croatia | Real Madrid | 33 | 172 | 66 | 90 | 90 | 45000000 | 340000 | 4 | 4 | 4 | 92300000.0 | 2020.0 | 74.0 | 76.0 | 89.0 | 89.0 | 72.0 | 66.0 | nan | nan | nan | nan | nan | nan | 86 | 72 | 55 | 92 | 76 | 87 | 85 | 78 | 88 | 92 | 77 | 71 | 92 | 89 | 93 | 79 | 68 | 85 | 58 | 82 | 62 | 82 | 79 | 91 | 82 | 92 | 68 | 76 | 71 | 13 | 9 | 7 | 14 | 9 | . 9 M. Salah | Attacker | Egypt | Liverpool | 27 | 175 | 71 | 90 | 90 | 80500000 | 240000 | 3 | 3 | 4 | 148900000.0 | 2023.0 | 93.0 | 86.0 | 81.0 | 89.0 | 45.0 | 74.0 | nan | nan | nan | nan | nan | nan | 79 | 90 | 59 | 84 | 79 | 89 | 83 | 69 | 75 | 89 | 94 | 92 | 91 | 92 | 88 | 80 | 69 | 85 | 73 | 84 | 63 | 55 | 92 | 84 | 77 | 91 | 38 | 43 | 41 | 14 | 14 | 9 | 11 | 14 | . 📝📝 We do not want to use 18000+ players to group so we have extracted players whose overall score is above 80 and these are 411 players. . # missing values print(&#39; nMissing Values in each column of the data : n n&#39; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) missing = data.isnull().sum().sort_values(ascending=False) missing = missing.head(50) missing = missing.to_frame() missing.columns = [&#39;missing_values&#39;] missing.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A15F86&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Missing Values in each column of the data : 👇🏻👇🏻👇🏻 . missing_values . gk_positioning 357 | . gk_speed 357 | . gk_reflexes 357 | . gk_kicking 357 | . gk_handling 357 | . gk_diving 357 | . physic 54 | . defending 54 | . dribbling 54 | . passing 54 | . shooting 54 | . pace 54 | . release_clause_eur 14 | . contract_valid_until 6 | . attacking_crossing 0 | . goalkeeping_reflexes 0 | . goalkeeping_positioning 0 | . skill_moves 0 | . weak_foot 0 | . international_reputation 0 | . wage_eur 0 | . value_eur 0 | . potential 0 | . overall 0 | . weight_kg 0 | . height_cm 0 | . age 0 | . club 0 | . nationality 0 | . positions 0 | . attacking_finishing 0 | . attacking_heading_accuracy 0 | . attacking_short_passing 0 | . attacking_volleys 0 | . goalkeeping_kicking 0 | . goalkeeping_handling 0 | . goalkeeping_diving 0 | . defending_sliding_tackle 0 | . defending_standing_tackle 0 | . defending_marking 0 | . mentality_composure 0 | . mentality_penalties 0 | . mentality_vision 0 | . mentality_positioning 0 | . mentality_interceptions 0 | . mentality_aggression 0 | . power_long_shots 0 | . power_strength 0 | . power_stamina 0 | . power_jumping 0 | . 📝📝 There are few missing values in the extracted data and are only in the numerical features. . # replacing null with the mean data = data.fillna(data.mean()) . . # after handling missing values print(&#39; nAfter Imputing Missing Values in the data : n n&#39; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) missing = data.isnull().sum().sort_values(ascending=False) missing = missing.head(50) missing = missing.to_frame() missing.columns = [&#39;missing_values&#39;] missing.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#932A06&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . After Imputing Missing Values in the data : 👇🏻👇🏻👇🏻 . missing_values . goalkeeping_reflexes 0 | . contract_valid_until 0 | . attacking_crossing 0 | . gk_positioning 0 | . gk_speed 0 | . gk_reflexes 0 | . gk_kicking 0 | . gk_handling 0 | . gk_diving 0 | . physic 0 | . defending 0 | . dribbling 0 | . passing 0 | . shooting 0 | . pace 0 | . release_clause_eur 0 | . goalkeeping_positioning 0 | . skill_moves 0 | . weak_foot 0 | . international_reputation 0 | . wage_eur 0 | . value_eur 0 | . potential 0 | . overall 0 | . weight_kg 0 | . height_cm 0 | . age 0 | . club 0 | . nationality 0 | . positions 0 | . attacking_finishing 0 | . attacking_heading_accuracy 0 | . attacking_short_passing 0 | . attacking_volleys 0 | . goalkeeping_kicking 0 | . goalkeeping_handling 0 | . goalkeeping_diving 0 | . defending_sliding_tackle 0 | . defending_standing_tackle 0 | . defending_marking 0 | . mentality_composure 0 | . mentality_penalties 0 | . mentality_vision 0 | . mentality_positioning 0 | . mentality_interceptions 0 | . mentality_aggression 0 | . power_long_shots 0 | . power_strength 0 | . power_stamina 0 | . power_jumping 0 | . 📝📝 We have repalced the missing values with the corresponding mean of the feature and now there are no missing values in the dataset. . # saving for later names = data.short_name.tolist() positions = data.positions.tolist() club = data.club.tolist() nationality = data.nationality.tolist() . . # dropping the short_name column data = data.drop([&#39;short_name&#39;,&#39;positions&#39;,&#39;club&#39;,&#39;nationality&#39;],axis=1) . . 📝📝 We have saved the categorical features in a list for later use and dropped from the dataset. . data.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#68524B&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).highlight_max(color=&#39;#0074FF&#39;,axis=0).highlight_min(color=&#39;#00FFE5&#39;,axis=0) . . age height_cm weight_kg overall potential value_eur wage_eur international_reputation weak_foot skill_moves release_clause_eur contract_valid_until pace shooting passing dribbling defending physic gk_diving gk_handling gk_kicking gk_reflexes gk_speed gk_positioning attacking_crossing attacking_finishing attacking_heading_accuracy attacking_short_passing attacking_volleys skill_dribbling skill_curve skill_fk_accuracy skill_long_passing skill_ball_control movement_acceleration movement_sprint_speed movement_agility movement_reactions movement_balance power_shot_power power_jumping power_stamina power_strength power_long_shots mentality_aggression mentality_interceptions mentality_positioning mentality_vision mentality_penalties mentality_composure defending_marking defending_standing_tackle defending_sliding_tackle goalkeeping_diving goalkeeping_handling goalkeeping_kicking goalkeeping_positioning goalkeeping_reflexes . 0 32 | 170 | 72 | 94 | 94 | 95500000 | 565000 | 5 | 4 | 4 | 195800000.0 | 2021.0 | 87.0 | 92.0 | 92.0 | 96.0 | 39.0 | 66.0 | 83.6 | 81.1 | 76.6 | 85.3 | 49.7 | 82.4 | 88 | 95 | 70 | 92 | 88 | 97 | 93 | 94 | 92 | 96 | 91 | 84 | 93 | 95 | 95 | 86 | 68 | 75 | 68 | 94 | 48 | 40 | 94 | 94 | 75 | 96 | 33 | 37 | 26 | 6 | 11 | 15 | 14 | 8 | . 1 34 | 187 | 83 | 93 | 93 | 58500000 | 405000 | 5 | 4 | 5 | 96500000.0 | 2022.0 | 90.0 | 93.0 | 82.0 | 89.0 | 35.0 | 78.0 | 83.6 | 81.1 | 76.6 | 85.3 | 49.7 | 82.4 | 84 | 94 | 89 | 83 | 87 | 89 | 81 | 76 | 77 | 92 | 89 | 91 | 87 | 96 | 71 | 95 | 95 | 85 | 78 | 93 | 63 | 29 | 95 | 82 | 85 | 95 | 28 | 32 | 24 | 7 | 11 | 15 | 14 | 11 | . 2 27 | 175 | 68 | 92 | 92 | 105500000 | 290000 | 5 | 5 | 5 | 195200000.0 | 2022.0 | 91.0 | 85.0 | 87.0 | 95.0 | 32.0 | 58.0 | 83.6 | 81.1 | 76.6 | 85.3 | 49.7 | 82.4 | 87 | 87 | 62 | 87 | 87 | 96 | 88 | 87 | 81 | 95 | 94 | 89 | 96 | 92 | 84 | 80 | 61 | 81 | 49 | 84 | 51 | 36 | 87 | 90 | 90 | 94 | 27 | 26 | 29 | 9 | 9 | 15 | 15 | 11 | . 3 26 | 188 | 87 | 91 | 93 | 77500000 | 125000 | 3 | 3 | 1 | 164700000.0 | 2023.0 | 73.8 | 70.3 | 74.8 | 78.5 | 62.5 | 73.1 | 87.0 | 92.0 | 78.0 | 89.0 | 52.0 | 90.0 | 13 | 11 | 15 | 43 | 13 | 12 | 13 | 14 | 40 | 30 | 43 | 60 | 67 | 88 | 49 | 59 | 78 | 41 | 78 | 12 | 34 | 19 | 11 | 65 | 11 | 68 | 27 | 12 | 18 | 87 | 92 | 78 | 90 | 89 | . 4 28 | 175 | 74 | 91 | 91 | 90000000 | 470000 | 4 | 4 | 4 | 184500000.0 | 2024.0 | 91.0 | 83.0 | 86.0 | 94.0 | 35.0 | 66.0 | 83.6 | 81.1 | 76.6 | 85.3 | 49.7 | 82.4 | 81 | 84 | 61 | 89 | 83 | 95 | 83 | 79 | 83 | 94 | 94 | 88 | 95 | 90 | 94 | 82 | 56 | 84 | 63 | 80 | 54 | 41 | 87 | 89 | 88 | 91 | 34 | 27 | 22 | 11 | 12 | 6 | 8 | 8 | . 5 28 | 181 | 70 | 91 | 91 | 90000000 | 370000 | 4 | 5 | 4 | 166500000.0 | 2023.0 | 76.0 | 86.0 | 92.0 | 86.0 | 61.0 | 78.0 | 83.6 | 81.1 | 76.6 | 85.3 | 49.7 | 82.4 | 93 | 82 | 55 | 92 | 82 | 86 | 85 | 83 | 91 | 91 | 77 | 76 | 78 | 91 | 76 | 91 | 63 | 89 | 74 | 90 | 76 | 61 | 88 | 94 | 79 | 91 | 68 | 58 | 51 | 15 | 13 | 5 | 10 | 13 | . 6 27 | 187 | 85 | 90 | 93 | 67500000 | 250000 | 3 | 4 | 1 | 143400000.0 | 2022.0 | 73.8 | 70.3 | 74.8 | 78.5 | 62.5 | 73.1 | 88.0 | 85.0 | 88.0 | 90.0 | 45.0 | 88.0 | 18 | 14 | 11 | 61 | 14 | 21 | 18 | 12 | 63 | 30 | 38 | 50 | 37 | 86 | 43 | 66 | 79 | 35 | 78 | 10 | 43 | 22 | 11 | 70 | 25 | 70 | 25 | 13 | 10 | 88 | 85 | 88 | 88 | 90 | . 7 27 | 193 | 92 | 90 | 91 | 78000000 | 200000 | 3 | 3 | 2 | 150200000.0 | 2023.0 | 77.0 | 60.0 | 70.0 | 71.0 | 90.0 | 86.0 | 83.6 | 81.1 | 76.6 | 85.3 | 49.7 | 82.4 | 53 | 52 | 86 | 78 | 45 | 70 | 60 | 70 | 81 | 76 | 74 | 79 | 61 | 88 | 53 | 81 | 90 | 75 | 92 | 64 | 82 | 89 | 47 | 65 | 62 | 89 | 91 | 92 | 85 | 13 | 10 | 13 | 11 | 11 | . 8 33 | 172 | 66 | 90 | 90 | 45000000 | 340000 | 4 | 4 | 4 | 92300000.0 | 2020.0 | 74.0 | 76.0 | 89.0 | 89.0 | 72.0 | 66.0 | 83.6 | 81.1 | 76.6 | 85.3 | 49.7 | 82.4 | 86 | 72 | 55 | 92 | 76 | 87 | 85 | 78 | 88 | 92 | 77 | 71 | 92 | 89 | 93 | 79 | 68 | 85 | 58 | 82 | 62 | 82 | 79 | 91 | 82 | 92 | 68 | 76 | 71 | 13 | 9 | 7 | 14 | 9 | . 9 27 | 175 | 71 | 90 | 90 | 80500000 | 240000 | 3 | 3 | 4 | 148900000.0 | 2023.0 | 93.0 | 86.0 | 81.0 | 89.0 | 45.0 | 74.0 | 83.6 | 81.1 | 76.6 | 85.3 | 49.7 | 82.4 | 79 | 90 | 59 | 84 | 79 | 89 | 83 | 69 | 75 | 89 | 94 | 92 | 91 | 92 | 88 | 80 | 69 | 85 | 73 | 84 | 63 | 55 | 92 | 84 | 77 | 91 | 38 | 43 | 41 | 14 | 14 | 9 | 11 | 14 | . Clustering . # standardizing the data def std_data(dataset): x = dataset.values scaler = preprocessing.StandardScaler() x_scaled = scaler.fit_transform(x) std_data.x_standardized = pd.DataFrame(x_scaled) # Using PCA def pca_data(): pca = PCA(n_components = 2) pca_data.x_std = pd.DataFrame(pca.fit_transform(std_data.x_standardized)) . . 📝📝 We have to standardize the data as variables are measured on different scales. Then we will use PCA to reduce 62 dimensions into 2 for plot. . 1. K-Means Clustering . # K-Means Clustering def kmeans_clustering(dataset,n): # specify the number of clusters kmeans = KMeans(n_clusters=n,max_iter=600) # fit the input data kmeans = kmeans.fit(pca_data.x_std) # get the cluster labels labels = kmeans.predict(pca_data.x_std) # centroid values centroids = kmeans.cluster_centers_ # cluster values clusters = kmeans.labels_.tolist() # making a new dataframe by adding players&#39; names,positions,club,nationality and their clusters kmeans_clustering.reduced = pca_data.x_std.copy() kmeans_clustering.reduced[&#39;cluster&#39;] = clusters kmeans_clustering.reduced[&#39;name&#39;] = names kmeans_clustering.reduced[&#39;positions&#39;] = positions kmeans_clustering.reduced[&#39;club&#39;] = club kmeans_clustering.reduced[&#39;nationality&#39;] = nationality kmeans_clustering.reduced.columns = [&#39;x&#39;, &#39;y&#39;, &#39;cluster&#39;, &#39;name&#39;,&#39;positions&#39;,&#39;club&#39;,&#39;nationality&#39;] ax = sns.lmplot(x=&quot;x&quot;, y=&quot;y&quot;, hue=&#39;cluster&#39;, data = kmeans_clustering.reduced, legend=False,palette = &#39;hls&#39;,fit_reg=False, size = 15) ax = plt.scatter(centroids[:, 0], centroids[:, 1], marker=&#39;*&#39;, s=300,c=&#39;white&#39;, label=&#39;centroids&#39;) plt.legend() plt.ylim=(-10, 10) plt.title(&quot;K-Means Clustering&quot;,fontsize = 30) plt.tick_params(labelsize=15) plt.xlabel(&quot;PC 1&quot;, fontsize = 20) plt.ylabel(&quot;PC 2&quot;, fontsize = 20) plt.show() . . # performing standardization std_data(data) # performing PCA pca_data() print() print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Plotting K-Means Clustering &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) # performing k-means clustering and specifying 5 clusters kmeans_clustering(data,5) . . 📈📈📈📈📈 Plotting K-Means Clustering 📈📈📈📈📈 . 📝📝 Clusters are formed based on the players positions. The above graph shows yhe scatter plot of the data colored by the cluster they belong to. The symbol &#39;*&#39; is the centroid of each cluster. . #No. of Positions in each Cluster print(&#39; nNo. of Positions in each Cluster : n n&#39; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) cluster_players = kmeans_clustering.reduced.groupby([&#39;cluster&#39;]) players_count = cluster_players[&#39;positions&#39;].count() players_count.to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#874E8A&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . No. of Positions in each Cluster : 👇🏻👇🏻👇🏻 . positions . cluster . 0 116 | . 1 54 | . 2 78 | . 3 52 | . 4 111 | . 📝📝 In first cluster there are 60 players, in second 101 players, in third 54 players, in fourth 114 players and in last 82 players. . cluster_positions = kmeans_clustering.reduced.groupby([&#39;cluster&#39;,&#39;positions&#39;]) . . #Count of every Position in each Cluster print(&#39; nCount of every position in each Cluster : n n&#39; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) positions_count = cluster_positions[&#39;positions&#39;].count() positions_count = positions_count.to_frame() positions_count.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#5E5373&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Count of every position in each Cluster : 👇🏻👇🏻👇🏻 . positions . cluster positions . 0 Attacker 62 | . Midfielder 54 | . 1 Goalkeeper 54 | . 2 Attacker 10 | . Defender 31 | . Midfielder 37 | . 3 Defender 51 | . Midfielder 1 | . 4 Attacker 35 | . Defender 12 | . Midfielder 64 | . 📝📝 The third cluster contains only 54 Goalkeepers and other 4 clusters contain different no. of Attackers, Defenders and Midfielders. . #Nationwise Positions of the Players within each Cluster print(&#39; nNationwise Positions of the Players within each Cluster : n n&#39; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) cluster_nations = kmeans_clustering.reduced.groupby([&#39;cluster&#39;,&#39;positions&#39;,&#39;nationality&#39;]) nations_count = cluster_nations[&#39;nationality&#39;].count() nations_count = nations_count.to_frame() nations_count.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#5E83A8&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Nationwise Positions of the Players within each Cluster : 👇🏻👇🏻👇🏻 . nationality . cluster positions nationality . 0 Attacker Algeria 1 | . Argentina 4 | . Belgium 2 | . Brazil 7 | . Chile 1 | . Croatia 1 | . Egypt 1 | . England 2 | . France 8 | . Gabon 1 | . Germany 4 | . Italy 4 | . Ivory Coast 2 | . Jamaica 1 | . Korea Republic 1 | . Mexico 2 | . Netherlands 3 | . Poland 1 | . Portugal 1 | . Senegal 1 | . Serbia 2 | . Slovenia 1 | . Spain 8 | . Switzerland 1 | . Uruguay 1 | . Venezuela 1 | . Midfielder Argentina 3 | . Belgium 3 | . Bosnia Herzegovina 1 | . Brazil 6 | . Colombia 1 | . Croatia 1 | . Denmark 1 | . England 1 | . France 4 | . Germany 7 | . Italy 2 | . Mexico 1 | . Morocco 1 | . Portugal 7 | . Scotland 1 | . Serbia 1 | . Spain 9 | . Sweden 1 | . Ukraine 1 | . Uruguay 2 | . 1 Goalkeeper Albania 1 | . Argentina 1 | . Belgium 2 | . Brazil 4 | . Cameroon 1 | . Costa Rica 1 | . Czech Republic 2 | . Denmark 1 | . England 1 | . Finland 1 | . France 4 | . Germany 6 | . Hungary 1 | . Italy 7 | . Netherlands 1 | . Norway 1 | . Poland 2 | . Portugal 2 | . Slovenia 2 | . Spain 10 | . Switzerland 2 | . Uruguay 1 | . 2 Attacker Colombia 1 | . Croatia 1 | . Denmark 1 | . France 2 | . Netherlands 3 | . Spain 2 | . Defender Argentina 1 | . Belgium 3 | . Brazil 5 | . England 2 | . France 4 | . Germany 2 | . Italy 1 | . Netherlands 3 | . Poland 1 | . Portugal 1 | . Russia 1 | . Serbia 1 | . Spain 5 | . Sweden 1 | . Midfielder Argentina 1 | . Belgium 2 | . Brazil 8 | . Central African Rep. 1 | . Chile 1 | . Denmark 1 | . England 2 | . France 4 | . Germany 2 | . Ghana 1 | . Italy 2 | . Mexico 1 | . Nigeria 1 | . Portugal 2 | . Senegal 1 | . Serbia 2 | . Spain 4 | . Switzerland 1 | . 3 Defender Argentina 2 | . Belgium 1 | . Brazil 6 | . Cameroon 2 | . Colombia 1 | . Croatia 1 | . Denmark 1 | . England 2 | . France 5 | . Germany 6 | . Greece 2 | . Hungary 1 | . Italy 5 | . Montenegro 1 | . Netherlands 1 | . Portugal 2 | . Senegal 1 | . Slovakia 1 | . Spain 3 | . Switzerland 1 | . Togo 1 | . Uruguay 5 | . Midfielder Spain 1 | . 4 Attacker Argentina 5 | . Austria 1 | . Belgium 1 | . Bosnia Herzegovina 1 | . Brazil 3 | . Colombia 1 | . DR Congo 1 | . Ecuador 1 | . England 2 | . France 3 | . Germany 1 | . Israel 1 | . Italy 4 | . Poland 1 | . Spain 5 | . Sweden 1 | . Uruguay 2 | . Wales 1 | . Defender Austria 1 | . Brazil 2 | . England 1 | . France 1 | . Portugal 3 | . Scotland 1 | . Spain 3 | . Midfielder Argentina 3 | . Armenia 1 | . Austria 1 | . Belgium 1 | . Bosnia Herzegovina 1 | . Brazil 6 | . Chile 1 | . Colombia 1 | . Croatia 4 | . England 2 | . France 5 | . Germany 6 | . Guinea 1 | . Iceland 1 | . Italy 3 | . Mexico 1 | . Netherlands 3 | . Poland 1 | . Portugal 4 | . Serbia 2 | . Slovakia 1 | . Slovenia 1 | . Spain 12 | . Uruguay 1 | . Wales 1 | . 📝📝 From above we can see that which countries players are grouped together on the basis of their positions. . cluster_nations[&#39;nationality&#39;] . . &lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x7f87fa8c75d0&gt; . #Clubwise Positions of the Players within each Cluster print(&#39; nClubwise Positions of the Players within each Cluster : n n&#39; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) cluster_clubs = kmeans_clustering.reduced.groupby([&#39;cluster&#39;,&#39;positions&#39;,&#39;club&#39;]) clubs_count = cluster_clubs[&#39;club&#39;].count() clubs_count = clubs_count.to_frame() clubs_count.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#337F93&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Clubwise Positions of the Players within each Cluster : 👇🏻👇🏻👇🏻 . club . cluster positions club . 0 Attacker AS Monaco 1 | . Ajax 3 | . Al Hilal 1 | . Arsenal 3 | . Atalanta 2 | . Atlanta United 1 | . Bayer 04 Leverkusen 2 | . Borussia Dortmund 1 | . Chelsea 2 | . Crystal Palace 1 | . FC Barcelona 4 | . FC Bayern München 2 | . Fenerbahçe SK 1 | . Grêmio 1 | . Juventus 2 | . Levante UD 1 | . Liverpool 3 | . Los Angeles FC 1 | . Manchester City 5 | . Manchester United 3 | . Milan 1 | . Napoli 2 | . Olympique Lyonnais 1 | . Olympique de Marseille 1 | . PSV 2 | . Paris Saint-Germain 3 | . RB Leipzig 1 | . RC Celta 1 | . Real Madrid 4 | . Real Sociedad 1 | . Sampdoria 1 | . Shanghai Greenland Shenhua FC 1 | . TSG 1899 Hoffenheim 1 | . Tottenham Hotspur 1 | . Valencia CF 1 | . Midfielder AS Monaco 1 | . Ajax 1 | . Al Nassr 1 | . Arsenal 1 | . Athletic Club de Bilbao 1 | . Atlético Madrid 2 | . Bayer 04 Leverkusen 1 | . Beşiktaş JK 2 | . Borussia Dortmund 6 | . Bournemouth 1 | . Chicago Fire 1 | . Dalian YiFang FC 1 | . FC Bayern München 3 | . FC Porto 1 | . Fluminense 1 | . Inter 1 | . Juventus 2 | . Lazio 1 | . Manchester City 3 | . Manchester United 1 | . Medipol Başakşehir FK 1 | . Napoli 1 | . Olympique de Marseille 1 | . Paris Saint-Germain 2 | . RB Leipzig 1 | . Real Betis 2 | . Real Madrid 3 | . SL Benfica 1 | . Shakhtar Donetsk 2 | . Shanghai SIPG FC 1 | . Sporting CP 1 | . Tottenham Hotspur 2 | . Uruguay 2 | . Valencia CF 1 | . West Ham United 1 | . 1 Goalkeeper 1. FC Köln 1 | . AS Monaco 1 | . AS Saint-Étienne 1 | . Ajax 1 | . Arsenal 1 | . Atlético Madrid 2 | . Bayer 04 Leverkusen 1 | . Borussia Dortmund 1 | . Borussia Mönchengladbach 1 | . Cagliari 1 | . Chelsea 1 | . Deportivo Alavés 1 | . Eintracht Frankfurt 1 | . Everton 1 | . FC Barcelona 2 | . FC Bayern München 1 | . FC Porto 1 | . Galatasaray SK 1 | . Getafe CF 1 | . Grêmio 1 | . Hertha BSC 1 | . Inter 1 | . Juventus 3 | . Lazio 1 | . Leicester City 1 | . Liverpool 1 | . Manchester City 1 | . Manchester United 1 | . Milan 2 | . Montpellier HSC 1 | . Olympique Lyonnais 1 | . Paris Saint-Germain 1 | . RB Leipzig 1 | . Real Madrid 2 | . Real Valladolid CF 1 | . Roma 1 | . SV Werder Bremen 1 | . Sassuolo 1 | . Sevilla FC 1 | . Sporting CP 1 | . TSG 1899 Hoffenheim 1 | . Torino 1 | . Tottenham Hotspur 1 | . Valencia CF 1 | . VfL Wolfsburg 1 | . Villarreal CF 1 | . West Ham United 1 | . Wolverhampton Wanderers 1 | . 2 Attacker Atalanta 1 | . Athletic Club de Bilbao 1 | . Atlético Madrid 1 | . Chelsea 1 | . Juventus 1 | . RB Leipzig 1 | . Sevilla FC 1 | . Sporting CP 1 | . VfL Wolfsburg 1 | . West Ham United 1 | . Defender Ajax 2 | . Arsenal 1 | . Borussia Dortmund 3 | . Chelsea 2 | . FC Barcelona 1 | . FC Bayern München 1 | . Juventus 3 | . Liverpool 1 | . Manchester City 2 | . Manchester United 2 | . PFC CSKA Moscow 1 | . Paris Saint-Germain 3 | . Real Betis 1 | . Real Madrid 3 | . Roma 1 | . SL Benfica 1 | . Tottenham Hotspur 2 | . VfL Wolfsburg 1 | . Midfielder Arsenal 1 | . Athletic Club de Bilbao 1 | . Atlético Madrid 2 | . Atlético Mineiro 1 | . Boca Juniors 1 | . Borussia Dortmund 2 | . Crystal Palace 1 | . FC Barcelona 2 | . FC Porto 1 | . Guangzhou Evergrande Taobao FC 1 | . Guangzhou R&amp;F FC 1 | . Juventus 4 | . Lazio 2 | . Leicester City 1 | . Liverpool 3 | . Manchester City 2 | . Manchester United 1 | . Milan 1 | . Olympique de Marseille 1 | . Paris Saint-Germain 1 | . Real Betis 1 | . Real Madrid 1 | . Real Sociedad 1 | . Sevilla FC 1 | . Tottenham Hotspur 1 | . Valencia CF 1 | . Watford 1 | . 3 Defender Arsenal 1 | . Atlético Madrid 3 | . Bayer 04 Leverkusen 1 | . Borussia Dortmund 1 | . Borussia Mönchengladbach 1 | . Chelsea 1 | . Cruzeiro 1 | . FC Barcelona 2 | . FC Bayern München 2 | . FC Girondins de Bordeaux 1 | . FC Porto 1 | . Getafe CF 1 | . Grêmio 1 | . Inter 3 | . Jiangsu Suning FC 1 | . Juventus 2 | . LOSC Lille 1 | . Lazio 1 | . Liverpool 1 | . Lokomotiv Moscow 2 | . Manchester City 2 | . Manchester United 1 | . Milan 1 | . Napoli 2 | . Paris Saint-Germain 1 | . RB Leipzig 1 | . RSC Anderlecht 1 | . Real Madrid 2 | . SL Benfica 1 | . Sevilla FC 1 | . Sporting CP 2 | . Torino 2 | . Tottenham Hotspur 1 | . Uruguay 2 | . Valencia CF 2 | . Villarreal CF 1 | . Midfielder FC Bayern München 1 | . 4 Attacker AS Monaco 1 | . Al Hilal 1 | . Athletic Club de Bilbao 1 | . Atlético Madrid 1 | . Atlético Mineiro 1 | . Bayer 04 Leverkusen 1 | . Beijing Sinobo Guoan FC 1 | . Borussia Mönchengladbach 1 | . Brescia 1 | . Ecuador 1 | . Guangzhou R&amp;F FC 1 | . Inter 3 | . Juventus 1 | . LA Galaxy 1 | . Lazio 1 | . Leicester City 1 | . Liverpool 1 | . Napoli 1 | . Paris Saint-Germain 1 | . Racing Club 1 | . Real Betis 1 | . Real Madrid 2 | . Real Sociedad 1 | . Roma 2 | . Shanghai SIPG FC 1 | . Sporting CP 1 | . Torino 1 | . Tottenham Hotspur 1 | . Uruguay 1 | . Valencia CF 1 | . Villarreal CF 1 | . Defender Everton 1 | . FC Barcelona 2 | . FC Bayern München 1 | . FC Porto 1 | . Leicester City 1 | . Liverpool 2 | . Manchester City 1 | . Real Madrid 1 | . SL Benfica 1 | . Valencia CF 1 | . Midfielder AS Monaco 1 | . Ajax 1 | . Arsenal 3 | . Athletic Club de Bilbao 1 | . Atlético Madrid 2 | . Bayer 04 Leverkusen 2 | . Beijing Sinobo Guoan FC 1 | . Cagliari 1 | . Chelsea 3 | . Dalian YiFang FC 1 | . Eintracht Frankfurt 1 | . Everton 1 | . FC Barcelona 5 | . FC Bayern München 5 | . Guangzhou Evergrande Taobao FC 1 | . Inter 2 | . Juventus 3 | . Lazio 1 | . Liverpool 2 | . Manchester City 1 | . Manchester United 2 | . Milan 1 | . Napoli 3 | . Olympique Lyonnais 2 | . Paris Saint-Germain 2 | . RB Leipzig 2 | . Real Betis 1 | . Real Madrid 1 | . SL Benfica 1 | . Sevilla FC 3 | . Tottenham Hotspur 3 | . Valencia CF 1 | . Villarreal CF 1 | . West Ham United 1 | . Wolverhampton Wanderers 2 | . Evaluation of K-Means Clustering . 📝📝 Clustering analysis doesnt have a solid evaluation metric that we can use to evaluate the outcome of different clustering algorithms. In cluster-predict methodology, we can evaluate how well the models are performing based on different k clusters since clusters are used in the downstream modeling. We will discuss two metrics that may give us some intuition about k: ✏️ Elbow Method ✏️ Silhouette Analysis . # Standardize the data X_std = StandardScaler().fit_transform(data) . . i. Elbow Method . number_clusters = range(1, 8) kmeans = [KMeans(n_clusters=i, max_iter = 600) for i in number_clusters] kmeans score = [kmeans[i].fit(X_std).score(X_std) for i in range(len(kmeans))] score plt.plot(number_clusters, score, color=&#39;#DB4462&#39;) plt.xlabel(&#39;Number of Clusters&#39;) plt.ylabel(&#39;Score&#39;) plt.title(&#39;Elbow Method&#39;) plt.show() . . 📝📝 For k=3, the curve starts to flatten out and forming an elbow. . ii. Silhouette Analysis . for i, k in enumerate([2,3, 4, 5, 6,7]): fig, (ax1, ax2) = plt.subplots(2,1) fig.set_size_inches(20, 10) # Run the Kmeans algorithm km = KMeans(n_clusters=k) labels = km.fit_predict(X_std) centroids = km.cluster_centers_ # Get silhouette samples silhouette_vals = silhouette_samples(X_std, labels) # Silhouette plot y_ticks = [] y_lower, y_upper = 0, 0 for i, cluster in enumerate(np.unique(labels)): cluster_silhouette_vals = silhouette_vals[labels == cluster] cluster_silhouette_vals.sort() y_upper += len(cluster_silhouette_vals) ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor=&#39;none&#39;, height=1) ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1)) y_lower += len(cluster_silhouette_vals) # Get the average silhouette score and plot it avg_score = np.mean(silhouette_vals) ax1.axvline(avg_score, linestyle=&#39;--&#39;, linewidth=2, color=&#39;green&#39;) ax1.set_yticks([]) ax1.set_xlim([-0.1, 1]) ax1.set_xlabel(&#39;Silhouette coefficient values&#39;,fontsize = 20) ax1.set_ylabel(&#39;Cluster labels&#39;,fontsize = 20) ax1.set_title(&#39;Silhouette plot for the various clusters&#39;, y=1.02,fontsize = 20) # Scatter plot of data colored with labels ax2.scatter(X_std[:, 0], X_std[:, 1], c=labels) ax2.scatter(centroids[:, 0], centroids[:, 1], marker=&#39;*&#39;, c=&#39;r&#39;, s=250) ax2.set_xlim([-15, 15]) ax2.set_ylim([-10, 10]) ax2.set_xlabel(&#39;PCA1&#39;,fontsize = 20) ax2.set_ylabel(&#39;PCA2&#39;,fontsize = 20) ax2.set_title(&#39;Visualization of clustered data&#39;, y=1.02,fontsize = 20) ax2.set_aspect(&#39;equal&#39;) plt.tight_layout() plt.suptitle(f&#39;Silhouette Analysis using K = {k}&#39;,fontsize=25, fontweight=&#39;semibold&#39;, y=1.05) . . 📝📝 Essence of above plot : ✏️ As the above plot show, n_clusters=2 has the best average silhouette score of around 0.5 and but all the clusters are not above the average shows that its not a good choice. Also the thickness of the plot gives an indication of how big each cluster is. The plot shows that cluster 1 is so much bigger than the cluster 2. ✏️ However, as we increase n_cluster=3, the average silhouette score decreased to around 0.25 and all the clusters are above the average. So its a better choice than n_cluster=2. . Applying K-Means with a new no. of clusters . 📝📝 So we will again plot k-means clustering for n_clusters=3 as discussed above. . print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Plotting K-Means Clustering &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) kmeans_clustering(data,3) . . 📈📈📈📈📈 Plotting K-Means Clustering 📈📈📈📈📈 . 📝📝 Essence of above plot : ✏️ We can see three pretty distinct clusters here with particularly large separation for the green cluster indicating quite a difference in terms of the position of the players. The majority of the data is contained within the blue cluster, however. ✏️ The goal of kmeans is to group data points into distinct non-overlapping subgroups. It does a very good job when the clusters have a kind of spherical shapes. However, it suffers as the geometric shapes of clusters deviates from spherical shapes. Moreover, it also doesn’t learn the number of clusters from the data and requires it to be pre-defined. . Calculating Silhouette,Calinski Harabasz and Davies Bouldin Scores for K-means Clustering . # Fit K-Means kmeans_1 = KMeans(n_clusters=3,random_state= 30) # Use fit_predict to cluster the dataset predictions = kmeans_1.fit_predict(pca_data.x_std) # Calculate cluster validation metrics score_kmeans_s = silhouette_score(pca_data.x_std, kmeans_1.labels_, metric=&#39;euclidean&#39;) score_kmeans_c = calinski_harabasz_score(pca_data.x_std, kmeans_1.labels_) score_kmeans_d = davies_bouldin_score(pca_data.x_std, predictions) r1 = pd.DataFrame({&#39;Scores&#39;: &#39;Silhouette Score&#39;, &#39;Values&#39; : &quot;{:.4f}&quot;.format(score_kmeans_s)},index={&#39;1&#39;}) r2 = pd.DataFrame({&#39;Scores&#39;: &#39;Calinski Harabasz Score&#39;, &#39;Values&#39; : &quot;{:.4f}&quot;.format(score_kmeans_c)},index={&#39;2&#39;}) r3 = pd.DataFrame({&#39;Scores&#39;: &#39;Davies Bouldin Score&#39;, &#39;Values&#39; : &quot;{:.4f}&quot;.format(score_kmeans_d)},index={&#39;3&#39;}) res1 = pd.concat([r1,r2,r3]) res1.columns = [&#39;Scores&#39;,&#39;Values&#39;] res1=res1.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#34495E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#FEF5E7&#39;}, subset=[&#39;Scores&#39;]) res1 . . Scores Values . 1 Silhouette Score | 0.5892 | . 2 Calinski Harabasz Score | 1244.3950 | . 3 Davies Bouldin Score | 0.5150 | . 2. Hierarchical Clustering . i. Hierarchical Clustering with Average Linkage . #collapse print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Plotting Hierarchical Clustering with Average Linkage &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize=(18,90)) plt.title(&#39;Hierarchical Clustering Dendrogram with Average Linkage&#39;) dendrogram = sch.dendrogram(sch.linkage(std_data.x_standardized, method=&quot;average&quot;), labels= names, leaf_font_size = 13, orientation=&#39;right&#39;) . . 📈📈📈📈📈 Plotting Hierarchical Clustering with Average Linkage 📈📈📈📈📈 . 📝📝 It grouped into three positions. . ii. Hierarchical Clustering with Single Linkage . #collapse print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Plotting Hierarchical Clustering with Single Linkage &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize=(18,90)) plt.title(&#39;Hierarchical Clustering Dendrogram with Single Linkage&#39;) dendrogram = sch.dendrogram(sch.linkage(std_data.x_standardized, method=&quot;single&quot;), labels= names, leaf_font_size = 13, orientation=&#39;right&#39;) . . 📈📈📈📈📈 Plotting Hierarchical Clustering with Single Linkage 📈📈📈📈📈 . 📝📝 It grouped into three positions and one group is almost negligible compared to other groups.. . iii. Hierarchical Clustering with Centroid Linkage . #collapse print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Plotting Hierarchical Clustering with Centroid Linkage &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize=(18,90)) plt.title(&#39;Hierarchical Clustering Dendrogram with Centroid Linkage&#39;) dendrogram = sch.dendrogram(sch.linkage(std_data.x_standardized, method=&quot;centroid&quot;), labels= names, leaf_font_size = 13, orientation=&#39;right&#39;) . . 📈📈📈📈📈 Plotting Hierarchical Clustering with Centroid Linkage 📈📈📈📈📈 . 📝📝 It grouped into two positions. . iv. Hierarchical Clustering with Complete Linkage . #collapse print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Plotting Hierarchical Clustering with Complete Linkage &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) plt.figure(figsize=(18,90)) plt.title(&#39;Hierarchical Clustering Dendrogram with Centroid Linkage&#39;) dendrogram = sch.dendrogram(sch.linkage(std_data.x_standardized, method=&quot;complete&quot;), labels= names, leaf_font_size = 13, orientation=&#39;right&#39;) . . 📈📈📈📈📈 Plotting Hierarchical Clustering with Complete Linkage 📈📈📈📈📈 . 📝📝 It grouped into five positions. . 📝📝 Hierarchical Clustering with Average Linkage groups the dataset better than single, Centroid and Complete Linkages . Calculating Silhouette,Calinski Harabasz and Davies Bouldin Scores for Hierarchical Clustering . # define the model model = AgglomerativeClustering(n_clusters=3) # fit model and predict clusters yhat = model.fit(pca_data.x_std) yhat_2 = model.fit_predict(pca_data.x_std) # retrieve unique clusters clusters = unique(yhat) # Calculate cluster validation metrics score_AGclustering_s = silhouette_score(pca_data.x_std, yhat.labels_, metric=&#39;euclidean&#39;) score_AGclustering_c = calinski_harabasz_score(pca_data.x_std, yhat.labels_) score_AGclustering_d = davies_bouldin_score(pca_data.x_std, yhat_2) s1 = pd.DataFrame({&#39;Scores&#39;: &#39;Silhouette Score&#39;, &#39;Values&#39; : &quot;{:.4f}&quot;.format(score_AGclustering_s)},index={&#39;1&#39;}) s2 = pd.DataFrame({&#39;Scores&#39;: &#39;Calinski Harabasz Score&#39;, &#39;Values&#39; : &quot;{:.4f}&quot;.format(score_AGclustering_c)},index={&#39;2&#39;}) s3 = pd.DataFrame({&#39;Scores&#39;: &#39;Davies Bouldin Score&#39;, &#39;Values&#39; : &quot;{:.4f}&quot;.format(score_AGclustering_d)},index={&#39;3&#39;}) res2 = pd.concat([s1,s2,s3]) res2.columns = [&#39;Scores&#39;,&#39;Values&#39;] res2=res2.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#34495E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#FEF5E7&#39;}, subset=[&#39;Scores&#39;]) res2 . . Scores Values . 1 Silhouette Score | 0.5919 | . 2 Calinski Harabasz Score | 1211.5509 | . 3 Davies Bouldin Score | 0.4953 | . 3. DBSCAN Clustering . # performing standardization std_data(data) # performing PCA pca_data() . . # DBSCAN clustering def dbscan_clustering(e,n): # train the model using DBSCAN dbscan = DBSCAN(eps=e, min_samples=n) # the prediction for dbscan clusters clusters = dbscan.fit_predict(pca_data.x_std) # making a new dataframe by adding players&#39; names,positions and their clusters dbscan_clustering.reduced = pca_data.x_std.copy() dbscan_clustering.reduced[&#39;cluster&#39;] = clusters dbscan_clustering.reduced[&#39;name&#39;] = names dbscan_clustering.reduced[&#39;positions&#39;] = positions dbscan_clustering.reduced[&#39;club&#39;] = club dbscan_clustering.reduced[&#39;nationality&#39;] = nationality dbscan_clustering.reduced.columns = [&#39;x&#39;, &#39;y&#39;, &#39;cluster&#39;, &#39;name&#39;,&#39;positions&#39;,&#39;club&#39;,&#39;nationality&#39;] ax = sns.lmplot(x=&quot;x&quot;, y=&quot;y&quot;, hue=&#39;cluster&#39;, data = dbscan_clustering.reduced, legend=False,palette = &#39;hls&#39;,fit_reg=False, size = 15) plt.legend() plt.ylim=(-10, 10) plt.title(&quot;DBSCAN Clustering&quot;,fontsize = 30) plt.tick_params(labelsize=15) plt.xlabel(&quot;PC 1&quot;, fontsize = 20) plt.ylabel(&quot;PC 2&quot;, fontsize = 20) plt.show() . . # performing DBSCAN clustering and specifying min_samples=10 and Eps=1 print() print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Plotting DBSCAN Clustering &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) dbscan_clustering(1,10) . . 📈📈📈📈📈 Plotting DBSCAN Clustering 📈📈📈📈📈 . 📝📝 Clusters are formed into Goalkeepers vs the rest. also this plot is not very accurate. . Finding an Optimal Epsilon . # calculate the distance from each point to its closest neighbor nn = NearestNeighbors(n_neighbors = 2) # fit the nearest neighbor nbrs = nn.fit(pca_data.x_std) # returns two arrays - distance to the closest n_neighbors points and index for each point distances, indices = nbrs.kneighbors(pca_data.x_std) # sort the distance and plot it distances = np.sort(distances, axis=0) distances = distances[:,1] plt.plot(distances, color=&#39;#098BAE&#39;) . . [&lt;matplotlib.lines.Line2D at 0x7f87fd3bd790&gt;] . 📝📝 It looks like the curvature starts picking up at around eps=0.62 and can be considered as optimal epsilon for this dataset. . Applying DBSCAN with a new Eps . # performing DBSCAN clustering and specifying min_samples=11 and Eps=0.62 print() print(&quot; t t t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5 ,&quot;Plotting DBSCAN Clustering &quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*5) print(&quot;&quot;) dbscan_clustering(0.62,11) . . 📈📈📈📈📈 Plotting DBSCAN Clustering 📈📈📈📈📈 . 📝📝 DBSCAN with Eps = 0.62 and min_samples = 11 is doing a better job at grouping and detecting outliers. . Calculating Silhouette,Calinski Harabasz and Davies Bouldin Scores for DBSCAN Clustering . # define the model model = DBSCAN(eps=0.62, min_samples= 11) # fit model and predict clusters yhat = model.fit_predict(pca_data.x_std) # retrieve unique clusters clusters = unique(yhat) # Calculate cluster validation metrics score_dbsacn_s = silhouette_score(pca_data.x_std, yhat, metric=&#39;euclidean&#39;) score_dbsacn_c = calinski_harabasz_score(pca_data.x_std, yhat) score_dbsacn_d = davies_bouldin_score(pca_data.x_std, yhat) t1 = pd.DataFrame({&#39;Scores&#39;: &#39;Silhouette Score&#39;, &#39;Values&#39; : &quot;{:.4f}&quot;.format(score_dbsacn_s)},index={&#39;1&#39;}) t2 = pd.DataFrame({&#39;Scores&#39;: &#39;Calinski Harabasz Score&#39;, &#39;Values&#39; : &quot;{:.4f}&quot;.format(score_dbsacn_c)},index={&#39;2&#39;}) t3 = pd.DataFrame({&#39;Scores&#39;: &#39;Davies Bouldin Score&#39;, &#39;Values&#39; : &quot;{:.4f}&quot;.format(score_dbsacn_d)},index={&#39;3&#39;}) res3 = pd.concat([t1,t2,t3]) res3.columns = [&#39;Scores&#39;,&#39;Values&#39;] res3=res3.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#34495E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#FEF5E7&#39;}, subset=[&#39;Scores&#39;]) res3 . . Scores Values . 1 Silhouette Score | 0.2742 | . 2 Calinski Harabasz Score | 259.0584 | . 3 Davies Bouldin Score | 2.2460 | . Conclusion . z1 = pd.DataFrame({&#39;Metrics&#39;: &#39;Silhouette Score&#39; ,&#39;K-Means Clustering&#39; : &quot;{:.4f}&quot;.format(score_kmeans_s) ,&#39;Hierarchical Clustering&#39; : &quot;{:.4f}&quot;.format(score_AGclustering_s) ,&#39;DBSCAN Clustering&#39;: &quot;{:.4f}&quot;.format(score_dbsacn_s)},index={&#39;1&#39;}) z2 = pd.DataFrame({&#39;Metrics&#39;: &#39;Calinski Harabasz Score&#39; ,&#39;K-Means Clustering&#39; : &quot;{:.4f}&quot;.format(score_kmeans_c) ,&#39;Hierarchical Clustering&#39; : &quot;{:.4f}&quot;.format(score_AGclustering_c) ,&#39;DBSCAN Clustering&#39;: &quot;{:.4f}&quot;.format(score_dbsacn_c)},index={&#39;2&#39;}) z3 = pd.DataFrame({&#39;Metrics&#39;: &#39;Davies Bouldin Score&#39; ,&#39;K-Means Clustering&#39; : &quot;{:.4f}&quot;.format(score_kmeans_d) ,&#39;Hierarchical Clustering&#39; : &quot;{:.4f}&quot;.format(score_AGclustering_d) ,&#39;DBSCAN Clustering&#39;: &quot;{:.4f}&quot;.format(score_dbsacn_d)},index={&#39;3&#39;}) result = pd.concat([z1,z2,z3]) result.columns = [&#39;Metrics&#39;,&#39;K-Means Clustering&#39;,&#39;Hierarchical Clustering&#39;,&#39;DBSCAN Clustering&#39;] result=result.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#66070E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#E7A4A9&#39;}, subset=[&#39;Metrics&#39;]) result . . Metrics K-Means Clustering Hierarchical Clustering DBSCAN Clustering . 1 Silhouette Score | 0.5892 | 0.5919 | 0.2742 | . 2 Calinski Harabasz Score | 1244.3950 | 1211.5509 | 259.0584 | . 3 Davies Bouldin Score | 0.5150 | 0.4953 | 2.2460 | . 📝📝 We can see that K-means outperforms Hierarchical and DBSCAN clusterings based on all cluster validation metrics. . &lt;/div&gt; .",
            "url": "https://swati5140.github.io/my_data_science_portfolio/2021/09/21/_08_31_FIFA2020.html",
            "relUrl": "/2021/09/21/_08_31_FIFA2020.html",
            "date": " • Sep 21, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Predicting the subscription of a bank term deposit using machine learning",
            "content": "Connection to the drive . # Mounting drive from google.colab import drive drive.mount(&#39;/content/drive&#39;) . . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . Install and import the required libraries and packages . !pip install pingouin . Requirement already satisfied: pingouin in /usr/local/lib/python3.7/dist-packages (0.4.0) Requirement already satisfied: statsmodels&gt;=0.12.0 in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.12.2) Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.8.9) Requirement already satisfied: scipy&gt;=1.7 in /usr/local/lib/python3.7/dist-packages (from pingouin) (1.7.1) Requirement already satisfied: seaborn&gt;=0.9.0 in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.11.1) Requirement already satisfied: numpy&gt;=1.19 in /usr/local/lib/python3.7/dist-packages (from pingouin) (1.19.5) Requirement already satisfied: pandas&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from pingouin) (1.1.5) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.24.2) Requirement already satisfied: pandas-flavor&gt;=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.2.0) Requirement already satisfied: matplotlib&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from pingouin) (3.2.2) Requirement already satisfied: outdated in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.2.1) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.0.2-&gt;pingouin) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.0.2-&gt;pingouin) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.0.2-&gt;pingouin) (1.3.1) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.0.2-&gt;pingouin) (2.8.2) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler&gt;=0.10-&gt;matplotlib&gt;=3.0.2-&gt;pingouin) (1.15.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=1.0-&gt;pingouin) (2018.9) Requirement already satisfied: xarray in /usr/local/lib/python3.7/dist-packages (from pandas-flavor&gt;=0.2.0-&gt;pingouin) (0.18.2) Requirement already satisfied: patsy&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels&gt;=0.12.0-&gt;pingouin) (0.5.1) Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated-&gt;pingouin) (0.2.2) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated-&gt;pingouin) (2.23.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;outdated-&gt;pingouin) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;outdated-&gt;pingouin) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;outdated-&gt;pingouin) (2021.5.30) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;outdated-&gt;pingouin) (2.10) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;pingouin) (2.2.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;pingouin) (1.0.1) Requirement already satisfied: setuptools&gt;=40.4 in /usr/local/lib/python3.7/dist-packages (from xarray-&gt;pandas-flavor&gt;=0.2.0-&gt;pingouin) (57.4.0) . . !pip install -U scikit-learn . Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.24.2) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.2.0) Requirement already satisfied: scipy&gt;=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.1) Requirement already satisfied: numpy&gt;=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1) . . !pip install emoji . Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.4.2) . . !pip install emojis . Requirement already satisfied: emojis in /usr/local/lib/python3.7/dist-packages (0.6.0) . . #Packages related to general operating system &amp; warnings import os import warnings warnings.filterwarnings(&#39;ignore&#39;) # importing required libraries and packages import emoji import emojis import pandas as pd from pandas.plotting import table import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pingouin as pg from matplotlib import colors from IPython.display import display_html import scipy from scipy.stats import pearsonr, spearmanr from sklearn import preprocessing from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn import metrics from sklearn.metrics import roc_auc_score,roc_curve,auc from sklearn.metrics import confusion_matrix, accuracy_score from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import cross_val_score, KFold, GridSearchCV from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier from xgboost import XGBClassifier print(emoji.emojize(&quot;:laptop:&quot;)*28 ,&quot; n nAll the required libraries and packages are imported successfully !!! n n&quot; ,emoji.emojize(&quot;:laptop:&quot;)*28) . . 💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻 All the required libraries and packages are imported successfully !!! 💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻💻 . Load the dataset . #loading the datasets data = pd.read_csv(&#39;/content/drive/MyDrive/ColabNotebooks/1000_PBM/bank-additional-full.csv&#39;,sep=&#39;;&#39;) print(emoji.emojize(&quot;:file_folder:&quot;)*12 ,&quot; n nData loaded successfully !!! n n&quot; ,emoji.emojize(&quot;:file_folder:&quot;)*12) . . 📁📁📁📁📁📁📁📁📁📁📁📁 Data loaded successfully !!! 📁📁📁📁📁📁📁📁📁📁📁📁 . 📝📝 In the datasets the data is separated using &#39;;&#39;, therefore, while reading the CSV file, we should instruct pandas about the separator as the default separator is &#39;,&#39;. If we do not instruct as mentioned, the dataframe will have all the data in one cell. . # highlighting postive &amp; negative values def above_zero(val): &quot;&quot;&quot; Takes value as input &amp; returns in green color if positive, in red color if negative and black otherwise. &quot;&quot;&quot; if val &gt; 0: color = &#39;green&#39; elif val &lt; 0: color = &#39;red&#39; else: color = &#39;black&#39; return &#39;color: %s&#39; % color . . # To have a glimpse of the data print(&quot; nGlimpse of data : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A60B2E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).highlight_max(color=&#39;#0074FF&#39;,axis=0) .highlight_min(color=&#39;#00FFE5&#39;,axis=0) . . Glimpse of data : 👇🏻👇🏻👇🏻 . age job marital education default housing loan contact month day_of_week duration campaign pdays previous poutcome emp.var.rate cons.price.idx cons.conf.idx euribor3m nr.employed y . 0 56 | housemaid | married | basic.4y | no | no | no | telephone | may | mon | 261 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 1 57 | services | married | high.school | unknown | no | no | telephone | may | mon | 149 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 2 37 | services | married | high.school | no | yes | no | telephone | may | mon | 226 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 3 40 | admin. | married | basic.6y | no | no | no | telephone | may | mon | 151 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 4 56 | services | married | high.school | no | no | yes | telephone | may | mon | 307 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 5 45 | services | married | basic.9y | unknown | no | no | telephone | may | mon | 198 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 6 59 | admin. | married | professional.course | no | no | no | telephone | may | mon | 139 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 7 41 | blue-collar | married | unknown | unknown | no | no | telephone | may | mon | 217 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 8 24 | technician | single | professional.course | no | yes | no | telephone | may | mon | 380 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 9 25 | services | single | high.school | no | yes | no | telephone | may | mon | 50 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . # Converting the target from binary/categoric into binary/numeric data[&#39;target&#39;] = data.apply(lambda row: 1 if row[&quot;y&quot;] == &quot;yes&quot; else 0, axis=1) data.drop([&quot;y&quot;],axis=1,inplace=True) print(emojis.encode(&quot;:crayon:&quot;)*26 ,&quot; n nConverted the target from binary/categoric into binary/numeric. n n&quot; ,emojis.encode(&quot;:crayon:&quot;)*26) . . 🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️ Converted the target from binary/categoric into binary/numeric 🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️🖍️ . # Renaming some columns for better typing and calling variables data.rename(columns={&quot;emp.var.rate&quot;:&quot;emp_var_rate&quot;,&quot;cons.price.idx&quot;:&quot;cons_price_idx&quot;,&quot;cons.conf.idx&quot;:&quot;cons_conf_idx&quot;,&quot;nr.employed&quot;:&quot;nr_employed&quot;},inplace=True) print(emojis.encode(&quot;:pencil2:&quot;)*25 ,&quot; n nRenamed some columns for better typing and calling variables n n&quot; ,emojis.encode(&quot;:pencil2:&quot;)*25) . . ✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️ Renamed some columns for better typing and calling variables ✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️✏️ . # Setting up numeric (num_data) and categoric (cat_data) dataframes num_data = data.copy().select_dtypes(include=[&quot;float64&quot;,&quot;int64&quot;]) cat_data = data.copy().select_dtypes(exclude=[&quot;float64&quot;,&quot;int64&quot;]) print(emoji.emojize(&quot;:bookmark_tabs:&quot;)*28 ,&quot; n nSetting up numeric (num_data) and categoric (cat_data) dataframes n n&quot; ,emoji.emojize(&quot;:bookmark_tabs:&quot;)*28) . . 📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑 Setting up numeric (num_data) and categoric (cat_data) dataframes 📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑📑 . Basic Info of the dataset . #finding the no. of rows and cols print(&quot; nFinding the no. of rows and cols in the dataset : n n&quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;No. of clients : {}&quot;.format(data.shape[0])) print(&quot;No. of features : {} including target&quot;.format(data.shape[1])) . . Finding the no. of rows and cols in the dataset : 👇🏻👇🏻👇🏻 No. of clients : 41188 No. of features : 21 including target . # How many clients have subscribed the term deposit and how many didn&#39;t? print(&quot; nHow many clients have subscribed the term deposit and how many didn&#39;t ? n n &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;No. of clients who haven&#39;t subscribed the term deposit : {}&quot;.format(data.target.value_counts()[0])) print(&quot;No. of clients who have subscribed the term deposit : {}&quot;.format(data.target.value_counts()[1])) . . How many clients have subscribed the term deposit and how many didn&#39;t ? 👇🏻👇🏻👇🏻 No. of clients who haven&#39;t subscribed the term deposit : 36548 No. of clients who have subscribed the term deposit : 4640 . #Checking the dataset is balanced or not based on target values in the classification. print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Plotting the graph to check the dataset&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) total = len(data[&#39;target&#39;])*1 ax=sns.countplot(x=&#39;target&#39;,data=data) for p in ax.patches: ax.annotate(&#39;{:.1f}%&#39;.format(100*p.get_height()/total), (p.get_x()+0.1, p.get_height()+5), fontsize = 12) . . 📈📈 Plotting the graph to check the dataset 📈📈 . 📝📝 The dataset is imbalanced and highly skewed, where the no. of neagative class is close to 8 times the no. of positive class. . # Overview of shape, attributes, types and missing values print(&quot; nOverview of shape, attributes, types and missing values : n n&quot; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.info() . . Overview of shape, attributes, types and missing values : 👇🏻👇🏻👇🏻 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 41188 entries, 0 to 41187 Data columns (total 21 columns): # Column Non-Null Count Dtype -- -- 0 age 41188 non-null int64 1 job 41188 non-null object 2 marital 41188 non-null object 3 education 41188 non-null object 4 default 41188 non-null object 5 housing 41188 non-null object 6 loan 41188 non-null object 7 contact 41188 non-null object 8 month 41188 non-null object 9 day_of_week 41188 non-null object 10 duration 41188 non-null int64 11 campaign 41188 non-null int64 12 pdays 41188 non-null int64 13 previous 41188 non-null int64 14 poutcome 41188 non-null object 15 emp_var_rate 41188 non-null float64 16 cons_price_idx 41188 non-null float64 17 cons_conf_idx 41188 non-null float64 18 euribor3m 41188 non-null float64 19 nr_employed 41188 non-null float64 20 target 41188 non-null int64 dtypes: float64(5), int64(6), object(10) memory usage: 6.6+ MB . # General stats of the numerical variables print(&quot; nGeneral stats of the numerical variables : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.describe().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#753976&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).applymap(above_zero) . . General stats of the numerical variables : 👇🏻👇🏻👇🏻 . age duration campaign pdays previous emp_var_rate cons_price_idx cons_conf_idx euribor3m nr_employed target . count 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | . mean 40.024060 | 258.285010 | 2.567593 | 962.475454 | 0.172963 | 0.081886 | 93.575664 | -40.502600 | 3.621291 | 5167.035911 | 0.112654 | . std 10.421250 | 259.279249 | 2.770014 | 186.910907 | 0.494901 | 1.570960 | 0.578840 | 4.628198 | 1.734447 | 72.251528 | 0.316173 | . min 17.000000 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | -3.400000 | 92.201000 | -50.800000 | 0.634000 | 4963.600000 | 0.000000 | . 25% 32.000000 | 102.000000 | 1.000000 | 999.000000 | 0.000000 | -1.800000 | 93.075000 | -42.700000 | 1.344000 | 5099.100000 | 0.000000 | . 50% 38.000000 | 180.000000 | 2.000000 | 999.000000 | 0.000000 | 1.100000 | 93.749000 | -41.800000 | 4.857000 | 5191.000000 | 0.000000 | . 75% 47.000000 | 319.000000 | 3.000000 | 999.000000 | 0.000000 | 1.400000 | 93.994000 | -36.400000 | 4.961000 | 5228.100000 | 0.000000 | . max 98.000000 | 4918.000000 | 56.000000 | 999.000000 | 7.000000 | 1.400000 | 94.767000 | -26.900000 | 5.045000 | 5228.100000 | 1.000000 | . 📝📝 Essence of above dataframe : ✏️ age: The youngest client is 17 years old and the oldest is 98 years with a median of 38 years whilst the average is 40 years old. ✏️ campaign: Minimum number of contacts performed during this campaign and for this client included last contact is 1 and maximum is 56. On an average a client is contacted 3 times. ✏️ pdays: The majority of the clients have the 999 number wich indicates that most people did not contact nor were contacted by the bank. Those are considered to be &#39;out of range&#39; values. ✏️ previous: The vast majority were never contacted before. ✏️ emp_var_rate: during the period the index varied from [-3.4, 1.4] ✏️ cons_price_idx: The index varied from [92.2, 94.8] ✏️ cons_conf_idx: The consumer confidence level during that period kept always negative with a range of variation of [-51, -27]. These negative values might be explained by the recession that severely affected Portugal due the financial global crisis during that same period the data was recorded. ✏️ euribor3m: There were a huge variation of the euribor rate during the period of analysis [5% to 0.6%]. This abrupt change in euribor together with the negative confidance verified reinforces the hipothesis that this data provides information from a crisis period. ✏️ nr_employed: The number of employed people varied around 200 during the campaign. . # General stats of the categorical variables print(&quot; nGeneral stats of the categorical variables : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.describe(include=[&#39;object&#39;]).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#117A65&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . General stats of the categorical variables : 👇🏻👇🏻👇🏻 . job marital education default housing loan contact month day_of_week poutcome . count 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | . unique 12 | 4 | 8 | 3 | 3 | 3 | 2 | 10 | 5 | 3 | . top admin. | married | university.degree | no | yes | no | cellular | may | thu | nonexistent | . freq 10422 | 24928 | 12168 | 32588 | 21576 | 33950 | 26144 | 13769 | 8623 | 35563 | . 📝📝 Essence of above dataframe : ✏️ job: there are 12 types of jobs recordings in wich the &#39;administrative&#39; role is the most comum with almost 10.5k of the clients ✏️ marital: the majority of clients are married with almost 25k records ✏️ education: more than 12k people have university degree ✏️ default: from all the 41.188 clients, 32.588 don&#39;t have any credit in default ✏️ housing: almost half of the customers have a housing loan ✏️ loan: almost 34k clients don&#39;t have any personal loans ✏️ poutcome: there is no information about the outcome of any previous marketing campaign . Exploratory Data Analysis . 1. Finding duplicate values . # checking for duplicate values if present in the dataframe print(&quot;Duplicate Data&quot; ,emoji.emojize(&quot;:red_question_mark:&quot;)*2,&quot; n&quot;) print(emoji.emojize(&quot;:cross_mark_button:&quot;)*3 ,&quot; n n&quot;,data.duplicated().any() ,&quot; n n&quot;,emoji.emojize(&quot;:cross_mark_button:&quot;)*3) . . Duplicate Data ❓❓ ❎❎❎ True ❎❎❎ . # Selecting duplicate rows except first occurence based on all columns duplicate_data = data[data.duplicated(keep = &quot;last&quot;)] print(&#39; nduplicate rows : &#39; ,&quot;{} n&quot;.format(duplicate_data.shape[0]) ,&quot; n n&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) # print the resulatant dataframe containing duplicate rows duplicate_data.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#2B0E46&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . duplicate rows : 12 👇🏻👇🏻👇🏻 . age job marital education default housing loan contact month day_of_week duration campaign pdays previous poutcome emp_var_rate cons_price_idx cons_conf_idx euribor3m nr_employed target . 1265 39 | blue-collar | married | basic.6y | no | no | no | telephone | may | thu | 124 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.855000 | 5191.000000 | 0 | . 12260 36 | retired | married | unknown | no | no | no | telephone | jul | thu | 88 | 1 | 999 | 0 | nonexistent | 1.400000 | 93.918000 | -42.700000 | 4.966000 | 5228.100000 | 0 | . 14155 27 | technician | single | professional.course | no | no | no | cellular | jul | mon | 331 | 2 | 999 | 0 | nonexistent | 1.400000 | 93.918000 | -42.700000 | 4.962000 | 5228.100000 | 0 | . 16819 47 | technician | divorced | high.school | no | yes | no | cellular | jul | thu | 43 | 3 | 999 | 0 | nonexistent | 1.400000 | 93.918000 | -42.700000 | 4.962000 | 5228.100000 | 0 | . 18464 32 | technician | single | professional.course | no | yes | no | cellular | jul | thu | 128 | 1 | 999 | 0 | nonexistent | 1.400000 | 93.918000 | -42.700000 | 4.968000 | 5228.100000 | 0 | . 20072 55 | services | married | high.school | unknown | no | no | cellular | aug | mon | 33 | 1 | 999 | 0 | nonexistent | 1.400000 | 93.444000 | -36.100000 | 4.965000 | 5228.100000 | 0 | . 20531 41 | technician | married | professional.course | no | yes | no | cellular | aug | tue | 127 | 1 | 999 | 0 | nonexistent | 1.400000 | 93.444000 | -36.100000 | 4.966000 | 5228.100000 | 0 | . 25183 39 | admin. | married | university.degree | no | no | no | cellular | nov | tue | 123 | 2 | 999 | 0 | nonexistent | -0.100000 | 93.200000 | -42.000000 | 4.153000 | 5195.800000 | 0 | . 28476 24 | services | single | high.school | no | yes | no | cellular | apr | tue | 114 | 1 | 999 | 0 | nonexistent | -1.800000 | 93.075000 | -47.100000 | 1.423000 | 5099.100000 | 0 | . 32505 35 | admin. | married | university.degree | no | yes | no | cellular | may | fri | 348 | 4 | 999 | 0 | nonexistent | -1.800000 | 92.893000 | -46.200000 | 1.313000 | 5099.100000 | 0 | . 36950 45 | admin. | married | university.degree | no | no | no | cellular | jul | thu | 252 | 1 | 999 | 0 | nonexistent | -2.900000 | 92.469000 | -33.600000 | 1.072000 | 5076.200000 | 1 | . 38255 71 | retired | single | university.degree | no | no | no | telephone | oct | tue | 120 | 1 | 999 | 0 | nonexistent | -3.400000 | 92.431000 | -26.900000 | 0.742000 | 5017.500000 | 0 | . # removing duplicate values data = data.drop_duplicates(keep=&#39;last&#39;) print(emojis.encode(&quot;:scissors:&quot;)*16 ,&quot; n nDuplicate data removed successfully !! n n&quot; ,emojis.encode(&quot;:scissors:&quot;)*16) . . ✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️ Duplicate data removed successfully !! ✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️ . 2. Finding Unwanted Columns . 📝📝 duration attribute : This attribute highly affects the output target (e.g., if duration=0 then y=&quot;no&quot;). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model. . 3. Finding Missing Values . 📝📝 There are several missing values in some categorical attributes, all coded with the &quot;unknown&quot; label. These missing values can be treated as a possible class label by leaving as it is or treated using deletion or imputation techniques. We need to check the performance on all the 3 cases and check how these approaches help. . #checking for missing values print(&quot;Missing values&quot; ,emoji.emojize(&quot;:red_question_mark:&quot;)*2,&quot; n&quot;) print(emoji.emojize(&quot;:check_mark_button:&quot;)*3 ,&quot; n n&quot;,data.isnull().values.any() ,&quot; n n&quot;,emoji.emojize(&quot;:check_mark_button:&quot;)*3) . . Missing values ❓❓ ✅✅✅ False ✅✅✅ . # Features containing &#39;unknown&#39; values # making a list of missing value types missing_values = [&#39;unknown&#39;] # reading data again with the defined non-standard missing value new_data = pd.read_csv(&#39;/content/drive/MyDrive/ColabNotebooks/1000_PBM/bank-additional-full.csv&#39;,sep=&#39;;&#39;, na_values = missing_values) print(&#39; nMissing Values as &quot;Unknown&quot; in the data : &#39; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) new_data.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#610646&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).highlight_null(null_color=&#39;#CCB3C5&#39;) . . Missing Values as &#34;Unknown&#34; in the data : 👇🏻👇🏻👇🏻 . age job marital education default housing loan contact month day_of_week duration campaign pdays previous poutcome emp.var.rate cons.price.idx cons.conf.idx euribor3m nr.employed y . 0 56 | housemaid | married | basic.4y | no | no | no | telephone | may | mon | 261 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 1 57 | services | married | high.school | nan | no | no | telephone | may | mon | 149 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 2 37 | services | married | high.school | no | yes | no | telephone | may | mon | 226 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 3 40 | admin. | married | basic.6y | no | no | no | telephone | may | mon | 151 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 4 56 | services | married | high.school | no | no | yes | telephone | may | mon | 307 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 5 45 | services | married | basic.9y | nan | no | no | telephone | may | mon | 198 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 6 59 | admin. | married | professional.course | no | no | no | telephone | may | mon | 139 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 7 41 | blue-collar | married | nan | nan | no | no | telephone | may | mon | 217 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 8 24 | technician | single | professional.course | no | yes | no | telephone | may | mon | 380 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 9 25 | services | single | high.school | no | yes | no | telephone | may | mon | 50 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . #getting total no. of &#39;unknown&#39; values print(&#39; nCount of Missing Values as &quot;Unknown&quot; in each column of the data : n n&#39; ,&quot; &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) new_data.isnull().sum().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A15F86&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Count of Missing Values as &#34;Unknown&#34; in each column of the data : 👇🏻👇🏻👇🏻 . 0 . age 0 | . job 330 | . marital 80 | . education 1731 | . default 8597 | . housing 990 | . loan 990 | . contact 0 | . month 0 | . day_of_week 0 | . duration 0 | . campaign 0 | . pdays 0 | . previous 0 | . poutcome 0 | . emp.var.rate 0 | . cons.price.idx 0 | . cons.conf.idx 0 | . euribor3m 0 | . nr.employed 0 | . y 0 | . 📝📝 There are 6 categorical features - job, marital, education, default, housing &amp; loan containing the non-standard missing value &#39;unknown&#39;. . new_data.drop([&#39;age&#39;, &#39;job&#39;, &#39;marital&#39;, &#39;education&#39;, &#39;default&#39;, &#39;housing&#39;, &#39;loan&#39;, &#39;contact&#39;, &#39;month&#39;, &#39;day_of_week&#39;, &#39;duration&#39;, &#39;campaign&#39;, &#39;pdays&#39;, &#39;previous&#39;, &#39;poutcome&#39;, &#39;emp.var.rate&#39;, &#39;cons.price.idx&#39;, &#39;cons.conf.idx&#39;, &#39;euribor3m&#39;, &#39;nr.employed&#39;, &#39;y&#39;], axis=1,inplace=True) del new_data print(emojis.encode(&quot;:x:&quot;)*14 ,&quot; n nDataframe deleted successfully !! n n&quot; ,emojis.encode(&quot;:x:&quot;)*14) . . ❌❌❌❌❌❌❌❌❌❌❌❌❌❌ Dataframe deleted successfully !! ❌❌❌❌❌❌❌❌❌❌❌❌❌❌ . 4. Finding Features with one value . # All the features with their unique values print(&#39; nUnique Values in each column of the data : n n&#39; ,&quot; &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) for column in data.columns: print(emoji.emojize(&quot;:arrow_right:&quot;, use_aliases=True) ,column ,emoji.emojize(&quot;:1234:&quot;, use_aliases=True) ,data[column].nunique()) . . Unique Values in each column of the data : 👇🏻👇🏻👇🏻 ➡ age 🔢 78 ➡ job 🔢 12 ➡ marital 🔢 4 ➡ education 🔢 8 ➡ default 🔢 3 ➡ housing 🔢 3 ➡ loan 🔢 3 ➡ contact 🔢 2 ➡ month 🔢 10 ➡ day_of_week 🔢 5 ➡ duration 🔢 1544 ➡ campaign 🔢 42 ➡ pdays 🔢 27 ➡ previous 🔢 8 ➡ poutcome 🔢 3 ➡ emp_var_rate 🔢 10 ➡ cons_price_idx 🔢 26 ➡ cons_conf_idx 🔢 26 ➡ euribor3m 🔢 316 ➡ nr_employed 🔢 11 ➡ target 🔢 2 . 📝📝 There is no feature with 1 value. . 5. Exploring the numerical features . #list of numerical variables numerical_features = [feature for feature in data.columns if ((data[feature].dtypes!=&#39;O&#39;) &amp; (feature not in [&#39;target&#39;]))] print(&#39;No. of numerical variables&#39; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;)*2 ,len(numerical_features)) print(&quot;&quot;) #all the numerical variables for feature in numerical_features: print(&quot;The variable&quot; ,emoji.emojize(&quot;:memo:&quot;) ,&quot;&#39;{}&#39;&quot;.format(feature) ,&quot;has datatype&quot; ,emoji.emojize(&quot;:1234:&quot;, use_aliases=True) ,&quot;{}&quot;.format(data[feature].dtypes) ,&quot;and&quot; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;) ,&quot;{}&quot;.format(len(data[feature].unique())) ,&quot;unique values&quot;) . . No. of numerical variables 👉🏻👉🏻 10 The variable 📝 &#39;age&#39; has datatype 🔢 int64 and 👉🏻 78 unique values The variable 📝 &#39;duration&#39; has datatype 🔢 int64 and 👉🏻 1544 unique values The variable 📝 &#39;campaign&#39; has datatype 🔢 int64 and 👉🏻 42 unique values The variable 📝 &#39;pdays&#39; has datatype 🔢 int64 and 👉🏻 27 unique values The variable 📝 &#39;previous&#39; has datatype 🔢 int64 and 👉🏻 8 unique values The variable 📝 &#39;emp_var_rate&#39; has datatype 🔢 float64 and 👉🏻 10 unique values The variable 📝 &#39;cons_price_idx&#39; has datatype 🔢 float64 and 👉🏻 26 unique values The variable 📝 &#39;cons_conf_idx&#39; has datatype 🔢 float64 and 👉🏻 26 unique values The variable 📝 &#39;euribor3m&#39; has datatype 🔢 float64 and 👉🏻 316 unique values The variable 📝 &#39;nr_employed&#39; has datatype 🔢 float64 and 👉🏻 11 unique values . 6. Distribution of numerical features . def num_histplot(feature, dataset): &quot;&quot;&quot; It takes the numerical variable and dataset as input and plots the histogram for the particular. &quot;&quot;&quot; if feature in numerical_features: print(&quot; n t t &quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3 ,&quot;Distribution of &#39;{}&#39;&quot;.format(feature) ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(10,8)) sns.histplot(dataset[feature], kde=True, bins=10,color=&#39;#F78AB2&#39;) plt.show() else: print(emoji.emojize(&quot;:cross_mark:&quot;)*2 ,&quot;It&#39;s not a numerical feature !!!&quot; ,emoji.emojize(&quot;:cross_mark:&quot;)*2) . . 7. Relationship b/w numerical features and the target label . def num_boxplot_wrt_Y(feature, dataset): &quot;&quot;&quot; It takes the numerical variable and dataset as input and plots the boxplot for the particular with styler object of Skewness, Kurtosis, Median, Count, Mean ,Standard Deviation, Min. value, Q1, Q2, Q3, Q4, Max. value, IQR, Lower Outliers Limit ,Upper Outliers Limit, Lower Outliers Count with percentage, Upper Outliers Count with percentage,Outliers Count with percentage . &quot;&quot;&quot; if feature in numerical_features: print(&quot; n t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3 ,&quot;Boxplot of &#39;{}&#39; w.r.t target&quot;.format(feature) ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3) print(&quot;&quot;) #plotting the boxplot plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(10,8)) sns.boxplot(x=&#39;target&#39;,y=dataset[feature] , data=dataset, palette=&#39;flare&#39;) plt.xlabel(feature) plt.show() # Parameters to check presence of outliers in the distribution v1 = pd.DataFrame({&#39; Parameters &#39;: &#39; Skewness&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].skew())},index={&#39;1&#39;}) v2 = pd.DataFrame({&#39; Parameters &#39;: &#39; Kurtosis&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].kurtosis())},index={&#39;2&#39;}) v3 = pd.DataFrame({&#39; Parameters &#39;: &#39; Median&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].quantile())},index={&#39;3&#39;}) v4 = pd.DataFrame({&#39; Parameters &#39;: &#39; Count&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[0])},index={&#39;4&#39;}) v5 = pd.DataFrame({&#39; Parameters &#39;: &#39; Mean&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[1])},index={&#39;5&#39;}) v6 = pd.DataFrame({&#39; Parameters &#39;: &#39; Stand. Dev.&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[2])},index={&#39;6&#39;}) v7 = pd.DataFrame({&#39; Parameters &#39;: &#39; Minimum&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[3])},index={&#39;7&#39;}) v8 = pd.DataFrame({&#39; Parameters &#39;: &#39; Q1 (25%)&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[4])},index={&#39;8&#39;}) v9 = pd.DataFrame({&#39; Parameters &#39;: &#39; Q2 (50%)&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[5])},index={&#39;9&#39;}) v10 = pd.DataFrame({&#39; Parameters &#39;: &#39; Q3 (75%)&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[6])},index={&#39;10&#39;}) v11 = pd.DataFrame({&#39; Parameters &#39;: &#39; Maximum&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[7])},index={&#39;11&#39;}) #finding Interquartile range iqr = dataset[feature].quantile(q = 0.75) - dataset[feature].quantile(q = 0.25) v12 = pd.DataFrame({&#39; Parameters &#39;: &#39; IQR&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(iqr)},index={&#39;12&#39;}) #Outlier detection from IQR lower_outliers = dataset[feature].quantile(q = 0.25) - (iqr*1.5) v13 = pd.DataFrame({&#39; Parameters &#39;: &#39; Lower outliers Limit &#39; , &#39; Values &#39; : &quot;{:.2f} &quot;.format(lower_outliers)},index={&#39;13&#39;}) upper_outliers = dataset[feature].quantile(q = 0.75) + (iqr*1.5) v14 = pd.DataFrame({&#39; Parameters &#39;: &#39; Upper outliers Limit &#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(upper_outliers)},index={&#39;14&#39;}) #checking the presence of outliers with upper &amp; lower limits lower_out_count = dataset[(dataset[feature] &lt; (dataset[feature].quantile(q = 0.25)-(iqr*1.5)) )][feature].count() lower_out_pct = round(lower_out_count / dataset[feature].count() * 100, 1) v15 = pd.DataFrame({&#39; Parameters &#39;: &#39; Lower outliers count &#39; , &#39; Values &#39; : &quot;{} ({}%)&quot;.format(lower_out_count, lower_out_pct)},index={&#39;15&#39;}) upper_out_count = dataset[(dataset[feature] &gt; (dataset[feature].quantile(q = 0.75)+(iqr*1.5)) )][feature].count() upper_out_pct = round(upper_out_count / dataset[feature].count() * 100, 1) v16 = pd.DataFrame({&#39; Parameters &#39;: &#39; Upper outliers count &#39; , &#39; Values &#39; : &quot;{} ({}%)&quot;.format(upper_out_count, upper_out_pct)},index={&#39;16&#39;}) outliers = dataset[(dataset[feature]&lt; (dataset[feature].quantile(q = 0.25)-(iqr*1.5))) | (dataset[feature] &gt; (dataset[feature].quantile(q = 0.75)+(iqr*1.5)) )][feature].count() outliers_pct = round(outliers / dataset[feature].count() * 100, 1) v17 = pd.DataFrame({&#39; Parameters &#39;: &#39; Outliers count &#39; , &#39; Values &#39; : &quot;{} ({}%)&quot;.format(outliers, outliers_pct)},index={&#39;17&#39;}) result = pd.concat([v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17]) result.columns = [&#39; Parameters &#39;,&#39; Values &#39;] if outliers==0: print(&quot; n&quot;,emoji.emojize(&quot;:check_mark:&quot;)*2 ,emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*2 ,&quot; NO Outliers &quot; ,emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*2 ,emoji.emojize(&quot;:check_mark:&quot;)*2,&quot; n&quot;) not_outliers = dataset[(dataset[feature]&gt;= (dataset[feature].quantile(q = 0.25)-(iqr*1.5))) | (dataset[feature] &lt;= (dataset[feature].quantile(q = 0.75)+(iqr*1.5)) )][feature].count() not_outliers_pct = round(not_outliers / dataset[feature].count() * 100, 1) result = result.iloc[:-5,:] v18 = pd.DataFrame({&#39; Parameters &#39;: &#39; observation count w/o outliers &#39; , &#39; Values &#39; : &quot;{} ({}%)&quot;.format(not_outliers, not_outliers_pct)},index={&#39;13&#39;}) result = pd.concat([result,v18]) result.columns = [&#39; Parameters &#39;,&#39; Values &#39;] else: print(&quot; n&quot;,emoji.emojize(&quot;:cross_mark:&quot;)*2 ,emoji.emojize(&quot;:thumbs_down_light_skin_tone:&quot;)*2 ,&quot;Outliers Present&quot; ,emoji.emojize(&quot;:thumbs_down_light_skin_tone:&quot;)*2 ,emoji.emojize(&quot;:cross_mark:&quot;)*2,&quot; n&quot;) result = result.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#34495E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#FEF5E7&#39;}, subset=[&#39; Parameters &#39;]) return result else: print(emoji.emojize(&quot;:cross_mark:&quot;)*2 ,&quot;It&#39;s not a numerical feature !!!&quot; ,emoji.emojize(&quot;:cross_mark:&quot;)*2) . . Feature : age . num_histplot(&#39;age&#39;, data) print(&quot; n t t t&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: clients&#39; age&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;age&#39; 📈📈📈 . 📃📃 Discription: clients&#39; age 📃📃 . 📝📝 We can see that this plot is right-skewed,but also similar to normal distribution. In the above distribution, we can see that most of the customers are in the age range of 30-40. . num_boxplot_wrt_Y(&#39;age&#39;, data) . . 📈📈📈 Boxplot of &#39;age&#39; w.r.t target 📈📈📈 . ❌❌ 👎🏻👎🏻 Outliers Present 👎🏻👎🏻 ❌❌ . Parameters Values . 1 Skewness | 0.78 | . 2 Kurtosis | 0.79 | . 3 Median | 38.00 | . 4 Count | 41176.00 | . 5 Mean | 40.02 | . 6 Stand. Dev. | 10.42 | . 7 Minimum | 17.00 | . 8 Q1 (25%) | 32.00 | . 9 Q2 (50%) | 38.00 | . 10 Q3 (75%) | 47.00 | . 11 Maximum | 98.00 | . 12 IQR | 15.00 | . 13 Lower outliers Limit | 9.50 | . 14 Upper outliers Limit | 69.50 | . 15 Lower outliers count | 0 (0.0%) | . 16 Upper outliers count | 468 (1.1%) | . 17 Outliers count | 468 (1.1%) | . 📝📝 From the above boxplot we know that for both the customers that subscibed or didn&#39;t subscribe a term deposit, has a median age of around 38-40. We see distribution for clients, who subscribed a term deposit is more diffused,but it is because people who said yes are less. . # grouping the clients on the basis of age print(&#39; nGrouping the clients on the basis of age : n n&#39; ,&quot; t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data[&#39;age_bins&#39;] = pd.cut(data[&#39;age&#39;], bins = [data[&#39;age&#39;].min(), 30, 60, data[&#39;age&#39;].max()], labels=[&#39;Young&#39;, &#39;Adult&#39;, &#39;Senior&#39;]) group_age_target1 = pd.DataFrame(data.groupby([&#39;age_bins&#39;])[&#39;target&#39;].mean().multiply(100)) group_age_target = pd.DataFrame(data.groupby([&#39;age_bins&#39;])[&#39;target&#39;].mean().multiply(100)) group_age_target = group_age_target.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#E06689 &#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) group_age_target . . Grouping the clients on the basis of age : 👇🏻👇🏻👇🏻 . target . age_bins . Young 15.211497 | . Adult 9.429544 | . Senior 45.544554 | . # Display graph print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Plotting the clients on the basis of age group&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) print(&quot;&quot;) group_age_target1.plot.barh() plt.xlabel(&#39;Subscribed [%]&#39;); . . 📈📈 Plotting the clients on the basis of age group 📈📈 . 📝📝 It is very clear the relation betweem the subscription rate and age of customers: ✏️ 45.5% of Seniors (+60 years old) subscribed to the term deposit ✏️ less than 1 in 10 Adults (&gt;30 and &lt;=60 years old) subscribed ✏️ Young people were the 2nd group that subscribed the deposit corresponding to 1/6 of all young people. ✏️ Senior subscribers alone were almost as much as Young and Adults subscribers, respectively, all together. . Feature : campaign . num_histplot(&#39;campaign&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: number of contacts performed during this campaign and for this client included last contact&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;campaign&#39; 📈📈📈 . 📃📃 Discription: number of contacts performed during this campaign and for this client included last contact 📃📃 . num_boxplot_wrt_Y(&#39;campaign&#39;, data) . . 📈📈📈 Boxplot of &#39;campaign&#39; w.r.t target 📈📈📈 . ❌❌ 👎🏻👎🏻 Outliers Present 👎🏻👎🏻 ❌❌ . Parameters Values . 1 Skewness | 4.76 | . 2 Kurtosis | 36.97 | . 3 Median | 2.00 | . 4 Count | 41176.00 | . 5 Mean | 2.57 | . 6 Stand. Dev. | 2.77 | . 7 Minimum | 1.00 | . 8 Q1 (25%) | 1.00 | . 9 Q2 (50%) | 2.00 | . 10 Q3 (75%) | 3.00 | . 11 Maximum | 56.00 | . 12 IQR | 2.00 | . 13 Lower outliers Limit | -2.00 | . 14 Upper outliers Limit | 6.00 | . 15 Lower outliers count | 0 (0.0%) | . 16 Upper outliers count | 2406 (5.8%) | . 17 Outliers count | 2406 (5.8%) | . 📝📝 The &#39;campaign&#39; attribute shows the total no. of contacts performed by the bank during the marketing campaigns. We can see that the plot is right-skewed which shows that the no. of clients contacted who did not subscribed to the bank term deposit policy was more than the no. of clients were contacted that did subscribed to the bank term deposit. . Feature : pdays . num_histplot(&#39;pdays&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: number of days that passed by after the client was last contacted from a previous campaign&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;pdays&#39; 📈📈📈 . 📃📃 Discription: number of days that passed by after the client was last contacted from a previous campaign 📃📃 . 📝📝 This attribute shows the no. of days that passed by after the client was previously contacted by the campaign team. The value of pdays from 999 and upwards, means the client was not contacted previously. . print(&#39; nUnique Values in &quot;pdays&quot; n n&#39; ,&quot; &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data[&quot;pdays&quot;].value_counts().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#035753&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Unique Values in &#34;pdays&#34; 👇🏻👇🏻👇🏻 . pdays . 999 39661 | . 3 439 | . 6 412 | . 4 118 | . 9 64 | . 2 61 | . 7 60 | . 12 58 | . 10 52 | . 5 46 | . 13 36 | . 11 28 | . 1 26 | . 15 24 | . 14 20 | . 8 18 | . 0 15 | . 16 11 | . 17 8 | . 18 7 | . 19 3 | . 22 3 | . 21 2 | . 26 1 | . 20 1 | . 25 1 | . 27 1 | . 📝📝 It can be seen that the lesser the no. of days that passed by the higher the no. of contacts made previously. Most of the values are 999, which means that the most of the customers have never been contacted before. . num_boxplot_wrt_Y(&#39;pdays&#39;, data) . . 📈📈📈 Boxplot of &#39;pdays&#39; w.r.t target 📈📈📈 . ❌❌ 👎🏻👎🏻 Outliers Present 👎🏻👎🏻 ❌❌ . Parameters Values . 1 Skewness | -4.92 | . 2 Kurtosis | 22.22 | . 3 Median | 999.00 | . 4 Count | 41176.00 | . 5 Mean | 962.46 | . 6 Stand. Dev. | 186.94 | . 7 Minimum | 0.00 | . 8 Q1 (25%) | 999.00 | . 9 Q2 (50%) | 999.00 | . 10 Q3 (75%) | 999.00 | . 11 Maximum | 999.00 | . 12 IQR | 0.00 | . 13 Lower outliers Limit | 999.00 | . 14 Upper outliers Limit | 999.00 | . 15 Lower outliers count | 1515 (3.7%) | . 16 Upper outliers count | 0 (0.0%) | . 17 Outliers count | 1515 (3.7%) | . print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;No. of days passed by after the client was last contacted from a previous campaign&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) print(&quot;&quot;) plt.figure(figsize=(10,8)) sns.histplot(data=data, x=&quot;pdays&quot;, hue=&quot;target&quot;, multiple=&quot;stack&quot;) plt.show() . . 📈📈 No. of days passed by after the client was last contacted from a previous campaign 📈📈 . # counting the days that passed by after previous campaign print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Plotting the days that passed by after previous campaign&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) dummy = data.loc[(data[&#39;pdays&#39;]!=999) &amp; (data[&#39;target&#39;] == 1), &#39;pdays&#39;] print(&#39;Median: {:.2} n&#39;.format(dummy.median())) dummy.hist().grid(True) plt.title(&#39;Histogram&#39;) plt.xlabel(&#39;Counting days after contact n for those who subscribed&#39;); . . 📈📈 Plotting the days that passed by after previous campaign 📈📈 Median: 6.0 . 📝📝 Considering only the clients who had subscribed let&#39;s count the days that passed by after contact from a previous campaign. Most of the people will respond on the 6th day and within 8 days. . Feature : previous . num_histplot(&#39;previous&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: number of contacts performed before this campaign and for this client&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;previous&#39; 📈📈📈 . 📃📃 Discription: number of contacts performed before this campaign and for this client 📃📃 . print(&#39; nUnique Values in &quot;previous&quot; n n&#39; ,&quot; &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data[&quot;previous&quot;].value_counts().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#436C3A&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Unique Values in &#34;previous&#34; 👇🏻👇🏻👇🏻 . previous . 0 35551 | . 1 4561 | . 2 754 | . 3 216 | . 4 70 | . 5 18 | . 6 5 | . 7 1 | . print(&quot;People who were previously contacted with success&quot; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;)*2 ,&quot; {} n&quot;.format(data.poutcome.value_counts()[1])) data[data[&quot;target&quot;]==1][&quot;previous&quot;].value_counts().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#436C3A&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . People who were previously contacted with success 👉🏻👉🏻 4252 . previous . 0 3140 | . 1 967 | . 2 350 | . 3 128 | . 4 38 | . 5 13 | . 6 3 | . print(&quot;People who were previously contacted with failure&quot; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;)*2 ,&quot; {} n&quot;.format(data.poutcome.value_counts()[2])) data[data[&quot;target&quot;]==0][&quot;previous&quot;].value_counts().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#436C3A&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . People who were previously contacted with failure 👉🏻👉🏻 1373 . previous . 0 32411 | . 1 3594 | . 2 404 | . 3 88 | . 4 32 | . 5 5 | . 6 2 | . 7 1 | . 📝📝 People that were previously contacted subscribed in a much higher rate to the term deposit. The vast majority were never been contacted before. . print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Plotting the people who were previously contacted&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) plt.figure(figsize=(10,8)) sns.kdeplot(data=data, x=&quot;previous&quot;, hue=&quot;target&quot;, multiple=&quot;stack&quot;) plt.show() . . 📈📈 Plotting the people who were previously contacted 📈📈 . 📝📝 The previous feature is similarly distributed for both the classes in the target variable. From here we can see that mostly people who were not contacted previously have not subscribed to the bank term deposit. . print(&#39; n t&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Boxplot of the people who were previously contacted&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) plt.figure(figsize=(10,8)) sns.boxplot(x=&#39;previous&#39;, data=data) plt.xlabel(&#39;previous&#39;) plt.show() num_boxplot_wrt_Y(&#39;previous&#39;, data) . . 📈📈 Boxplot of the people who were previously contacted 📈📈 . 📈📈📈 Boxplot of &#39;previous&#39; w.r.t target 📈📈📈 . ❌❌ 👎🏻👎🏻 Outliers Present 👎🏻👎🏻 ❌❌ . Parameters Values . 1 Skewness | 3.83 | . 2 Kurtosis | 20.10 | . 3 Median | 0.00 | . 4 Count | 41176.00 | . 5 Mean | 0.17 | . 6 Stand. Dev. | 0.49 | . 7 Minimum | 0.00 | . 8 Q1 (25%) | 0.00 | . 9 Q2 (50%) | 0.00 | . 10 Q3 (75%) | 0.00 | . 11 Maximum | 7.00 | . 12 IQR | 0.00 | . 13 Lower outliers Limit | 0.00 | . 14 Upper outliers Limit | 0.00 | . 15 Lower outliers count | 0 (0.0%) | . 16 Upper outliers count | 5625 (13.7%) | . 17 Outliers count | 5625 (13.7%) | . Feature : emp_var_rate . num_histplot(&#39;emp_var_rate&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: employment variation rate - quarterly indicator&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;emp_var_rate&#39; 📈📈📈 . 📃📃 Discription: employment variation rate - quarterly indicator 📃📃 . print(&#39; nUnique Values in &quot;emp_var_rate&quot; n n&#39; ,&quot; t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data[&quot;emp_var_rate&quot;].value_counts().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#424B17&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Unique Values in &#34;emp_var_rate&#34; 👇🏻👇🏻👇🏻 . emp_var_rate . 1.4 16228 | . -1.8 9182 | . 1.1 7762 | . -0.1 3682 | . -2.9 1662 | . -3.4 1070 | . -1.7 773 | . -1.1 635 | . -3.0 172 | . -0.2 10 | . print(&#39; nEmployment Variation Rate with successful subscription n n&#39; ,&quot; t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data[data[&quot;target&quot;]==1][&quot;emp_var_rate&quot;].value_counts().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#424B17&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Employment Variation Rate with successful subscription 👇🏻👇🏻👇🏻 . emp_var_rate . -1.8 1461 | . 1.4 866 | . -2.9 593 | . -3.4 454 | . -1.7 403 | . -1.1 301 | . 1.1 240 | . -0.1 232 | . -3.0 88 | . -0.2 1 | . print(&#39; nEmployment Variation Rate with failure in subscription n n&#39; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data[data[&quot;target&quot;]==0][&quot;emp_var_rate&quot;].value_counts().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#424B17&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Employment Variation Rate with failure in subscription 👇🏻👇🏻👇🏻 . emp_var_rate . 1.4 15362 | . -1.8 7721 | . 1.1 7522 | . -0.1 3450 | . -2.9 1069 | . -3.4 616 | . -1.7 370 | . -1.1 334 | . -3.0 84 | . -0.2 9 | . num_boxplot_wrt_Y(&#39;emp_var_rate&#39;, data) . . 📈📈📈 Boxplot of &#39;emp_var_rate&#39; w.r.t target 📈📈📈 . ✔✔ 👍🏻👍🏻 NO Outliers 👍🏻👍🏻 ✔✔ . Parameters Values . 1 Skewness | -0.72 | . 2 Kurtosis | -1.06 | . 3 Median | 1.10 | . 4 Count | 41176.00 | . 5 Mean | 0.08 | . 6 Stand. Dev. | 1.57 | . 7 Minimum | -3.40 | . 8 Q1 (25%) | -1.80 | . 9 Q2 (50%) | 1.10 | . 10 Q3 (75%) | 1.40 | . 11 Maximum | 1.40 | . 12 IQR | 3.20 | . 13 observation count w/o outliers | 41176 (100.0%) | . 📝📝 When the emp_var_rate (the employment rate) is negative there is a higher positive response to the campaign. . Feature : cons_price_idx . num_histplot( &#39;cons_price_idx&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: consumer price index - monthly indicator&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;cons_price_idx&#39; 📈📈📈 . 📃📃 Discription: consumer price index - monthly indicator 📃📃 . num_boxplot_wrt_Y( &#39;cons_price_idx&#39;, data) . . 📈📈📈 Boxplot of &#39;cons_price_idx&#39; w.r.t target 📈📈📈 . ✔✔ 👍🏻👍🏻 NO Outliers 👍🏻👍🏻 ✔✔ . Parameters Values . 1 Skewness | -0.23 | . 2 Kurtosis | -0.83 | . 3 Median | 93.75 | . 4 Count | 41176.00 | . 5 Mean | 93.58 | . 6 Stand. Dev. | 0.58 | . 7 Minimum | 92.20 | . 8 Q1 (25%) | 93.08 | . 9 Q2 (50%) | 93.75 | . 10 Q3 (75%) | 93.99 | . 11 Maximum | 94.77 | . 12 IQR | 0.92 | . 13 observation count w/o outliers | 41176 (100.0%) | . 📝📝 When the cons_price_idx (consumer price index) increases there is a strong negative response from the clients&#39; subscriptions. . Feature : cons_conf_idx . num_histplot(&#39;cons_conf_idx&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: consumer confidence index - monthly indicator&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;cons_conf_idx&#39; 📈📈📈 . 📃📃 Discription: consumer confidence index - monthly indicator 📃📃 . num_boxplot_wrt_Y(&#39;cons_conf_idx&#39;, data) . . 📈📈📈 Boxplot of &#39;cons_conf_idx&#39; w.r.t target 📈📈📈 . ❌❌ 👎🏻👎🏻 Outliers Present 👎🏻👎🏻 ❌❌ . Parameters Values . 1 Skewness | 0.30 | . 2 Kurtosis | -0.36 | . 3 Median | -41.80 | . 4 Count | 41176.00 | . 5 Mean | -40.50 | . 6 Stand. Dev. | 4.63 | . 7 Minimum | -50.80 | . 8 Q1 (25%) | -42.70 | . 9 Q2 (50%) | -41.80 | . 10 Q3 (75%) | -36.40 | . 11 Maximum | -26.90 | . 12 IQR | 6.30 | . 13 Lower outliers Limit | -52.15 | . 14 Upper outliers Limit | -26.95 | . 15 Lower outliers count | 0 (0.0%) | . 16 Upper outliers count | 446 (1.1%) | . 17 Outliers count | 446 (1.1%) | . Feature : euribor3m . num_histplot(&#39;euribor3m&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: euribor 3 month rate - daily indicator&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;euribor3m&#39; 📈📈📈 . 📃📃 Discription: euribor 3 month rate - daily indicator 📃📃 . num_boxplot_wrt_Y(&#39;euribor3m&#39;, data) . . 📈📈📈 Boxplot of &#39;euribor3m&#39; w.r.t target 📈📈📈 . ✔✔ 👍🏻👍🏻 NO Outliers 👍🏻👍🏻 ✔✔ . Parameters Values . 1 Skewness | -0.71 | . 2 Kurtosis | -1.41 | . 3 Median | 4.86 | . 4 Count | 41176.00 | . 5 Mean | 3.62 | . 6 Stand. Dev. | 1.73 | . 7 Minimum | 0.63 | . 8 Q1 (25%) | 1.34 | . 9 Q2 (50%) | 4.86 | . 10 Q3 (75%) | 4.96 | . 11 Maximum | 5.04 | . 12 IQR | 3.62 | . 13 observation count w/o outliers | 41176 (100.0%) | . 📝📝 The lower the euribor3m is, the higher the number of subscriptions. . Feature : nr_employed . num_histplot(&#39;nr_employed&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: number of employed citizens - quarterly indicator&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;nr_employed&#39; 📈📈📈 . 📃📃 Discription: number of employed citizens - quarterly indicator 📃📃 . print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Plotting no. of employed people&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) plt.figure(figsize=(10,8)) sns.histplot(data=data, x=&quot;nr_employed&quot;, hue=&quot;target&quot;, multiple=&quot;stack&quot;) plt.show() . . 📈📈 Plotting no. of employed people 📈📈 . num_boxplot_wrt_Y(&#39;nr_employed&#39;, data) . . 📈📈📈 Boxplot of &#39;nr_employed&#39; w.r.t target 📈📈📈 . ✔✔ 👍🏻👍🏻 NO Outliers 👍🏻👍🏻 ✔✔ . Parameters Values . 1 Skewness | -1.04 | . 2 Kurtosis | -0.00 | . 3 Median | 5191.00 | . 4 Count | 41176.00 | . 5 Mean | 5167.03 | . 6 Stand. Dev. | 72.25 | . 7 Minimum | 4963.60 | . 8 Q1 (25%) | 5099.10 | . 9 Q2 (50%) | 5191.00 | . 10 Q3 (75%) | 5228.10 | . 11 Maximum | 5228.10 | . 12 IQR | 129.00 | . 13 observation count w/o outliers | 41176 (100.0%) | . 📝📝 From here, we can see that people that were contacted had higher rates of subscription. The above histogram shows that the first contacts were exclusively made to known clients resulting in a much more efficient campaign with a low number of employed people. . 8. Exploring categorical features. . # displaying each categorical feature with its unique no. of categories categorical_features = [feature for feature in data.columns if ((data[feature].dtypes==&#39;O&#39;) &amp; (feature not in [&#39;y&#39;]))] for feature in categorical_features: print(&quot; nThe variable &#39;{}&#39; has {} categories &quot;.format(feature,len(data[feature].unique())) ,&quot; n&quot; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3 ,&quot; n&quot; ,&quot;{}&quot;.format(data[feature].unique())) . . The variable &#39;job&#39; has 12 categories 👇🏻👇🏻👇🏻 [&#39;housemaid&#39; &#39;services&#39; &#39;admin.&#39; &#39;blue-collar&#39; &#39;technician&#39; &#39;retired&#39; &#39;management&#39; &#39;unemployed&#39; &#39;self-employed&#39; &#39;unknown&#39; &#39;entrepreneur&#39; &#39;student&#39;] The variable &#39;marital&#39; has 4 categories 👇🏻👇🏻👇🏻 [&#39;married&#39; &#39;single&#39; &#39;divorced&#39; &#39;unknown&#39;] The variable &#39;education&#39; has 8 categories 👇🏻👇🏻👇🏻 [&#39;basic.4y&#39; &#39;high.school&#39; &#39;basic.6y&#39; &#39;basic.9y&#39; &#39;professional.course&#39; &#39;unknown&#39; &#39;university.degree&#39; &#39;illiterate&#39;] The variable &#39;default&#39; has 3 categories 👇🏻👇🏻👇🏻 [&#39;no&#39; &#39;unknown&#39; &#39;yes&#39;] The variable &#39;housing&#39; has 3 categories 👇🏻👇🏻👇🏻 [&#39;no&#39; &#39;yes&#39; &#39;unknown&#39;] The variable &#39;loan&#39; has 3 categories 👇🏻👇🏻👇🏻 [&#39;no&#39; &#39;yes&#39; &#39;unknown&#39;] The variable &#39;contact&#39; has 2 categories 👇🏻👇🏻👇🏻 [&#39;telephone&#39; &#39;cellular&#39;] The variable &#39;month&#39; has 10 categories 👇🏻👇🏻👇🏻 [&#39;may&#39; &#39;jun&#39; &#39;jul&#39; &#39;aug&#39; &#39;oct&#39; &#39;nov&#39; &#39;dec&#39; &#39;mar&#39; &#39;apr&#39; &#39;sep&#39;] The variable &#39;day_of_week&#39; has 5 categories 👇🏻👇🏻👇🏻 [&#39;mon&#39; &#39;tue&#39; &#39;wed&#39; &#39;thu&#39; &#39;fri&#39;] The variable &#39;poutcome&#39; has 3 categories 👇🏻👇🏻👇🏻 [&#39;nonexistent&#39; &#39;failure&#39; &#39;success&#39;] . 📝📝 There are 10 categorical features with the above listed unique values.The variables &#39;job&#39; and &#39;month&#39; have the highest no. of categories. . 9. Distribution of categorical features. . def cat_countplot(cat_feature, dataset): &quot;&quot;&quot; Takes categorical feature and dataset as input and plots the countplot in decreasing order of the xlabel for the paricular. &quot;&quot;&quot; if cat_feature in categorical_features: print() print(&quot; n t t &quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3 ,&quot;Distribution of &#39;{}&#39;&quot;.format(cat_feature) ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(10,8)) Y = data[cat_feature] total = len(Y)*1 ax = sns.countplot(x=cat_feature,data=data,order=data[cat_feature].value_counts().index) plt.xlabel(cat_feature, fontsize = 20) #put 11 ticks (for 10 steps), from 0 to the total number of rows in the dataframe ax.yaxis.set_ticks(np.linspace(0, total, 11)) #adjust the ticklabel to the desired format, without changing the position of the ticks ax.set_yticklabels(map(&#39;{:.1f}%&#39;.format, 100*ax.yaxis.get_majorticklocs()/total)) ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=&quot;right&quot;) for p in ax.patches: ax.annotate(&#39;{:.1f}%&#39;.format(100*p.get_height()/total), (p.get_x()+0.1, p.get_height()+5), fontsize = 8) plt.show() else: print(emoji.emojize(&quot;:cross_mark:&quot;)*2 ,&quot;The entered feature is not categorical !!!!&quot; ,emoji.emojize(&quot;:cross_mark:&quot;)*2) . . 10. Relationship b/w categorical features and the target label . #finding the relationship b/w categorical variables and target label def cat_countplot_wrt_Y(cat_feature, dataset): &quot;&quot;&quot; Takes categorical feature and dataset as input and plots the countplot of the categorical feature w.r.t the target in decreasing order of the xalabel. &quot;&quot;&quot; if cat_feature in categorical_features: print(&quot; n t t &quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3 ,&quot;Distribution of &#39;{}&#39; w.r.t target&quot;.format(cat_feature) ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(10,8)) Y = data[cat_feature] total = len(Y)*1 ax = sns.countplot(x=cat_feature,data=data, hue=&quot;target&quot;,order=data[cat_feature].value_counts().index, palette=&#39;hls&#39;) plt.xlabel(cat_feature, fontsize = 20) #put 11 ticks (for 10 steps), from 0 to the total number of rows in the dataframe ax.yaxis.set_ticks(np.linspace(0, total, 11)) #adjust the ticklabel to the desired format, without changing the position of the ticks ax.set_yticklabels(map(&#39;{:.1f}%&#39;.format, 100*ax.yaxis.get_majorticklocs()/total)) ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=&quot;right&quot;) for p in ax.patches: ax.annotate(&#39;{:.1f}%&#39;.format(100*p.get_height()/total), (p.get_x()+0.1, p.get_height()+5), fontsize = 8) plt.show() else: print(emoji.emojize(&quot;:cross_mark:&quot;)*2 ,&quot;The entered feature is not categorical !!!!&quot; ,emoji.emojize(&quot;:cross_mark:&quot;)*2) . . Feature : job . cat_countplot(&#39;job&#39;, data) print(&quot; n t t t&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: type of job&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;job&#39; 📈📈📈 . 📃📃 Discription: type of job 📃📃 . 📝📝 The plot of job shows that the &#39;admin.&#39; job has the highest no. of employees. The &#39;blue-collar&#39; job has the second highest no. of employees. The rest can be clearly interpreted in a similar manner from the bar-plot. The 5 most common jobs are enough to represent 80% of the data. . cat_countplot_wrt_Y(&#39;job&#39;, data) . . 📈📈📈 Distribution of &#39;job&#39; w.r.t target 📈📈📈 . 📝📝 From the bar-plot of job we can see that the job type that subscribed to a term deposit the most is &#39;admin.&#39;, followed by &#39;blue-collar&#39; and &#39;students&#39; have the least subscription to a term deposit. . print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Plotting customers by profession and age&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) type_pivot = data.pivot_table( columns=&quot;target&quot;, index=&quot;job&quot;, values=&quot;age&quot;, aggfunc=np.mean) type_pivot.sort_values(by=[&quot;job&quot;], ascending=True).plot(kind=&quot;bar&quot;, title=(&quot;Type of customer by professional occupation and age&quot;), figsize=(6,4), fontsize = 12); . . 📈📈 Plotting customers by profession and age 📈📈 . 📝📝 We see that ‘retired’ and ‘housemaid’ are the oldest clients and the ones who have accepted the subscription more than any of the other classes. . Feature : marital . cat_countplot(&#39;marital&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: Marital situation&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;marital&#39; 📈📈📈 . 📃📃 Discription: Marital situation 📃📃 . 📝📝 The clients who are &#39;married&#39; are high in records. . cat_countplot_wrt_Y(&#39;marital&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: Marriage Status&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;marital&#39; w.r.t target 📈📈📈 . 📃📃 Discription: Marriage Status 📃📃 . 📝📝 The no. of people who have not subscribed to a term deposit most are &#39;married&#39;. . Feature : education . cat_countplot(&#39;education&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: Education level&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;education&#39; 📈📈📈 . 📃📃 Discription: Education level 📃📃 . 📝📝 ‘Education’ has 8 unique values. The top 4 education levels correspond to 80% of the data. The people having &#39;university.degree&#39; as highest education show more interest in a term deposit followed by the people having done &#39;high.school&#39; and &#39;illiterate&#39; people are not interested in a term deposit. . cat_countplot_wrt_Y(&#39;education&#39;, data) . . 📈📈📈 Distribution of &#39;education&#39; w.r.t target 📈📈📈 . 📝📝 Also the highest refusal to the subscription is made by the &#39;university.degree&#39; holders followed by &#39;high.school&#39; graduated. Clients with 4 years basic or illiterate are the oldest and prone to subscribe to the product. . Feature : default . cat_countplot(&#39;default&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: has credit in default?&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;default&#39; 📈📈📈 . 📃📃 Discription: has credit in default? 📃📃 . 📝📝 The highest interest in a term deposit is shown by the people who are having no credit &#39;defaults&#39;. . cat_countplot_wrt_Y(&#39;default&#39;, data) . . 📈📈📈 Distribution of &#39;default&#39; w.r.t target 📈📈📈 . 📝📝 People with no credit defaults are not so interested in subscribing a term deposit. With 3 unique values, the class ‘yes’ is meaningless, the variable is unexpressive and totally imbalanced. So we will drop this column. . Feature: housing . cat_countplot(&#39;housing&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: has housing loan?&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;housing&#39; 📈📈📈 . 📃📃 Discription: has housing loan? 📃📃 . 📝📝 The proportion of ‘yes’ and ‘no’ is very tight might reduce its predictive power. . cat_countplot_wrt_Y(&#39;housing&#39;, data) . . 📈📈📈 Distribution of &#39;housing&#39; w.r.t target 📈📈📈 . 📝📝 People with &#39;housing&#39; loan or without &#39;housing&#39; loan do not make much difference in the subscription of a term deposit. . Feature: loan . cat_countplot(&#39;loan&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: has personal loan?&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;loan&#39; 📈📈📈 . 📃📃 Discription: has personal loan? 📃📃 . 📝📝 ‘loan’ shows a high number of non-subscribers, this variable has some similarities with ‘housing’ in the sense that, proportionally, ‘yes’ and ‘no’ are very even. Once again, it might reduce its predictive power. . cat_countplot_wrt_Y(&#39;loan&#39;, data) . . 📈📈📈 Distribution of &#39;loan&#39; w.r.t target 📈📈📈 . 📝📝 People with no personal &#39;loan&#39; tends to be more interested in a term deposit subscription. . Feature: contact . cat_countplot(&#39;contact&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: contact communication type&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;contact&#39; 📈📈📈 . 📃📃 Discription: contact communication type 📃📃 . 📝📝 Most of the people are contacted through &#39;cellular&#39;. . cat_countplot_wrt_Y(&#39;contact&#39;, data) . . 📈📈📈 Distribution of &#39;contact&#39; w.r.t target 📈📈📈 . 📝📝 When the people are contacted through &#39;cellular&#39; tends to subscribe as well as unsubscribe more than contacted through &#39;telephone&#39;. . Feature: month . cat_countplot(&#39;month&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: last contact month of year&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;month&#39; 📈📈📈 . 📃📃 Discription: last contact month of year 📃📃 . 📝📝 Most of the people were last contacted in the month of May then followed by July &amp; August to a term subscription and least contacted lastly in December. . cat_countplot_wrt_Y(&#39;month&#39;, data) . . 📈📈📈 Distribution of &#39;month&#39; w.r.t target 📈📈📈 . 📝📝 The highest no. of subscription and unsubscription are also made in the month of May then followed by July &amp; August. . Feature: day_of_week . cat_countplot(&#39;day_of_week&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: last contact day of the week&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;day_of_week&#39; 📈📈📈 . 📃📃 Discription: last contact day of the week 📃📃 . cat_countplot_wrt_Y(&#39;day_of_week&#39;, data) . . 📈📈📈 Distribution of &#39;day_of_week&#39; w.r.t target 📈📈📈 . 📝📝 This feature doesnt make any significant change on the subscription of a term deposit and it can be dropped. . Feature: poutcome . cat_countplot(&#39;poutcome&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: outcome of the previous marketing campaign&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . 📈📈📈 Distribution of &#39;poutcome&#39; 📈📈📈 . 📃📃 Discription: outcome of the previous marketing campaign 📃📃 . 📝📝 The outcome in the previous campaigns is mostly as &#39;nonexistent&#39; followed by &#39;failure&#39; &amp; &#39;success&#39;. . cat_countplot_wrt_Y(&#39;poutcome&#39;, data) . . 📈📈📈 Distribution of &#39;poutcome&#39; w.r.t target 📈📈📈 . 📝📝 The plot shows &#39;nonexistent&#39; as the outcome where clients that didn&#39;t subscribed to a term deposit is more than the no. of clients that subscribed, &#39;failure&#39; is the outcome where large no. of clients refused to subscribe than to subscribe to a term deposit, &#39;success&#39; is the outcome where large no. of clients have subscribed than not subscribed a term deposit in the bank. Interestingly between the clients previously contacted from previous promotional campaigns that actually succeed, the majority subscribed this time. . 11. Exploring most common categories of categorical features . # What are the most common categories? print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*10 ,&#39; t&#39; ,&#39;Plotting most common categories&#39; ,&#39; t&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*10,&#39; n&#39;) total = len(categorical_features) plotnumber=1 plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(16,14)) for feature in data[categorical_features]: ax = plt.subplot(round(total/2),round(total/3), plotnumber) sns.countplot(x=data[feature], data=data,order=data[feature].value_counts().index) plt.xticks(rotation=90) plt.title(feature) plotnumber+=1 plt.tight_layout() . . 📈📈📈📈📈📈📈📈📈📈 Plotting most common categories 📈📈📈📈📈📈📈📈📈📈 . 📝📝 Most common categories are: ✏️ job: administrative ✏️ marital state: married ✏️ education: university degree ✏️ credit in default: no ✏️ housing: yes, however not having a housing loan is very close ✏️ loan: no ✏️ poutcome: did not participate in previous campaigns All the features include the category ‘unknown’ except the ‘poutcome’ variable. . 12. Exploring the influence of common categories on the target . # How these categories influence the target variable? print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*10 ,&#39; t&#39; ,&#39;Plotting most common categories influencing the target&#39; ,&#39; t&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*10,&#39; n&#39;) total = len(categorical_features) plotnumber=1 plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(16,14)) for feature in data[categorical_features]: ax = plt.subplot(round(total/2),round(total/3), plotnumber) data.groupby([feature])[&#39;target&#39;].mean().multiply(100).plot.barh() plt.xlabel(&#39;Subscribed [%]&#39;) plt.title(feature) plotnumber+=1 plt.tight_layout() . . 📈📈📈📈📈📈📈📈📈📈 Plotting most common categories influencing the target 📈📈📈📈📈📈📈📈📈📈 . 📝📝 Influence of common categories on the target : ✏️ ‘Student’ and ‘retired’ have the highest percentage of subscriptions (&gt;25%) whereas ‘blue-collar’ and ‘services’ have the lowest. ✏️ ‘Illiterate’ people have the highest percentage of subscriptions (&gt;20%), on the other hand ‘basic 9y’, ‘basic 6y’ and ‘basic 4y’ have the lowest. ✏️ People with credit in default did not subscribe. ✏️ More than 60% of the people previously contacted to other campaigns subscribed. ✏️ Marital state, existence of loans, and housing do not influence much the subscription rate. . 13. Exploring correlation between the numerical features . # Subdivision of target ynum = num_data.target Xnum = num_data.drop([&quot;target&quot;], axis= &quot;columns&quot;) . . # The numeric most correlated with the target (Pearson) print(&#39; nNumeric features most correlated with the target n n&#39; ,&quot; t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) pearson = num_data.corr() corr_target = pearson.target corr1 = corr_target.sort_values(ascending=False) corr1 = corr1.to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#139BB4&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).applymap(above_zero).format({&#39;target&#39;: &quot;{:.3f}&quot;}) corr1 . . Numeric features most correlated with the target 👇🏻👇🏻👇🏻 . target . target 1.000 | . duration 0.405 | . previous 0.230 | . cons_conf_idx 0.055 | . age 0.030 | . campaign -0.066 | . cons_price_idx -0.136 | . emp_var_rate -0.298 | . euribor3m -0.308 | . pdays -0.325 | . nr_employed -0.355 | . print(&quot; nOrdered by rank in absolute values n n&quot; ,&quot; t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) corr2 = corr_target.abs().sort_values(ascending=False) corr2 = corr2.to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#139BB4&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).applymap(above_zero).format({&#39;target&#39;: &quot;{:.3f}&quot;}) corr2 . . Ordered by rank in absolute values 👇🏻👇🏻👇🏻 . target . target 1.000 | . duration 0.405 | . nr_employed 0.355 | . pdays 0.325 | . euribor3m 0.308 | . emp_var_rate 0.298 | . previous 0.230 | . cons_price_idx 0.136 | . campaign 0.066 | . cons_conf_idx 0.055 | . age 0.030 | . # Heatmap visualization: Pearson print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8 ,&#39; t&#39; ,&#39;Heatmap visualization: Pearson&#39; ,&#39; t&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8,&#39; n&#39;) mask = np.triu(num_data.corr(method=&quot;pearson&quot;), 1) plt.figure(figsize=(19, 9)) sns.heatmap(num_data.corr(method=&quot;pearson&quot;), annot=True, vmax=1, vmin = -1, square=True, mask=mask) plt.title(&#39;Correlation b/w different features&#39;) plt.show() . . 📈📈📈📈📈📈📈📈 Heatmap visualization: Pearson 📈📈📈📈📈📈📈📈 . 📝📝 ‘Nr_employed’ is the most correlated with the target followed by ‘pdays’, ‘euribor3m’, and ‘emp_avr_rate’ and at the same time, the strength of their relationships with the target is low. . # Checking the predicting power of the features print(&quot; nChecking the predicting power of the features n n&quot; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) # Identifying variables with predictive power (Pearson Correlation p-value) corr_df = pd.DataFrame( [scipy.stats.pearsonr(Xnum[col], ynum) for col in Xnum.columns], columns=[&quot;Pearson Corr.&quot;, &quot;p-value&quot;], index=Xnum.columns, ) corr_df.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#636A92&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).applymap(above_zero, subset=[&#39;Pearson Corr.&#39;]).format({&#39;Pearson Corr.&#39;: &quot;{:.3f}&quot;,&#39;p-value&#39; : &quot;{:.3f}&quot;}) . . Checking the predicting power of the features 👇🏻👇🏻👇🏻 . Pearson Corr. p-value . age 0.030 | 0.000 | . duration 0.405 | 0.000 | . campaign -0.066 | 0.000 | . pdays -0.325 | 0.000 | . previous 0.230 | 0.000 | . emp_var_rate -0.298 | 0.000 | . cons_price_idx -0.136 | 0.000 | . cons_conf_idx 0.055 | 0.000 | . euribor3m -0.308 | 0.000 | . nr_employed -0.355 | 0.000 | . 📝📝 It leads us to conclude that all features have predictive power. . 📝📝 All linear variable’s relationships are monotonic at the same time but the inverse is not always true, simply because we can have both monotonic non-linear correlations. . # Numeric variables with higher monotonicity (spearman) data_spearman = num_data.copy() data_spearman.drop([&quot;target&quot;], axis=1, inplace=True) . . print(&quot; nThe 10 most correlated numerical pairs by Spearman method n n&quot; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) spearman_rank = pg.pairwise_corr(data_spearman, method=&#39;spearman&#39;).loc[:,[&#39;X&#39;,&#39;Y&#39;,&#39;r&#39;]] pos = spearman_rank.sort_values(kind=&quot;quicksort&quot;, by=[&#39;r&#39;], ascending=False).iloc[:5,:] neg = spearman_rank.sort_values(kind=&quot;quicksort&quot;, by=[&#39;r&#39;], ascending=False).iloc[-5:,:] con = pd.concat([pos,neg], axis=0) corr_pairs = pd.DataFrame(con.reset_index(drop=True)) corr_pairs.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#053975&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).applymap(above_zero, subset=[&#39;r&#39;]).format({&#39;r&#39; : &quot;{:.3f}&quot;}).set_properties(**{&#39;background-color&#39;: &#39;#D4E0EE&#39;}, subset=[&#39;X&#39;,&#39;Y&#39;]) . . The 10 most correlated numerical pairs by Spearman method 👇🏻👇🏻👇🏻 . X Y r . 0 emp_var_rate | nr_employed | 0.945 | . 1 emp_var_rate | euribor3m | 0.940 | . 2 euribor3m | nr_employed | 0.929 | . 3 emp_var_rate | cons_price_idx | 0.665 | . 4 cons_price_idx | euribor3m | 0.491 | . 5 previous | cons_price_idx | -0.283 | . 6 previous | emp_var_rate | -0.435 | . 7 previous | nr_employed | -0.439 | . 8 previous | euribor3m | -0.455 | . 9 pdays | previous | -0.510 | . # Heatmap visualization: Spearman print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8 ,&#39; t&#39; ,&#39;Heatmap visualization: Spearman&#39; ,&#39; t&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8,&#39; n&#39;) mask = np.triu(data_spearman.corr(method=&#39;spearman&#39;), 1) plt.figure(figsize=(19, 9)) sns.heatmap(data_spearman.corr(method=&#39;spearman&#39;), annot=True, vmax=1, vmin = -1, square=True, mask=mask) plt.title(&#39;Correlation b/w different features&#39;) plt.show() . . 📈📈📈📈📈📈📈📈 Heatmap visualization: Spearman 📈📈📈📈📈📈📈📈 . 📝📝 ‘nr_employed’ is the most correlated with the target. The variables ‘emp_var_rate’, ‘nr_employed’, and ‘euribor3m’ are very redundant but believe this does not represent a big issue. So we will keep all features for the time being. . Data Preprocessing . # To have a glimpse of the data print(&quot; nGlimpse of Categorical data : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) #Replacing &#39;unknown&#39; by NaN cat_data.replace(to_replace=&quot;unknown&quot;, value=np.nan, inplace=True) cat_data.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#610646&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).highlight_null(null_color=&#39;#CCB3C5&#39;) . . Glimpse of Categorical data : 👇🏻👇🏻👇🏻 . job marital education default housing loan contact month day_of_week poutcome . 0 housemaid | married | basic.4y | no | no | no | telephone | may | mon | nonexistent | . 1 services | married | high.school | nan | no | no | telephone | may | mon | nonexistent | . 2 services | married | high.school | no | yes | no | telephone | may | mon | nonexistent | . 3 admin. | married | basic.6y | no | no | no | telephone | may | mon | nonexistent | . 4 services | married | high.school | no | no | yes | telephone | may | mon | nonexistent | . 5 services | married | basic.9y | nan | no | no | telephone | may | mon | nonexistent | . 6 admin. | married | professional.course | no | no | no | telephone | may | mon | nonexistent | . 7 blue-collar | married | nan | nan | no | no | telephone | may | mon | nonexistent | . 8 technician | single | professional.course | no | yes | no | telephone | may | mon | nonexistent | . 9 services | single | high.school | no | yes | no | telephone | may | mon | nonexistent | . 1. Removing Unwanted Columns . #removing &#39;duration&#39;,&#39;default&#39;,&#39;day_of_week&#39; attributes cat_data = cat_data.drop([&quot;default&quot;,&quot;day_of_week&quot;],axis=1) num_data = num_data.drop(&quot;duration&quot;,axis=1) print(emojis.encode(&quot;:scissors:&quot;)*18 ,&quot; n nUnwanted columns successfully removed !!! n n&quot; ,emojis.encode(&quot;:scissors:&quot;)*18) . . ✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️ Unwanted columns successfully removed !!! ✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️ . print(&#39; nColumns &amp; Rows of both the dataframes n n&#39; ,&quot; t &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;Categorical Dataframe : &quot;,cat_data.shape) print(&quot;Numerical Dataframe : &quot;,num_data.shape) . . Columns &amp; Rows of both the dataframes 👇🏻👇🏻👇🏻 Categorical Dataframe : (41188, 8) Numerical Dataframe : (41188, 10) . 2. Handling missing values . # Imputation of missing values by the modal value print(&#39; nImputating missing values by the modal value : &#39; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) #cat_data = cat_data.fillna(cat_data.value_counts().index[0]) cat_data_imputed = cat_data.apply(lambda x: x.fillna(x.value_counts().index[0])) cat_data_imputed.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#2B0E46&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Imputating missing values by the modal value : 👇🏻👇🏻👇🏻 . job marital education housing loan contact month poutcome . 0 housemaid | married | basic.4y | no | no | telephone | may | nonexistent | . 1 services | married | high.school | no | no | telephone | may | nonexistent | . 2 services | married | high.school | yes | no | telephone | may | nonexistent | . 3 admin. | married | basic.6y | no | no | telephone | may | nonexistent | . 4 services | married | high.school | no | yes | telephone | may | nonexistent | . 5 services | married | basic.9y | no | no | telephone | may | nonexistent | . 6 admin. | married | professional.course | no | no | telephone | may | nonexistent | . 7 blue-collar | married | university.degree | no | no | telephone | may | nonexistent | . 8 technician | single | professional.course | yes | no | telephone | may | nonexistent | . 9 services | single | high.school | yes | no | telephone | may | nonexistent | . 3. Handling Outliers . Feature : age . 📝📝 It looks like normal distribution. The skewness value should be between -1 and +1, and any major deviation from this range indicates the presence of extreme values(outliers). Assuming that age follows a Gaussian Distribution we will calculate the boundaries which differentiate the outliers. We have computed Interquantile range as 15. . #calculating the boundaries which differentiate the outliers IQR = num_data.age.quantile(0.75)-num_data.age.quantile(0.25) lower_bridge = num_data[&#39;age&#39;].quantile(0.25)-(IQR*1.5) upper_bridge = num_data[&#39;age&#39;].quantile(0.75)+(IQR*1.5) print(&#39; nLower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;lower boundary limit : {} nupper boundary limit : {}&quot;.format(lower_bridge,upper_bridge)) . . Lower &amp; Upper Limits 👇🏻👇🏻👇🏻 lower boundary limit : 9.5 upper boundary limit : 69.5 . 📝📝 The maximum value of age is 98 years and from here we can see that the age value above 69.5 should be treated as outliers. . #replace outliers with upper boundary limit num_data.loc[num_data[&#39;age&#39;]&gt;=69.5,&#39;age&#39;] = 69.5 print(emoji.emojize(&quot;:plus:&quot;)*14 ,&quot; n nOutliers successfully replaced n n&quot; ,emoji.emojize(&quot;:plus:&quot;)*14) . . ➕➕➕➕➕➕➕➕➕➕➕➕➕➕ Outliers successfully replaced ➕➕➕➕➕➕➕➕➕➕➕➕➕➕ . #boxplot and other information after handling outlier num_boxplot_wrt_Y(&#39;age&#39;, num_data) . . 📈📈📈 Boxplot of &#39;age&#39; w.r.t target 📈📈📈 . ✔✔ 👍🏻👍🏻 NO Outliers 👍🏻👍🏻 ✔✔ . Parameters Values . 1 Skewness | 0.57 | . 2 Kurtosis | -0.25 | . 3 Median | 38.00 | . 4 Count | 41188.00 | . 5 Mean | 39.94 | . 6 Stand. Dev. | 10.13 | . 7 Minimum | 17.00 | . 8 Q1 (25%) | 32.00 | . 9 Q2 (50%) | 38.00 | . 10 Q3 (75%) | 47.00 | . 11 Maximum | 69.50 | . 12 IQR | 15.00 | . 13 observation count w/o outliers | 41188 (100.0%) | . # displaying each categorical feature with its unique no. of categories # without the category &#39;unknown&#39; cat_features = [feature for feature in cat_data_imputed.columns if (cat_data_imputed[feature].dtypes==&#39;O&#39;)] print(&quot;Categorical variables after imputing &#39;unknown&#39; values.&quot;) for feature in cat_features: print(&quot; nThe variable &#39;{}&#39; has {} categories&quot;.format(feature,len(cat_data_imputed[feature].unique())) ,&quot; n&quot; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3 ,&quot; n&quot; ,&quot;{}&quot;.format(cat_data_imputed[feature].unique())) . . Categorical variables after imputing &#39;unknown&#39; values. The variable &#39;job&#39; has 11 categories 👇🏻👇🏻👇🏻 [&#39;housemaid&#39; &#39;services&#39; &#39;admin.&#39; &#39;blue-collar&#39; &#39;technician&#39; &#39;retired&#39; &#39;management&#39; &#39;unemployed&#39; &#39;self-employed&#39; &#39;entrepreneur&#39; &#39;student&#39;] The variable &#39;marital&#39; has 3 categories 👇🏻👇🏻👇🏻 [&#39;married&#39; &#39;single&#39; &#39;divorced&#39;] The variable &#39;education&#39; has 7 categories 👇🏻👇🏻👇🏻 [&#39;basic.4y&#39; &#39;high.school&#39; &#39;basic.6y&#39; &#39;basic.9y&#39; &#39;professional.course&#39; &#39;university.degree&#39; &#39;illiterate&#39;] The variable &#39;housing&#39; has 2 categories 👇🏻👇🏻👇🏻 [&#39;no&#39; &#39;yes&#39;] The variable &#39;loan&#39; has 2 categories 👇🏻👇🏻👇🏻 [&#39;no&#39; &#39;yes&#39;] The variable &#39;contact&#39; has 2 categories 👇🏻👇🏻👇🏻 [&#39;telephone&#39; &#39;cellular&#39;] The variable &#39;month&#39; has 10 categories 👇🏻👇🏻👇🏻 [&#39;may&#39; &#39;jun&#39; &#39;jul&#39; &#39;aug&#39; &#39;oct&#39; &#39;nov&#39; &#39;dec&#39; &#39;mar&#39; &#39;apr&#39; &#39;sep&#39;] The variable &#39;poutcome&#39; has 3 categories 👇🏻👇🏻👇🏻 [&#39;nonexistent&#39; &#39;failure&#39; &#39;success&#39;] . Feature : campaign . 📝📝 The histogram shows that &#39;campaign&#39; attribute is right-skewed and also the skewness value is 4.8. Now we will calculate the extreme boundaries which differentiate the outliers. We have computed Interquantile range as 2. . # Upper &amp; lower limits of outliers print(&#39; nLower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;lower boundary limit : {} nupper boundary limit : {}&quot;.format(lower_bridge,upper_bridge)) . . Lower &amp; Upper Limits 👇🏻👇🏻👇🏻 lower boundary limit : 9.5 upper boundary limit : 69.5 . # Extreme outliers print(&#39; n Extreme Lower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; t&quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;lower boundary limit : {} nupper boundary limit : {}&quot;.format(num_data[&#39;campaign&#39;].quantile(0.10),num_data[&#39;campaign&#39;].quantile(0.90))) . . Extreme Lower &amp; Upper Limits 👇🏻👇🏻👇🏻 lower boundary limit : 1.0 upper boundary limit : 5.0 . #replace outliers with extreme lower &amp; upper boundary limit num_data.loc[num_data[&#39;campaign&#39;]&lt;=1,&#39;campaign&#39;] = 1 num_data.loc[num_data[&#39;campaign&#39;]&gt;=5,&#39;campaign&#39;] = 5 print(emoji.emojize(&quot;:plus:&quot;)*20 ,&quot; n nOutliers successfully replaced with extreme limits n n&quot; ,emoji.emojize(&quot;:plus:&quot;)*20) . . ➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕ Outliers successfully replaced with extreme limits ➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕ . #boxplot and other information after handling outliers num_boxplot_wrt_Y(&#39;campaign&#39;, num_data) . . 📈📈📈 Boxplot of &#39;campaign&#39; w.r.t target 📈📈📈 . ✔✔ 👍🏻👍🏻 NO Outliers 👍🏻👍🏻 ✔✔ . Parameters Values . 1 Skewness | 0.94 | . 2 Kurtosis | -0.38 | . 3 Median | 2.00 | . 4 Count | 41188.00 | . 5 Mean | 2.19 | . 6 Stand. Dev. | 1.37 | . 7 Minimum | 1.00 | . 8 Q1 (25%) | 1.00 | . 9 Q2 (50%) | 2.00 | . 10 Q3 (75%) | 3.00 | . 11 Maximum | 5.00 | . 12 IQR | 2.00 | . 13 observation count w/o outliers | 41188 (100.0%) | . Feature : previous . 📝📝 The method used to identify outliers selects many records which is 14% . so decided to keep the records because the data seems to have been measured correctly and reflects reality. To emphasize that the model is not affected by the extension of the no. of outliers, we will only guarantee that we will use a standardization technique that does not neglect to detail the distances between the central values. . Feature : cons_conf_idx . 📝📝 Assuming that &#39;cons_conf_idx&#39; follows a Gaussian Distribution we will calculate the boundaries which differentiate the outliers. We have computed Interquantile range as 6.3. . #calculating the boundaries which differentiate the outliers print(&#39; nLower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) IQR = num_data.cons_conf_idx.quantile(0.75)-num_data.cons_conf_idx.quantile(0.25) lower_bridge = num_data[&#39;cons_conf_idx&#39;].quantile(0.25)-(IQR*1.5) upper_bridge = num_data[&#39;cons_conf_idx&#39;].quantile(0.75)+(IQR*1.5) print(&quot;lower boundary limit : {:.2f} nupper boundary limit : {:.2f}&quot;.format(lower_bridge,upper_bridge)) . . Lower &amp; Upper Limits 👇🏻👇🏻👇🏻 lower boundary limit : -52.15 upper boundary limit : -26.95 . # Extreme outliers print(&#39; nExtreme Lower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;lower boundary limit : {} nupper boundary limit : {}&quot;.format(num_data[&#39;cons_conf_idx&#39;].quantile(0.10),num_data[&#39;cons_conf_idx&#39;].quantile(0.90))) . . Extreme Lower &amp; Upper Limits 👇🏻👇🏻👇🏻 lower boundary limit : -46.2 upper boundary limit : -36.1 . #replace outliers with upper boundary limit num_data.loc[num_data[&#39;cons_conf_idx&#39;]&gt;=-36,&#39;cons_conf_idx&#39;] = -36 print(emoji.emojize(&quot;:plus:&quot;)*20 ,&quot; n nOutliers successfully replaced with extreme limits n n&quot; ,emoji.emojize(&quot;:plus:&quot;)*20) . . ➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕ Outliers successfully replaced with extreme limits ➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕ . #boxplot and other information after handling outliers num_boxplot_wrt_Y(&#39;cons_conf_idx&#39;, num_data) . . 📈📈📈 Boxplot of &#39;cons_conf_idx&#39; w.r.t target 📈📈📈 . ✔✔ 👍🏻👍🏻 NO Outliers 👍🏻👍🏻 ✔✔ . Parameters Values . 1 Skewness | -0.19 | . 2 Kurtosis | -1.21 | . 3 Median | -41.80 | . 4 Count | 41188.00 | . 5 Mean | -40.82 | . 6 Stand. Dev. | 4.07 | . 7 Minimum | -50.80 | . 8 Q1 (25%) | -42.70 | . 9 Q2 (50%) | -41.80 | . 10 Q3 (75%) | -36.40 | . 11 Maximum | -36.00 | . 12 IQR | 6.30 | . 13 observation count w/o outliers | 41188 (100.0%) | . 4.Transformation . Feature : pdays . 📝📝 Our dataset is not evenly distributed as the values in pdays are out of range so we need to scale it. splitting &#39;pdays&#39; feature into 2 features - &#39;pdays1&#39; and &#39;pdays2&#39; . # creating a new column named &quot;pdays2&quot; based on the value in &quot;pdays&quot; column def function (row): if(row[&#39;pdays&#39;]==999): return 0; return 1; num_data[&#39;pdays2&#39;]=num_data.apply(lambda row: function(row),axis=1) # changing the value 999 in pdays column to value 30 def function1 (row): if(row[&#39;pdays&#39;]==999): return 30; return row[&#39;pdays&#39;]; num_data[&#39;pdays&#39;]=num_data.apply(lambda row: function1(row),axis=1) print(emoji.emojize(&quot;:plus:&quot;)*17 ,&quot; n nNew column &#39;pdays2&#39; created &amp; replaced n the 999 values with 30 in &#39;pdays&#39; n n&quot; ,emoji.emojize(&quot;:plus:&quot;)*17) . . ➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕ New column &#39;pdays2&#39; created &amp; replaced the 999 values with 30 in &#39;pdays&#39; ➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕➕ . #changing the type of pdays to int num_data[&#39;pdays&#39;]=num_data[&#39;pdays&#39;].astype(&#39;int64&#39;) #renaming column pdays to pdays1 num_data.rename(columns={&#39;pdays&#39;: &#39;pdays1&#39;},inplace=True) print(emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*20 ,&quot; n nSuccessfully converted the type of &#39;pdays&#39; n&amp; renamed to &#39;pdays1&#39; n n&quot; ,emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*20) . . 👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻 Successfully converted the type of &#39;pdays&#39; &amp; renamed to &#39;pdays1&#39; 👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻 . 5. Balancing the target variable . print(&quot; nComplete Dataset : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data_new = pd.concat([num_data,cat_data_imputed],axis=1) data_new.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A60B2E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Complete Dataset : 👇🏻👇🏻👇🏻 . age campaign pdays1 previous emp_var_rate cons_price_idx cons_conf_idx euribor3m nr_employed target pdays2 job marital education housing loan contact month poutcome . 0 56.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | housemaid | married | basic.4y | no | no | telephone | may | nonexistent | . 1 57.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | services | married | high.school | no | no | telephone | may | nonexistent | . 2 37.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | services | married | high.school | yes | no | telephone | may | nonexistent | . 3 40.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | admin. | married | basic.6y | no | no | telephone | may | nonexistent | . 4 56.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | services | married | high.school | no | yes | telephone | may | nonexistent | . 5 45.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | services | married | basic.9y | no | no | telephone | may | nonexistent | . 6 59.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | admin. | married | professional.course | no | no | telephone | may | nonexistent | . 7 41.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | blue-collar | married | university.degree | no | no | telephone | may | nonexistent | . 8 24.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | technician | single | professional.course | yes | no | telephone | may | nonexistent | . 9 25.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | services | single | high.school | yes | no | telephone | may | nonexistent | . data_new.shape . . (41188, 19) . 📝📝 Data corresponding to y is very skewed, so we duplicate the tuples corresponding to &#39;yes&#39; . #handling the imbalance dataset data1=data_new.copy() data2=data1[data1.target==1] data1=pd.concat([data1, data2]) data1=pd.concat([data1, data2]) data1=pd.concat([data1, data2]) data1=pd.concat([data1, data2]) data1=pd.concat([data1, data2]) data1=pd.concat([data1, data2]) data1=pd.concat([data1, data2]) bal_data=data1 print(emojis.encode(&quot;:crossed_flags:&quot;)*9 ,&quot; n nDataset is balanced n n&quot; ,emojis.encode(&quot;:crossed_flags:&quot;)*9) . . 🎌🎌🎌🎌🎌🎌🎌🎌🎌 Dataset is balanced 🎌🎌🎌🎌🎌🎌🎌🎌🎌 . #Checking the dataset is balanced or not based on target values in the classification. print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Plotting the balanced dataset&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) total = len(bal_data[&#39;target&#39;])*1 ax=sns.countplot(x=&#39;target&#39;,data=bal_data) for p in ax.patches: ax.annotate(&#39;{:.1f}%&#39;.format(100*p.get_height()/total), (p.get_x()+0.1, p.get_height()+5), fontsize = 12) . . 📈📈 Plotting the balanced dataset 📈📈 . 📝📝 Now the data looks much more balanced. . 6. Encoding the data . Label Encoding . print(&#39; nEncoded Data : &#39; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) le_data = bal_data.copy() le = preprocessing.LabelEncoder() le_data.job = le.fit_transform(le_data.job) le_data.marital = le.fit_transform(le_data.marital) le_data.education = le.fit_transform(le_data.education) le_data.housing = le.fit_transform(le_data.housing) le_data.loan = le.fit_transform(le_data.loan) le_data.contact = le.fit_transform(le_data.contact) le_data.month = le.fit_transform(le_data.month) le_data.poutcome = le.fit_transform(le_data.poutcome) le_data.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A15F86&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Encoded Data : 👇🏻👇🏻👇🏻 . age campaign pdays1 previous emp_var_rate cons_price_idx cons_conf_idx euribor3m nr_employed target pdays2 job marital education housing loan contact month poutcome . 0 56.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 3 | 1 | 0 | 0 | 0 | 1 | 6 | 1 | . 1 57.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 7 | 1 | 3 | 0 | 0 | 1 | 6 | 1 | . 2 37.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 7 | 1 | 3 | 1 | 0 | 1 | 6 | 1 | . 3 40.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 6 | 1 | . 4 56.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 7 | 1 | 3 | 0 | 1 | 1 | 6 | 1 | . 5 45.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 7 | 1 | 2 | 0 | 0 | 1 | 6 | 1 | . 6 59.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 0 | 1 | 5 | 0 | 0 | 1 | 6 | 1 | . 7 41.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 1 | 1 | 6 | 0 | 0 | 1 | 6 | 1 | . 8 24.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 9 | 2 | 5 | 1 | 0 | 1 | 6 | 1 | . 9 25.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 7 | 2 | 3 | 1 | 0 | 1 | 6 | 1 | . 7. Splitting the data into train &amp; test . # In label encoded data print(&#39; nLabel Encoded Data without Target :&#39; ,&quot; &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) # addinq a new column &#39;Target&#39; and dropping old column &#39;target&#39; ds = le_data.copy() ds[&quot;Target&quot;] = ds[&quot;target&quot;] ds = ds.drop(&quot;target&quot;,axis=1) ds.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#436C3A&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Label Encoded Data without Target : 👇🏻👇🏻👇🏻 . age campaign pdays1 previous emp_var_rate cons_price_idx cons_conf_idx euribor3m nr_employed pdays2 job marital education housing loan contact month poutcome Target . 0 56.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 3 | 1 | 0 | 0 | 0 | 1 | 6 | 1 | 0 | . 1 57.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 7 | 1 | 3 | 0 | 0 | 1 | 6 | 1 | 0 | . 2 37.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 7 | 1 | 3 | 1 | 0 | 1 | 6 | 1 | 0 | . 3 40.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 6 | 1 | 0 | . 4 56.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 7 | 1 | 3 | 0 | 1 | 1 | 6 | 1 | 0 | . 5 45.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 7 | 1 | 2 | 0 | 0 | 1 | 6 | 1 | 0 | . 6 59.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 1 | 5 | 0 | 0 | 1 | 6 | 1 | 0 | . 7 41.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 1 | 1 | 6 | 0 | 0 | 1 | 6 | 1 | 0 | . 8 24.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 9 | 2 | 5 | 1 | 0 | 1 | 6 | 1 | 0 | . 9 25.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 7 | 2 | 3 | 1 | 0 | 1 | 6 | 1 | 0 | . # Dividing the label encoded dataset into independent and dependent variables X = ds.iloc[:, : -1].values y = ds.iloc[:, -1].values # Splitting into train &amp; test data X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 42) print(emojis.encode(&quot;:scissors:&quot;)*22 ,&quot; n nData divided into Dependent &amp; Independent Variables nand Split into Train &amp; Test data n n&quot; ,emojis.encode(&quot;:scissors:&quot;)*22) . . ✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️ Data divided into Dependent &amp; Independent Variables and Split into Train &amp; Test data ✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️✂️ . 8. Feature Scaling . # Standardization in the label encoded data scaler = StandardScaler() X_train[:, : 10] = scaler.fit_transform(X_train[:, : 10]) X_test[:, : 10] = scaler.transform(X_test[:, : 10]) print(emoji.emojize(&quot;:triangular_flag:&quot;)*18 ,&quot; n nSuccessfully accomplished feature scaling n n&quot; ,emoji.emojize(&quot;:triangular_flag:&quot;)*18) . . 🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩 Successfully accomplished feature scaling 🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩🚩 . #convert array into dataframe X_train = pd.DataFrame(X_train) X_test = pd.DataFrame(X_test) y_train = pd.DataFrame(y_train) y_test = pd.DataFrame(y_test) print(emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*18 ,&quot; n nX_train,X_test,y_train &amp; y_test converted into dataframe n n&quot; ,emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*18) . . 👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻 X_train,X_test,y_train &amp; y_test converted into dataframe 👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻 . Building Models . # Confusion Matrix def confusion_mat(y_test,y_pred): &quot;&quot;&quot; Takes y_test &amp; y_pred of a model as input and return styler object of accuracy, precision,recall &amp; R1 Score for that particular model. &quot;&quot;&quot; cm = confusion_matrix(y_test,y_pred) print(&quot;Confusion Matrix : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3,&quot; n&quot;) plt.style.use(&#39;dark_background&#39;) sns.heatmap(cm,annot=True,fmt=&#39;d&#39;) plt.show() print(&quot;&quot;) accuracy = metrics.accuracy_score(y_pred, y_test)*100 precision = metrics.precision_score(y_pred,y_test)*100 recall = metrics.recall_score(y_pred,y_test)*100 r1_score = metrics.f1_score(y_pred,y_test)*100 v1 = pd.DataFrame({&#39;Parameters&#39;: &#39;Accuracy Score&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(accuracy)},index={&#39;1&#39;}) v2 = pd.DataFrame({&#39;Parameters&#39;: &#39;Precision Score&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(precision)},index={&#39;2&#39;}) v3 = pd.DataFrame({&#39;Parameters&#39;: &#39;Recall Score&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(recall)},index={&#39;3&#39;}) v4 = pd.DataFrame({&#39;Parameters&#39;: &#39;R1 Score&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(r1_score)},index={&#39;4&#39;}) result = pd.concat([v1,v2,v3,v4]) result.columns = [&#39;Parameters&#39;,&#39;Values&#39;] result = result.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#34495E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#FEF5E7&#39;}, subset=[&#39;Parameters&#39;]) return result . . # AUC_ROC curve def plot_roc_curve(y_test,y_pred,label,color): &quot;&quot;&quot; Takes y_test, y_pred, label &amp; color as input and plots the ROC curve. &quot;&quot;&quot; print(&quot; n&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&quot;Plotting ROC Curve&quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&quot; n&quot;) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(8,5)) plt.title(&#39;ROC Curve&#39;) # Computing False postive rate, and True positive rate fpr,tpr,threshold=roc_curve(y_test,y_pred) # Calculating Area under the curve to display on the plot auc = metrics.roc_auc_score(y_test,y_pred) # Now, plotting the computed values plt.plot(fpr, tpr,label = label , color=color, linewidth=2) # Custom settings for the plot plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.axis([-0.005, 1, 0, 1.005]) plt.xticks(np.arange(0,1, 0.05), rotation=90) plt.xlabel(&quot;False Positive Rate&quot;) plt.ylabel(&quot;True Positive Rate (Recall)&quot;) plt.legend(loc=&quot;lower right&quot;) plt.show() . . def kf_score(acc): &quot;&quot;&quot; Takes accuracy of the model as input and returns styler object of mean and standard deviation of accuracy of that particular model. &quot;&quot;&quot; r1 = pd.DataFrame({&#39;Score&#39;: &#39;Accuracy&#39;, &#39;Values&#39; : &quot;{:.2f}&quot;.format(acc.mean()*100)},index={&#39;1&#39;}) r2 = pd.DataFrame({&#39;Score&#39;: &#39;Stand. Dev.&#39;, &#39;Values&#39; : &quot;{:.2f}&quot;.format(acc.std()*100)},index={&#39;2&#39;}) res = pd.concat([r1,r2]) res.columns = [&#39;Score&#39;,&#39;Values&#39;] res=res.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A60B2E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#F2DBE0&#39;}, subset=[&#39;Score&#39;]) return res . . def grid_cv_params(model,param1,param2): &quot;&quot;&quot; Takes model and two parameters for which values are to be determined and returns styler object of Acuuracy and Best parameters. &quot;&quot;&quot; print(&quot;After Tuning Parameters : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3,&quot; n&quot;) p1 = model.best_params_[param1] p2 = model.best_params_[param2] r1 = pd.DataFrame({&#39;Parameters&#39;: &#39;Accuracy&#39;, &#39;Values&#39; : &quot;{:.2f}&quot;.format((model.best_score_)*100)},index={&#39;1&#39;}) r2 = pd.DataFrame({&#39;Parameters&#39;: &#39;Best Parameters&#39;, &#39;Values&#39; : &quot;&#39;{}&#39;: {} {} &#39;{}&#39;: &#39;{}&#39;&quot;.format(param1,p1,&quot;,&quot;,param2,p2)},index={&#39;2&#39;}) res = pd.concat([r1,r2]) res.columns = [&#39;Parameters&#39;,&#39;Values&#39;] res=res.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#035753&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#CAEEE5&#39;}, subset=[&#39;Parameters&#39;]) return res . . 1. Linear Logistic Regression . Training the model . # Training the model model1 = LogisticRegression() model1 = model1.fit(X_train,y_train) . . Testing the model . y_pred1 = model1.predict(X_test) . . Confusion Matrix . confusion_mat(y_test,y_pred1) . . Confusion Matrix : 👇🏻👇🏻👇🏻 . . Parameters Values . 1 Accuracy Score | 73.42 | . 2 Precision Score | 65.09 | . 3 Recall Score | 78.23 | . 4 R1 Score | 71.06 | . ROC Curve . plot_roc_curve(y_test,y_pred1,&quot;LLR&quot;,&quot;blue&quot;) . . 📈📈 Plotting ROC Curve 📈📈 . Applying k-fold cross validation . acc_llr_kf = cross_val_score(estimator=model1, X=X_train, y=y_train, cv=10) . . kf_score(acc_llr_kf) . . Score Values . 1 Accuracy | 73.21 | . 2 Stand. Dev. | 0.55 | . 2. Polynomial Logistic Regression(with degree=3) . Training the model . poly_feat = PolynomialFeatures(degree=3) X_train_poly = poly_feat.fit_transform(X_train) . . # Training the model on the label encoded training set model2 = LogisticRegression() model2 = model2.fit(X_train_poly,y_train) . . Testing the model . y_pred2 = model2.predict(poly_feat.fit_transform(X_test)) . . Confusion Matrix . confusion_mat(y_test,y_pred2) . . Confusion Matrix : 👇🏻👇🏻👇🏻 . . Parameters Values . 1 Accuracy Score | 74.29 | . 2 Precision Score | 66.00 | . 3 Recall Score | 79.25 | . 4 R1 Score | 72.02 | . ROC Curve . plot_roc_curve(y_test,y_pred2,&quot;PLR&quot;,&quot;darkorange&quot;) . . 📈📈 Plotting ROC Curve 📈📈 . Applying k-fold cross validation . acc_plr_kf = cross_val_score(estimator=model2, X=X_train_poly, y=y_train, cv=10) . . kf_score(acc_plr_kf) . . Score Values . 1 Accuracy | 74.25 | . 2 Stand. Dev. | 0.46 | . 3. K-NN . Training the model . # Training the model on the label encoded training set model3 = KNeighborsClassifier(n_neighbors=5, weights=&#39;distance&#39;, metric=&#39;minkowski&#39;, p=2) model3 = model3.fit(X_train,y_train) . . Testing the model . y_pred3 = model3.predict(X_test) . . Confusion Matrix . confusion_mat(y_test,y_pred3) . . Confusion Matrix : 👇🏻👇🏻👇🏻 . . Parameters Values . 1 Accuracy Score | 89.29 | . 2 Precision Score | 99.93 | . 3 Recall Score | 82.44 | . 4 R1 Score | 90.34 | . ROC Curve . plot_roc_curve(y_test,y_pred3,&quot;K_NN&quot;,&quot;green&quot;) . . 📈📈 Plotting ROC Curve 📈📈 . Tuning parameters with GridsearchCV . param_grid3 = {&#39;n_neighbors&#39;:[3,4,5,6,7,8,9,10,11,12], &#39;weights&#39;:[&#39;uniform&#39;,&#39;distance&#39;]} model3_gscv = GridSearchCV(KNeighborsClassifier(), param_grid = param_grid3, cv=10) model3_gscv = model3_gscv.fit(X_train,y_train) . . grid_cv_params(model3_gscv,&#39;n_neighbors&#39;,&#39;weights&#39;) . . After Tuning Parameters : 👇🏻👇🏻👇🏻 . Parameters Values . 1 Accuracy | 90.84 | . 2 Best Parameters | &#39;n_neighbors&#39;: 3 , &#39;weights&#39;: &#39;distance&#39; | . 4. Random Forest . Training the model . # Training the model on the label encoded training set model4 = RandomForestClassifier(oob_score=True,n_estimators=100 , max_features=6, n_jobs=-1) model4 = model4.fit(X_train,y_train) . . Testing the model . y_pred4 = model4.predict(X_test) . . Confusion Matrix . confusion_mat(y_test,y_pred4) . . Confusion Matrix : 👇🏻👇🏻👇🏻 . . Parameters Values . 1 Accuracy Score | 95.84 | . 2 Precision Score | 99.95 | . 3 Recall Score | 92.38 | . 4 R1 Score | 96.02 | . ROC Curve . plot_roc_curve(y_test,y_pred4,&quot;RF&quot;,&quot;red&quot;) . . 📈📈 Plotting ROC Curve 📈📈 . Feature Importance from Random Forest Model . model4.feature_importances_ indices = np.argsort(model4.feature_importances_)[::-1] . . X_train = pd.DataFrame(X_train) indices = np.argsort(model4.feature_importances_)[::-1] feature_rank = pd.DataFrame( columns = [&#39;rank&#39;, &#39;feature&#39;, &#39;importance&#39;] ) for f in range(X_train.shape[1]): feature_rank.loc[f] = [f+1, X_train.columns[indices[f]], model4.feature_importances_[indices[f]]] plt.style.use(&#39;dark_background&#39;) ax=sns.barplot( y = &#39;feature&#39;, x = &#39;importance&#39;, data = feature_rank) plt.xticks(rotation=90) plt.show() . . Tuning parameters with GridsearchCV . param_grid4 = {&#39;max_features&#39;:[5,6,7,8,9,10,11,12],&#39;n_estimators&#39;:[50,60,70,80,90,100]} model4_gscv = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid4, cv=5,verbose=True, n_jobs=-1) model4_detector = model4_gscv.fit(X_train,y_train) . . Fitting 5 folds for each of 48 candidates, totalling 240 fits . grid_cv_params(model4_detector,&#39;n_estimators&#39;,&#39;max_features&#39;) . . After Tuning Parameters : 👇🏻👇🏻👇🏻 . Parameters Values . 1 Accuracy | 95.27 | . 2 Best Parameters | &#39;n_estimators&#39;: 70 , &#39;max_features&#39;: &#39;5&#39; | . 5. XGBoost . Training the model . # Training the model on the label encoded training set model5 = XGBClassifier(n_estimators=200 ,random_state = 42,n_jobs=-1,verbose=1) model5 = model5.fit(X_train,y_train) . . Testing the model . y_pred5 = model5.predict(X_test) . . Confusion Matrix . confusion_mat(y_test,y_pred5) . . Confusion Matrix : 👇🏻👇🏻👇🏻 . . Parameters Values . 1 Accuracy Score | 75.50 | . 2 Precision Score | 65.85 | . 3 Recall Score | 81.74 | . 4 R1 Score | 72.94 | . ROC Curve . plot_roc_curve(y_test,y_pred5,&quot;XGB&quot;,&quot;purple&quot;) . . 📈📈 Plotting ROC Curve 📈📈 . After tuning all parameters . xgb_estimator = XGBClassifier( learning_rate=0.01,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=1, subsample=0.8,colsample_bytree=0.8,n_jobs=-1,reg_alpa=1,scale_pos_weight=1,random_state=42,verbose=1) . . xgb_estimator.fit(X_train, y_train) . . XGBClassifier(colsample_bytree=0.8, gamma=1, learning_rate=0.01, max_depth=5, n_estimators=1000, n_jobs=-1, random_state=42, reg_alpa=1, subsample=0.8, verbose=1) . y_pred_xgb = xgb_estimator.predict(X_test) . . confusion_mat(y_test,y_pred_xgb) . . Confusion Matrix : 👇🏻👇🏻👇🏻 . . Parameters Values . 1 Accuracy Score | 76.16 | . 2 Precision Score | 66.45 | . 3 Recall Score | 82.61 | . 4 R1 Score | 73.65 | . plot_roc_curve(y_test,y_pred_xgb,&quot;Tuned XGB&quot;,&quot;pink&quot;) . . 📈📈 Plotting ROC Curve 📈📈 . Plotting AUC_ROC of all the Models . # AUC_ROC curve plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(8,5)) plt.title(&#39;AUC_ROC Curve&#39;) # Computing False postive rate, and True positive rate fpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred1) # Calculating Area under the curve to display on the plot auc1 = metrics.roc_auc_score(y_test,y_pred1) # Now, plotting the computed values plt.plot(fpr1, tpr1, label = &#39;LLG&#39;, color=&#39;blue&#39;) # Computing False postive rate, and True positive rate fpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred2) # Calculating Area under the curve to display on the plot auc2 = metrics.roc_auc_score(y_test,y_pred2) # Now, plotting the computed values plt.plot(fpr2, tpr2,label = &#39;PLG&#39;, color=&#39;darkorange&#39;) # Computing False postive rate, and True positive rate fpr3, tpr3, thresholds3 = roc_curve(y_test, y_pred3) # Calculating Area under the curve to display on the plot auc3 = metrics.roc_auc_score(y_test,y_pred3) # Now, plotting the computed values plt.plot(fpr3, tpr3, label = &#39;K-NN&#39;, color=&#39;green&#39;) # Computing False postive rate, and True positive rate fpr4, tpr4, thresholds4 = roc_curve(y_test, y_pred4) # Calculating Area under the curve to display on the plot auc4 = metrics.roc_auc_score(y_test,y_pred4) # Now, plotting the computed values plt.plot(fpr4, tpr4, label = &#39;RF&#39;, color=&#39;red&#39;) # Computing False postive rate, and True positive rate fpr5, tpr5, thresholds5 = roc_curve(y_test, y_pred5) # Calculating Area under the curve to display on the plot auc5 = metrics.roc_auc_score(y_test,y_pred5) # Now, plotting the computed values plt.plot(fpr5, tpr5, label = &#39;XGB&#39;, color=&#39;purple&#39;) x = np.linspace(0,1,num=50) plt.plot(x,x,color=&#39;lightgrey&#39;,linestyle=&#39;--&#39;,marker=&#39;&#39;,lw=2,label=&#39;random guess&#39;) plt.legend(fontsize = 10) plt.xlabel(&#39;False positive rate&#39;, fontsize = 18) plt.ylabel(&#39;True positive rate&#39;, fontsize = 18) plt.xlim(0,1.1) plt.ylim(0,1.1) plt.show() . . Conclusion . print(&quot; nModels accuracy metrics before tuning the parameters n n&quot; ,&quot; t t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) a1 = pd.DataFrame({&#39;Models&#39;: &#39;Linear Logistic Regression&#39; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(metrics.accuracy_score(y_pred1, y_test)*100) ,&#39;Precision&#39; : &quot;{:.2f}%&quot;.format(metrics.precision_score(y_pred1, y_test)*100) ,&#39;Recall&#39; : &quot;{:.2f}%&quot;.format(metrics.recall_score(y_pred1, y_test)*100) ,&#39;R1 Score&#39; : &quot;{:.2f}%&quot;.format(metrics.f1_score(y_pred1, y_test)*100)},index={&#39;1&#39;}) a2 = pd.DataFrame({&#39;Models&#39;: &#39;Polynomial Logistic Regression&#39; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(metrics.accuracy_score(y_pred2, y_test)*100) ,&#39;Precision&#39; : &quot;{:.2f}%&quot;.format(metrics.precision_score(y_pred2, y_test)*100) ,&#39;Recall&#39; : &quot;{:.2f}%&quot;.format(metrics.recall_score(y_pred2, y_test)*100) ,&#39;R1 Score&#39; : &quot;{:.2f}%&quot;.format(metrics.f1_score(y_pred2, y_test)*100)},index={&#39;2&#39;}) a3 = pd.DataFrame({&#39;Models&#39;: &#39;K-Nearest Neighbors Classifier&#39; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(metrics.accuracy_score(y_pred3, y_test)*100) ,&#39;Precision&#39; : &quot;{:.2f}%&quot;.format(metrics.precision_score(y_pred3, y_test)*100) ,&#39;Recall&#39; : &quot;{:.2f}%&quot;.format(metrics.recall_score(y_pred3, y_test)*100) ,&#39;R1 Score&#39; : &quot;{:.2f}%&quot;.format(metrics.f1_score(y_pred3, y_test)*100)},index={&#39;3&#39;}) a4 = pd.DataFrame({&#39;Models&#39;: &#39;Random Forest Classifier&#39; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(metrics.accuracy_score(y_pred4, y_test)*100) ,&#39;Precision&#39; : &quot;{:.2f}%&quot;.format(metrics.precision_score(y_pred4, y_test)*100) ,&#39;Recall&#39; : &quot;{:.2f}%&quot;.format(metrics.recall_score(y_pred4, y_test)*100) ,&#39;R1 Score&#39; : &quot;{:.2f}%&quot;.format(metrics.f1_score(y_pred4, y_test)*100)},index={&#39;4&#39;}) a5 = pd.DataFrame({&#39;Models&#39;: &#39;X-Gradient Boosting Classifier&#39; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(metrics.accuracy_score(y_pred5, y_test)*100) ,&#39;Precision&#39; : &quot;{:.2f}%&quot;.format(metrics.precision_score(y_pred5, y_test)*100) ,&#39;Recall&#39; : &quot;{:.2f}%&quot;.format(metrics.recall_score(y_pred5, y_test)*100) ,&#39;R1 Score&#39; : &quot;{:.2f}%&quot;.format(metrics.f1_score(y_pred5, y_test)*100)},index={&#39;5&#39;}) res_table = pd.concat([a1,a2,a3,a4,a5]) res_table.columns = [&#39;Models&#39;,&#39;Accuracy&#39;,&#39;Precision&#39;,&#39;Recall&#39;,&#39;R1 Score&#39;] res_table=res_table.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#636A92&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#FEF5E7&#39;}, subset=[&#39;Models&#39;]) res_table . . Models accuracy metrics before tuning the parameters 👇🏻👇🏻👇🏻 . Models Accuracy Precision Recall R1 Score . 1 Linear Logistic Regression | 73.42% | 65.09% | 78.23% | 71.06% | . 2 Polynomial Logistic Regression | 74.29% | 66.00% | 79.25% | 72.02% | . 3 K-Nearest Neighbors Classifier | 89.29% | 99.93% | 82.44% | 90.34% | . 4 Random Forest Classifier | 95.84% | 99.95% | 92.38% | 96.02% | . 5 X-Gradient Boosting Classifier | 75.50% | 65.85% | 81.74% | 72.94% | . print(&quot; nModels accuracy after tuning the parameters n n&quot; ,&quot; t t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) b1 = pd.DataFrame({&#39;Models&#39;: &quot;Linear Logistic Regression(With K-Fold Cross Validation)&quot; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(acc_llr_kf.mean()*100)},index={&#39;1&#39;}) b2 = pd.DataFrame({&#39;Models&#39;: &quot;Polynomial Logistic Regression(With K-Fold Cross Validation)&quot; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(acc_plr_kf.mean()*100)},index={&#39;2&#39;}) b3 = pd.DataFrame({&#39;Models&#39;: &quot;K-Nearest Neighbors Classifier(With Gridsearch CV)&quot; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format((model3_gscv.best_score_)*100)},index={&#39;3&#39;}) b4 = pd.DataFrame({&#39;Models&#39;: &quot;Random Forest Classifier(With Gridsearch CV)&quot; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format((model4_detector.best_score_)*100)},index={&#39;4&#39;}) b5 = pd.DataFrame({&#39;Models&#39;: &quot;X-Gradient Boosting Classifier(With tuning all parameters)&quot; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(metrics.accuracy_score(y_pred_xgb, y_test)*100)},index={&#39;5&#39;}) result_table = pd.concat([b1,b2,b3,b4,b5]) result_table.columns = [&#39;Models&#39;,&#39;Accuracy&#39;] result_table=result_table.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#E06689&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#FEF5E7&#39;}, subset=[&#39;Models&#39;]) result_table . . Models accuracy after tuning the parameters 👇🏻👇🏻👇🏻 . Models Accuracy . 1 Linear Logistic Regression(With K-Fold Cross Validation) | 73.21% | . 2 Polynomial Logistic Regression(With K-Fold Cross Validation) | 74.25% | . 3 K-Nearest Neighbors Classifier(With Gridsearch CV) | 90.84% | . 4 Random Forest Classifier(With Gridsearch CV) | 95.27% | . 5 X-Gradient Boosting Classifier(With tuning all parameters) | 76.16% | . Comparing the results of all the Models . df1_styler = res_table.set_table_attributes(&quot;style=&#39;display:inline&#39;&quot;).set_caption(&#39;Before tuning the parameters&#39;) df2_styler = result_table.set_table_attributes(&quot;style=&#39;display:inline&#39;&quot;).set_caption(&#39;After tuning the parameters&#39;) space = &quot; xa0&quot; * 10 display_html(df1_styler._repr_html_()+ space + df2_styler._repr_html_(), raw=True) . . Before tuning the parameters Models Accuracy Precision Recall R1 Score . 1 Linear Logistic Regression | 73.42% | 65.09% | 78.23% | 71.06% | . 2 Polynomial Logistic Regression | 74.29% | 66.00% | 79.25% | 72.02% | . 3 K-Nearest Neighbors Classifier | 89.29% | 99.93% | 82.44% | 90.34% | . 4 Random Forest Classifier | 95.84% | 99.95% | 92.38% | 96.02% | . 5 X-Gradient Boosting Classifier | 75.50% | 65.85% | 81.74% | 72.94% | .           After tuning the parameters Models Accuracy . 1 Linear Logistic Regression(With K-Fold Cross Validation) | 73.21% | . 2 Polynomial Logistic Regression(With K-Fold Cross Validation) | 74.25% | . 3 K-Nearest Neighbors Classifier(With Gridsearch CV) | 90.84% | . 4 Random Forest Classifier(With Gridsearch CV) | 95.27% | . 5 X-Gradient Boosting Classifier(With tuning all parameters) | 76.16% | .",
            "url": "https://swati5140.github.io/my_data_science_portfolio/2021/09/21/_08_26_bank_marketing_project.html",
            "relUrl": "/2021/09/21/_08_26_bank_marketing_project.html",
            "date": " • Sep 21, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: There was a &#39;Net::OpenTimeout&#39; error fetching URL: &#39;https://twitter.com/jakevdp/status/1204765621767901185?s=20&#39; . . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://swati5140.github.io/my_data_science_portfolio/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . There was a &#39;Net::OpenTimeout&#39; error fetching URL: &#39;https://twitter.com/jakevdp/status/1204765621767901185?s=20&#39; . Footnotes . This is the footnote. &#8617; . |",
            "url": "https://swati5140.github.io/my_data_science_portfolio/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Portfolio 1. . Website showcasing Ml projects &#8617; . |",
          "url": "https://swati5140.github.io/my_data_science_portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://swati5140.github.io/my_data_science_portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}