{
  
    
        "post0": {
            "title": "Predicting the subscription of a bank term deposit using machine learning",
            "content": "Connection to the drive . # Mounting drive from google.colab import drive drive.mount(&#39;/content/drive&#39;) . . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . Install and import the required libraries and packages . !pip install pingouin . Requirement already satisfied: pingouin in /usr/local/lib/python3.7/dist-packages (0.4.0) Requirement already satisfied: statsmodels&gt;=0.12.0 in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.12.2) Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.8.9) Requirement already satisfied: scipy&gt;=1.7 in /usr/local/lib/python3.7/dist-packages (from pingouin) (1.7.1) Requirement already satisfied: seaborn&gt;=0.9.0 in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.11.1) Requirement already satisfied: numpy&gt;=1.19 in /usr/local/lib/python3.7/dist-packages (from pingouin) (1.19.5) Requirement already satisfied: pandas&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from pingouin) (1.1.5) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.24.2) Requirement already satisfied: pandas-flavor&gt;=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.2.0) Requirement already satisfied: matplotlib&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from pingouin) (3.2.2) Requirement already satisfied: outdated in /usr/local/lib/python3.7/dist-packages (from pingouin) (0.2.1) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.0.2-&gt;pingouin) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.0.2-&gt;pingouin) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.0.2-&gt;pingouin) (1.3.1) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.0.2-&gt;pingouin) (2.8.2) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler&gt;=0.10-&gt;matplotlib&gt;=3.0.2-&gt;pingouin) (1.15.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=1.0-&gt;pingouin) (2018.9) Requirement already satisfied: xarray in /usr/local/lib/python3.7/dist-packages (from pandas-flavor&gt;=0.2.0-&gt;pingouin) (0.18.2) Requirement already satisfied: patsy&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels&gt;=0.12.0-&gt;pingouin) (0.5.1) Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated-&gt;pingouin) (0.2.2) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated-&gt;pingouin) (2.23.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;outdated-&gt;pingouin) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;outdated-&gt;pingouin) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;outdated-&gt;pingouin) (2021.5.30) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;outdated-&gt;pingouin) (2.10) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;pingouin) (2.2.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;pingouin) (1.0.1) Requirement already satisfied: setuptools&gt;=40.4 in /usr/local/lib/python3.7/dist-packages (from xarray-&gt;pandas-flavor&gt;=0.2.0-&gt;pingouin) (57.4.0) . . !pip install -U scikit-learn . Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.24.2) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.2.0) Requirement already satisfied: scipy&gt;=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.1) Requirement already satisfied: numpy&gt;=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1) . . !pip install emoji . Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.4.2) . . !pip install emojis . Requirement already satisfied: emojis in /usr/local/lib/python3.7/dist-packages (0.6.0) . . #Packages related to general operating system &amp; warnings import os import warnings warnings.filterwarnings(&#39;ignore&#39;) # importing required libraries and packages import emoji import emojis import pandas as pd from pandas.plotting import table import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pingouin as pg from matplotlib import colors from IPython.display import display_html import scipy from scipy.stats import pearsonr, spearmanr from sklearn import preprocessing from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn import metrics from sklearn.metrics import roc_auc_score,roc_curve,auc from sklearn.metrics import confusion_matrix, accuracy_score from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import cross_val_score, KFold, GridSearchCV from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier from xgboost import XGBClassifier print(emoji.emojize(&quot;:laptop:&quot;)*28 ,&quot; n nAll the required libraries and packages are imported successfully !!! n n&quot; ,emoji.emojize(&quot;:laptop:&quot;)*28) . . ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’» All the required libraries and packages are imported successfully !!! ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’»ğŸ’» . Load the dataset . #loading the datasets data = pd.read_csv(&#39;/content/drive/MyDrive/ColabNotebooks/1000_PBM/bank-additional-full.csv&#39;,sep=&#39;;&#39;) print(emoji.emojize(&quot;:file_folder:&quot;)*12 ,&quot; n nData loaded successfully !!! n n&quot; ,emoji.emojize(&quot;:file_folder:&quot;)*12) . . ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ Data loaded successfully !!! ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ğŸ“ . ğŸ“ğŸ“ In the datasets the data is separated using &#39;;&#39;, therefore, while reading the CSV file, we should instruct pandas about the separator as the default separator is &#39;,&#39;. If we do not instruct as mentioned, the dataframe will have all the data in one cell. . # highlighting postive &amp; negative values def above_zero(val): &quot;&quot;&quot; Takes value as input &amp; returns in green color if positive, in red color if negative and black otherwise. &quot;&quot;&quot; if val &gt; 0: color = &#39;green&#39; elif val &lt; 0: color = &#39;red&#39; else: color = &#39;black&#39; return &#39;color: %s&#39; % color . . # To have a glimpse of the data print(&quot; nGlimpse of data : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A60B2E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).highlight_max(color=&#39;#0074FF&#39;,axis=0) .highlight_min(color=&#39;#00FFE5&#39;,axis=0) . . Glimpse of data : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . age job marital education default housing loan contact month day_of_week duration campaign pdays previous poutcome emp.var.rate cons.price.idx cons.conf.idx euribor3m nr.employed y . 0 56 | housemaid | married | basic.4y | no | no | no | telephone | may | mon | 261 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 1 57 | services | married | high.school | unknown | no | no | telephone | may | mon | 149 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 2 37 | services | married | high.school | no | yes | no | telephone | may | mon | 226 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 3 40 | admin. | married | basic.6y | no | no | no | telephone | may | mon | 151 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 4 56 | services | married | high.school | no | no | yes | telephone | may | mon | 307 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 5 45 | services | married | basic.9y | unknown | no | no | telephone | may | mon | 198 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 6 59 | admin. | married | professional.course | no | no | no | telephone | may | mon | 139 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 7 41 | blue-collar | married | unknown | unknown | no | no | telephone | may | mon | 217 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 8 24 | technician | single | professional.course | no | yes | no | telephone | may | mon | 380 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 9 25 | services | single | high.school | no | yes | no | telephone | may | mon | 50 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . # Converting the target from binary/categoric into binary/numeric data[&#39;target&#39;] = data.apply(lambda row: 1 if row[&quot;y&quot;] == &quot;yes&quot; else 0, axis=1) data.drop([&quot;y&quot;],axis=1,inplace=True) print(emojis.encode(&quot;:crayon:&quot;)*26 ,&quot; n nConverted the target from binary/categoric into binary/numeric. n n&quot; ,emojis.encode(&quot;:crayon:&quot;)*26) . . ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ Converted the target from binary/categoric into binary/numeric ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ . # Renaming some columns for better typing and calling variables data.rename(columns={&quot;emp.var.rate&quot;:&quot;emp_var_rate&quot;,&quot;cons.price.idx&quot;:&quot;cons_price_idx&quot;,&quot;cons.conf.idx&quot;:&quot;cons_conf_idx&quot;,&quot;nr.employed&quot;:&quot;nr_employed&quot;},inplace=True) print(emojis.encode(&quot;:pencil2:&quot;)*25 ,&quot; n nRenamed some columns for better typing and calling variables n n&quot; ,emojis.encode(&quot;:pencil2:&quot;)*25) . . âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸ Renamed some columns for better typing and calling variables âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸âœï¸ . # Setting up numeric (num_data) and categoric (cat_data) dataframes num_data = data.copy().select_dtypes(include=[&quot;float64&quot;,&quot;int64&quot;]) cat_data = data.copy().select_dtypes(exclude=[&quot;float64&quot;,&quot;int64&quot;]) print(emoji.emojize(&quot;:bookmark_tabs:&quot;)*28 ,&quot; n nSetting up numeric (num_data) and categoric (cat_data) dataframes n n&quot; ,emoji.emojize(&quot;:bookmark_tabs:&quot;)*28) . . ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ Setting up numeric (num_data) and categoric (cat_data) dataframes ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ğŸ“‘ . Basic Info of the dataset . #finding the no. of rows and cols print(&quot; nFinding the no. of rows and cols in the dataset : n n&quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;No. of clients : {}&quot;.format(data.shape[0])) print(&quot;No. of features : {} including target&quot;.format(data.shape[1])) . . Finding the no. of rows and cols in the dataset : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» No. of clients : 41188 No. of features : 21 including target . # How many clients have subscribed the term deposit and how many didn&#39;t? print(&quot; nHow many clients have subscribed the term deposit and how many didn&#39;t ? n n &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;No. of clients who haven&#39;t subscribed the term deposit : {}&quot;.format(data.target.value_counts()[0])) print(&quot;No. of clients who have subscribed the term deposit : {}&quot;.format(data.target.value_counts()[1])) . . How many clients have subscribed the term deposit and how many didn&#39;t ? ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» No. of clients who haven&#39;t subscribed the term deposit : 36548 No. of clients who have subscribed the term deposit : 4640 . #Checking the dataset is balanced or not based on target values in the classification. print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Plotting the graph to check the dataset&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) total = len(data[&#39;target&#39;])*1 ax=sns.countplot(x=&#39;target&#39;,data=data) for p in ax.patches: ax.annotate(&#39;{:.1f}%&#39;.format(100*p.get_height()/total), (p.get_x()+0.1, p.get_height()+5), fontsize = 12) . . ğŸ“ˆğŸ“ˆ Plotting the graph to check the dataset ğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ The dataset is imbalanced and highly skewed, where the no. of neagative class is close to 8 times the no. of positive class. . # Overview of shape, attributes, types and missing values print(&quot; nOverview of shape, attributes, types and missing values : n n&quot; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.info() . . Overview of shape, attributes, types and missing values : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 41188 entries, 0 to 41187 Data columns (total 21 columns): # Column Non-Null Count Dtype -- -- 0 age 41188 non-null int64 1 job 41188 non-null object 2 marital 41188 non-null object 3 education 41188 non-null object 4 default 41188 non-null object 5 housing 41188 non-null object 6 loan 41188 non-null object 7 contact 41188 non-null object 8 month 41188 non-null object 9 day_of_week 41188 non-null object 10 duration 41188 non-null int64 11 campaign 41188 non-null int64 12 pdays 41188 non-null int64 13 previous 41188 non-null int64 14 poutcome 41188 non-null object 15 emp_var_rate 41188 non-null float64 16 cons_price_idx 41188 non-null float64 17 cons_conf_idx 41188 non-null float64 18 euribor3m 41188 non-null float64 19 nr_employed 41188 non-null float64 20 target 41188 non-null int64 dtypes: float64(5), int64(6), object(10) memory usage: 6.6+ MB . # General stats of the numerical variables print(&quot; nGeneral stats of the numerical variables : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.describe().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#753976&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).applymap(above_zero) . . General stats of the numerical variables : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . age duration campaign pdays previous emp_var_rate cons_price_idx cons_conf_idx euribor3m nr_employed target . count 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | 41188.000000 | . mean 40.024060 | 258.285010 | 2.567593 | 962.475454 | 0.172963 | 0.081886 | 93.575664 | -40.502600 | 3.621291 | 5167.035911 | 0.112654 | . std 10.421250 | 259.279249 | 2.770014 | 186.910907 | 0.494901 | 1.570960 | 0.578840 | 4.628198 | 1.734447 | 72.251528 | 0.316173 | . min 17.000000 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | -3.400000 | 92.201000 | -50.800000 | 0.634000 | 4963.600000 | 0.000000 | . 25% 32.000000 | 102.000000 | 1.000000 | 999.000000 | 0.000000 | -1.800000 | 93.075000 | -42.700000 | 1.344000 | 5099.100000 | 0.000000 | . 50% 38.000000 | 180.000000 | 2.000000 | 999.000000 | 0.000000 | 1.100000 | 93.749000 | -41.800000 | 4.857000 | 5191.000000 | 0.000000 | . 75% 47.000000 | 319.000000 | 3.000000 | 999.000000 | 0.000000 | 1.400000 | 93.994000 | -36.400000 | 4.961000 | 5228.100000 | 0.000000 | . max 98.000000 | 4918.000000 | 56.000000 | 999.000000 | 7.000000 | 1.400000 | 94.767000 | -26.900000 | 5.045000 | 5228.100000 | 1.000000 | . ğŸ“ğŸ“ Essence of above dataframe : âœï¸ age: The youngest client is 17 years old and the oldest is 98 years with a median of 38 years whilst the average is 40 years old. âœï¸ campaign: Minimum number of contacts performed during this campaign and for this client included last contact is 1 and maximum is 56. On an average a client is contacted 3 times. âœï¸ pdays: The majority of the clients have the 999 number wich indicates that most people did not contact nor were contacted by the bank. Those are considered to be &#39;out of range&#39; values. âœï¸ previous: The vast majority were never contacted before. âœï¸ emp_var_rate: during the period the index varied from [-3.4, 1.4] âœï¸ cons_price_idx: The index varied from [92.2, 94.8] âœï¸ cons_conf_idx: The consumer confidence level during that period kept always negative with a range of variation of [-51, -27]. These negative values might be explained by the recession that severely affected Portugal due the financial global crisis during that same period the data was recorded. âœï¸ euribor3m: There were a huge variation of the euribor rate during the period of analysis [5% to 0.6%]. This abrupt change in euribor together with the negative confidance verified reinforces the hipothesis that this data provides information from a crisis period. âœï¸ nr_employed: The number of employed people varied around 200 during the campaign. . # General stats of the categorical variables print(&quot; nGeneral stats of the categorical variables : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data.describe(include=[&#39;object&#39;]).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#117A65&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . General stats of the categorical variables : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . job marital education default housing loan contact month day_of_week poutcome . count 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | . unique 12 | 4 | 8 | 3 | 3 | 3 | 2 | 10 | 5 | 3 | . top admin. | married | university.degree | no | yes | no | cellular | may | thu | nonexistent | . freq 10422 | 24928 | 12168 | 32588 | 21576 | 33950 | 26144 | 13769 | 8623 | 35563 | . ğŸ“ğŸ“ Essence of above dataframe : âœï¸ job: there are 12 types of jobs recordings in wich the &#39;administrative&#39; role is the most comum with almost 10.5k of the clients âœï¸ marital: the majority of clients are married with almost 25k records âœï¸ education: more than 12k people have university degree âœï¸ default: from all the 41.188 clients, 32.588 don&#39;t have any credit in default âœï¸ housing: almost half of the customers have a housing loan âœï¸ loan: almost 34k clients don&#39;t have any personal loans âœï¸ poutcome: there is no information about the outcome of any previous marketing campaign . Exploratory Data Analysis . 1. Finding duplicate values . # checking for duplicate values if present in the dataframe print(&quot;Duplicate Data&quot; ,emoji.emojize(&quot;:red_question_mark:&quot;)*2,&quot; n&quot;) print(emoji.emojize(&quot;:cross_mark_button:&quot;)*3 ,&quot; n n&quot;,data.duplicated().any() ,&quot; n n&quot;,emoji.emojize(&quot;:cross_mark_button:&quot;)*3) . . Duplicate Data â“â“ âââ True âââ . # Selecting duplicate rows except first occurence based on all columns duplicate_data = data[data.duplicated(keep = &quot;last&quot;)] print(&#39; nduplicate rows : &#39; ,&quot;{} n&quot;.format(duplicate_data.shape[0]) ,&quot; n n&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) # print the resulatant dataframe containing duplicate rows duplicate_data.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#2B0E46&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . duplicate rows : 12 ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . age job marital education default housing loan contact month day_of_week duration campaign pdays previous poutcome emp_var_rate cons_price_idx cons_conf_idx euribor3m nr_employed target . 1265 39 | blue-collar | married | basic.6y | no | no | no | telephone | may | thu | 124 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.855000 | 5191.000000 | 0 | . 12260 36 | retired | married | unknown | no | no | no | telephone | jul | thu | 88 | 1 | 999 | 0 | nonexistent | 1.400000 | 93.918000 | -42.700000 | 4.966000 | 5228.100000 | 0 | . 14155 27 | technician | single | professional.course | no | no | no | cellular | jul | mon | 331 | 2 | 999 | 0 | nonexistent | 1.400000 | 93.918000 | -42.700000 | 4.962000 | 5228.100000 | 0 | . 16819 47 | technician | divorced | high.school | no | yes | no | cellular | jul | thu | 43 | 3 | 999 | 0 | nonexistent | 1.400000 | 93.918000 | -42.700000 | 4.962000 | 5228.100000 | 0 | . 18464 32 | technician | single | professional.course | no | yes | no | cellular | jul | thu | 128 | 1 | 999 | 0 | nonexistent | 1.400000 | 93.918000 | -42.700000 | 4.968000 | 5228.100000 | 0 | . 20072 55 | services | married | high.school | unknown | no | no | cellular | aug | mon | 33 | 1 | 999 | 0 | nonexistent | 1.400000 | 93.444000 | -36.100000 | 4.965000 | 5228.100000 | 0 | . 20531 41 | technician | married | professional.course | no | yes | no | cellular | aug | tue | 127 | 1 | 999 | 0 | nonexistent | 1.400000 | 93.444000 | -36.100000 | 4.966000 | 5228.100000 | 0 | . 25183 39 | admin. | married | university.degree | no | no | no | cellular | nov | tue | 123 | 2 | 999 | 0 | nonexistent | -0.100000 | 93.200000 | -42.000000 | 4.153000 | 5195.800000 | 0 | . 28476 24 | services | single | high.school | no | yes | no | cellular | apr | tue | 114 | 1 | 999 | 0 | nonexistent | -1.800000 | 93.075000 | -47.100000 | 1.423000 | 5099.100000 | 0 | . 32505 35 | admin. | married | university.degree | no | yes | no | cellular | may | fri | 348 | 4 | 999 | 0 | nonexistent | -1.800000 | 92.893000 | -46.200000 | 1.313000 | 5099.100000 | 0 | . 36950 45 | admin. | married | university.degree | no | no | no | cellular | jul | thu | 252 | 1 | 999 | 0 | nonexistent | -2.900000 | 92.469000 | -33.600000 | 1.072000 | 5076.200000 | 1 | . 38255 71 | retired | single | university.degree | no | no | no | telephone | oct | tue | 120 | 1 | 999 | 0 | nonexistent | -3.400000 | 92.431000 | -26.900000 | 0.742000 | 5017.500000 | 0 | . # removing duplicate values data = data.drop_duplicates(keep=&#39;last&#39;) print(emojis.encode(&quot;:scissors:&quot;)*16 ,&quot; n nDuplicate data removed successfully !! n n&quot; ,emojis.encode(&quot;:scissors:&quot;)*16) . . âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸ Duplicate data removed successfully !! âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸ . 2. Finding Unwanted Columns . ğŸ“ğŸ“ duration attribute : This attribute highly affects the output target (e.g., if duration=0 then y=&quot;no&quot;). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model. . 3. Finding Missing Values . ğŸ“ğŸ“ There are several missing values in some categorical attributes, all coded with the &quot;unknown&quot; label. These missing values can be treated as a possible class label by leaving as it is or treated using deletion or imputation techniques. We need to check the performance on all the 3 cases and check how these approaches help. . #checking for missing values print(&quot;Missing values&quot; ,emoji.emojize(&quot;:red_question_mark:&quot;)*2,&quot; n&quot;) print(emoji.emojize(&quot;:check_mark_button:&quot;)*3 ,&quot; n n&quot;,data.isnull().values.any() ,&quot; n n&quot;,emoji.emojize(&quot;:check_mark_button:&quot;)*3) . . Missing values â“â“ âœ…âœ…âœ… False âœ…âœ…âœ… . # Features containing &#39;unknown&#39; values # making a list of missing value types missing_values = [&#39;unknown&#39;] # reading data again with the defined non-standard missing value new_data = pd.read_csv(&#39;/content/drive/MyDrive/ColabNotebooks/1000_PBM/bank-additional-full.csv&#39;,sep=&#39;;&#39;, na_values = missing_values) print(&#39; nMissing Values as &quot;Unknown&quot; in the data : &#39; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) new_data.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#610646&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).highlight_null(null_color=&#39;#CCB3C5&#39;) . . Missing Values as &#34;Unknown&#34; in the data : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . age job marital education default housing loan contact month day_of_week duration campaign pdays previous poutcome emp.var.rate cons.price.idx cons.conf.idx euribor3m nr.employed y . 0 56 | housemaid | married | basic.4y | no | no | no | telephone | may | mon | 261 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 1 57 | services | married | high.school | nan | no | no | telephone | may | mon | 149 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 2 37 | services | married | high.school | no | yes | no | telephone | may | mon | 226 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 3 40 | admin. | married | basic.6y | no | no | no | telephone | may | mon | 151 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 4 56 | services | married | high.school | no | no | yes | telephone | may | mon | 307 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 5 45 | services | married | basic.9y | nan | no | no | telephone | may | mon | 198 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 6 59 | admin. | married | professional.course | no | no | no | telephone | may | mon | 139 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 7 41 | blue-collar | married | nan | nan | no | no | telephone | may | mon | 217 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 8 24 | technician | single | professional.course | no | yes | no | telephone | may | mon | 380 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . 9 25 | services | single | high.school | no | yes | no | telephone | may | mon | 50 | 1 | 999 | 0 | nonexistent | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | no | . #getting total no. of &#39;unknown&#39; values print(&#39; nCount of Missing Values as &quot;Unknown&quot; in each column of the data : n n&#39; ,&quot; &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) new_data.isnull().sum().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A15F86&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Count of Missing Values as &#34;Unknown&#34; in each column of the data : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . 0 . age 0 | . job 330 | . marital 80 | . education 1731 | . default 8597 | . housing 990 | . loan 990 | . contact 0 | . month 0 | . day_of_week 0 | . duration 0 | . campaign 0 | . pdays 0 | . previous 0 | . poutcome 0 | . emp.var.rate 0 | . cons.price.idx 0 | . cons.conf.idx 0 | . euribor3m 0 | . nr.employed 0 | . y 0 | . ğŸ“ğŸ“ There are 6 categorical features - job, marital, education, default, housing &amp; loan containing the non-standard missing value &#39;unknown&#39;. . new_data.drop([&#39;age&#39;, &#39;job&#39;, &#39;marital&#39;, &#39;education&#39;, &#39;default&#39;, &#39;housing&#39;, &#39;loan&#39;, &#39;contact&#39;, &#39;month&#39;, &#39;day_of_week&#39;, &#39;duration&#39;, &#39;campaign&#39;, &#39;pdays&#39;, &#39;previous&#39;, &#39;poutcome&#39;, &#39;emp.var.rate&#39;, &#39;cons.price.idx&#39;, &#39;cons.conf.idx&#39;, &#39;euribor3m&#39;, &#39;nr.employed&#39;, &#39;y&#39;], axis=1,inplace=True) del new_data print(emojis.encode(&quot;:x:&quot;)*14 ,&quot; n nDataframe deleted successfully !! n n&quot; ,emojis.encode(&quot;:x:&quot;)*14) . . âŒâŒâŒâŒâŒâŒâŒâŒâŒâŒâŒâŒâŒâŒ Dataframe deleted successfully !! âŒâŒâŒâŒâŒâŒâŒâŒâŒâŒâŒâŒâŒâŒ . 4. Finding Features with one value . # All the features with their unique values print(&#39; nUnique Values in each column of the data : n n&#39; ,&quot; &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) for column in data.columns: print(emoji.emojize(&quot;:arrow_right:&quot;, use_aliases=True) ,column ,emoji.emojize(&quot;:1234:&quot;, use_aliases=True) ,data[column].nunique()) . . Unique Values in each column of the data : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» â¡ age ğŸ”¢ 78 â¡ job ğŸ”¢ 12 â¡ marital ğŸ”¢ 4 â¡ education ğŸ”¢ 8 â¡ default ğŸ”¢ 3 â¡ housing ğŸ”¢ 3 â¡ loan ğŸ”¢ 3 â¡ contact ğŸ”¢ 2 â¡ month ğŸ”¢ 10 â¡ day_of_week ğŸ”¢ 5 â¡ duration ğŸ”¢ 1544 â¡ campaign ğŸ”¢ 42 â¡ pdays ğŸ”¢ 27 â¡ previous ğŸ”¢ 8 â¡ poutcome ğŸ”¢ 3 â¡ emp_var_rate ğŸ”¢ 10 â¡ cons_price_idx ğŸ”¢ 26 â¡ cons_conf_idx ğŸ”¢ 26 â¡ euribor3m ğŸ”¢ 316 â¡ nr_employed ğŸ”¢ 11 â¡ target ğŸ”¢ 2 . ğŸ“ğŸ“ There is no feature with 1 value. . 5. Exploring the numerical features . #list of numerical variables numerical_features = [feature for feature in data.columns if ((data[feature].dtypes!=&#39;O&#39;) &amp; (feature not in [&#39;target&#39;]))] print(&#39;No. of numerical variables&#39; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;)*2 ,len(numerical_features)) print(&quot;&quot;) #all the numerical variables for feature in numerical_features: print(&quot;The variable&quot; ,emoji.emojize(&quot;:memo:&quot;) ,&quot;&#39;{}&#39;&quot;.format(feature) ,&quot;has datatype&quot; ,emoji.emojize(&quot;:1234:&quot;, use_aliases=True) ,&quot;{}&quot;.format(data[feature].dtypes) ,&quot;and&quot; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;) ,&quot;{}&quot;.format(len(data[feature].unique())) ,&quot;unique values&quot;) . . No. of numerical variables ğŸ‘‰ğŸ»ğŸ‘‰ğŸ» 10 The variable ğŸ“ &#39;age&#39; has datatype ğŸ”¢ int64 and ğŸ‘‰ğŸ» 78 unique values The variable ğŸ“ &#39;duration&#39; has datatype ğŸ”¢ int64 and ğŸ‘‰ğŸ» 1544 unique values The variable ğŸ“ &#39;campaign&#39; has datatype ğŸ”¢ int64 and ğŸ‘‰ğŸ» 42 unique values The variable ğŸ“ &#39;pdays&#39; has datatype ğŸ”¢ int64 and ğŸ‘‰ğŸ» 27 unique values The variable ğŸ“ &#39;previous&#39; has datatype ğŸ”¢ int64 and ğŸ‘‰ğŸ» 8 unique values The variable ğŸ“ &#39;emp_var_rate&#39; has datatype ğŸ”¢ float64 and ğŸ‘‰ğŸ» 10 unique values The variable ğŸ“ &#39;cons_price_idx&#39; has datatype ğŸ”¢ float64 and ğŸ‘‰ğŸ» 26 unique values The variable ğŸ“ &#39;cons_conf_idx&#39; has datatype ğŸ”¢ float64 and ğŸ‘‰ğŸ» 26 unique values The variable ğŸ“ &#39;euribor3m&#39; has datatype ğŸ”¢ float64 and ğŸ‘‰ğŸ» 316 unique values The variable ğŸ“ &#39;nr_employed&#39; has datatype ğŸ”¢ float64 and ğŸ‘‰ğŸ» 11 unique values . 6. Distribution of numerical features . def num_histplot(feature, dataset): &quot;&quot;&quot; It takes the numerical variable and dataset as input and plots the histogram for the particular. &quot;&quot;&quot; if feature in numerical_features: print(&quot; n t t &quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3 ,&quot;Distribution of &#39;{}&#39;&quot;.format(feature) ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(10,8)) sns.histplot(dataset[feature], kde=True, bins=10,color=&#39;#F78AB2&#39;) plt.show() else: print(emoji.emojize(&quot;:cross_mark:&quot;)*2 ,&quot;It&#39;s not a numerical feature !!!&quot; ,emoji.emojize(&quot;:cross_mark:&quot;)*2) . . 7. Relationship b/w numerical features and the target label . def num_boxplot_wrt_Y(feature, dataset): &quot;&quot;&quot; It takes the numerical variable and dataset as input and plots the boxplot for the particular with styler object of Skewness, Kurtosis, Median, Count, Mean ,Standard Deviation, Min. value, Q1, Q2, Q3, Q4, Max. value, IQR, Lower Outliers Limit ,Upper Outliers Limit, Lower Outliers Count with percentage, Upper Outliers Count with percentage,Outliers Count with percentage . &quot;&quot;&quot; if feature in numerical_features: print(&quot; n t t&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3 ,&quot;Boxplot of &#39;{}&#39; w.r.t target&quot;.format(feature) ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3) print(&quot;&quot;) #plotting the boxplot plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(10,8)) sns.boxplot(x=&#39;target&#39;,y=dataset[feature] , data=dataset, palette=&#39;flare&#39;) plt.xlabel(feature) plt.show() # Parameters to check presence of outliers in the distribution v1 = pd.DataFrame({&#39; Parameters &#39;: &#39; Skewness&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].skew())},index={&#39;1&#39;}) v2 = pd.DataFrame({&#39; Parameters &#39;: &#39; Kurtosis&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].kurtosis())},index={&#39;2&#39;}) v3 = pd.DataFrame({&#39; Parameters &#39;: &#39; Median&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].quantile())},index={&#39;3&#39;}) v4 = pd.DataFrame({&#39; Parameters &#39;: &#39; Count&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[0])},index={&#39;4&#39;}) v5 = pd.DataFrame({&#39; Parameters &#39;: &#39; Mean&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[1])},index={&#39;5&#39;}) v6 = pd.DataFrame({&#39; Parameters &#39;: &#39; Stand. Dev.&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[2])},index={&#39;6&#39;}) v7 = pd.DataFrame({&#39; Parameters &#39;: &#39; Minimum&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[3])},index={&#39;7&#39;}) v8 = pd.DataFrame({&#39; Parameters &#39;: &#39; Q1 (25%)&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[4])},index={&#39;8&#39;}) v9 = pd.DataFrame({&#39; Parameters &#39;: &#39; Q2 (50%)&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[5])},index={&#39;9&#39;}) v10 = pd.DataFrame({&#39; Parameters &#39;: &#39; Q3 (75%)&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[6])},index={&#39;10&#39;}) v11 = pd.DataFrame({&#39; Parameters &#39;: &#39; Maximum&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(dataset[feature].describe()[7])},index={&#39;11&#39;}) #finding Interquartile range iqr = dataset[feature].quantile(q = 0.75) - dataset[feature].quantile(q = 0.25) v12 = pd.DataFrame({&#39; Parameters &#39;: &#39; IQR&#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(iqr)},index={&#39;12&#39;}) #Outlier detection from IQR lower_outliers = dataset[feature].quantile(q = 0.25) - (iqr*1.5) v13 = pd.DataFrame({&#39; Parameters &#39;: &#39; Lower outliers Limit &#39; , &#39; Values &#39; : &quot;{:.2f} &quot;.format(lower_outliers)},index={&#39;13&#39;}) upper_outliers = dataset[feature].quantile(q = 0.75) + (iqr*1.5) v14 = pd.DataFrame({&#39; Parameters &#39;: &#39; Upper outliers Limit &#39; , &#39; Values &#39; : &quot;{:.2f}&quot;.format(upper_outliers)},index={&#39;14&#39;}) #checking the presence of outliers with upper &amp; lower limits lower_out_count = dataset[(dataset[feature] &lt; (dataset[feature].quantile(q = 0.25)-(iqr*1.5)) )][feature].count() lower_out_pct = round(lower_out_count / dataset[feature].count() * 100, 1) v15 = pd.DataFrame({&#39; Parameters &#39;: &#39; Lower outliers count &#39; , &#39; Values &#39; : &quot;{} ({}%)&quot;.format(lower_out_count, lower_out_pct)},index={&#39;15&#39;}) upper_out_count = dataset[(dataset[feature] &gt; (dataset[feature].quantile(q = 0.75)+(iqr*1.5)) )][feature].count() upper_out_pct = round(upper_out_count / dataset[feature].count() * 100, 1) v16 = pd.DataFrame({&#39; Parameters &#39;: &#39; Upper outliers count &#39; , &#39; Values &#39; : &quot;{} ({}%)&quot;.format(upper_out_count, upper_out_pct)},index={&#39;16&#39;}) outliers = dataset[(dataset[feature]&lt; (dataset[feature].quantile(q = 0.25)-(iqr*1.5))) | (dataset[feature] &gt; (dataset[feature].quantile(q = 0.75)+(iqr*1.5)) )][feature].count() outliers_pct = round(outliers / dataset[feature].count() * 100, 1) v17 = pd.DataFrame({&#39; Parameters &#39;: &#39; Outliers count &#39; , &#39; Values &#39; : &quot;{} ({}%)&quot;.format(outliers, outliers_pct)},index={&#39;17&#39;}) result = pd.concat([v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17]) result.columns = [&#39; Parameters &#39;,&#39; Values &#39;] if outliers==0: print(&quot; n&quot;,emoji.emojize(&quot;:check_mark:&quot;)*2 ,emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*2 ,&quot; NO Outliers &quot; ,emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*2 ,emoji.emojize(&quot;:check_mark:&quot;)*2,&quot; n&quot;) not_outliers = dataset[(dataset[feature]&gt;= (dataset[feature].quantile(q = 0.25)-(iqr*1.5))) | (dataset[feature] &lt;= (dataset[feature].quantile(q = 0.75)+(iqr*1.5)) )][feature].count() not_outliers_pct = round(not_outliers / dataset[feature].count() * 100, 1) result = result.iloc[:-5,:] v18 = pd.DataFrame({&#39; Parameters &#39;: &#39; observation count w/o outliers &#39; , &#39; Values &#39; : &quot;{} ({}%)&quot;.format(not_outliers, not_outliers_pct)},index={&#39;13&#39;}) result = pd.concat([result,v18]) result.columns = [&#39; Parameters &#39;,&#39; Values &#39;] else: print(&quot; n&quot;,emoji.emojize(&quot;:cross_mark:&quot;)*2 ,emoji.emojize(&quot;:thumbs_down_light_skin_tone:&quot;)*2 ,&quot;Outliers Present&quot; ,emoji.emojize(&quot;:thumbs_down_light_skin_tone:&quot;)*2 ,emoji.emojize(&quot;:cross_mark:&quot;)*2,&quot; n&quot;) result = result.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#34495E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#FEF5E7&#39;}, subset=[&#39; Parameters &#39;]) return result else: print(emoji.emojize(&quot;:cross_mark:&quot;)*2 ,&quot;It&#39;s not a numerical feature !!!&quot; ,emoji.emojize(&quot;:cross_mark:&quot;)*2) . . Feature : age . num_histplot(&#39;age&#39;, data) print(&quot; n t t t&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: clients&#39; age&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;age&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: clients&#39; age ğŸ“ƒğŸ“ƒ . ğŸ“ğŸ“ We can see that this plot is right-skewed,but also similar to normal distribution. In the above distribution, we can see that most of the customers are in the age range of 30-40. . num_boxplot_wrt_Y(&#39;age&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Boxplot of &#39;age&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . âŒâŒ ğŸ‘ğŸ»ğŸ‘ğŸ» Outliers Present ğŸ‘ğŸ»ğŸ‘ğŸ» âŒâŒ . Parameters Values . 1 Skewness | 0.78 | . 2 Kurtosis | 0.79 | . 3 Median | 38.00 | . 4 Count | 41176.00 | . 5 Mean | 40.02 | . 6 Stand. Dev. | 10.42 | . 7 Minimum | 17.00 | . 8 Q1 (25%) | 32.00 | . 9 Q2 (50%) | 38.00 | . 10 Q3 (75%) | 47.00 | . 11 Maximum | 98.00 | . 12 IQR | 15.00 | . 13 Lower outliers Limit | 9.50 | . 14 Upper outliers Limit | 69.50 | . 15 Lower outliers count | 0 (0.0%) | . 16 Upper outliers count | 468 (1.1%) | . 17 Outliers count | 468 (1.1%) | . ğŸ“ğŸ“ From the above boxplot we know that for both the customers that subscibed or didn&#39;t subscribe a term deposit, has a median age of around 38-40. We see distribution for clients, who subscribed a term deposit is more diffused,but it is because people who said yes are less. . # grouping the clients on the basis of age print(&#39; nGrouping the clients on the basis of age : n n&#39; ,&quot; t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data[&#39;age_bins&#39;] = pd.cut(data[&#39;age&#39;], bins = [data[&#39;age&#39;].min(), 30, 60, data[&#39;age&#39;].max()], labels=[&#39;Young&#39;, &#39;Adult&#39;, &#39;Senior&#39;]) group_age_target1 = pd.DataFrame(data.groupby([&#39;age_bins&#39;])[&#39;target&#39;].mean().multiply(100)) group_age_target = pd.DataFrame(data.groupby([&#39;age_bins&#39;])[&#39;target&#39;].mean().multiply(100)) group_age_target = group_age_target.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#E06689 &#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) group_age_target . . Grouping the clients on the basis of age : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . target . age_bins . Young 15.211497 | . Adult 9.429544 | . Senior 45.544554 | . # Display graph print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Plotting the clients on the basis of age group&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) print(&quot;&quot;) group_age_target1.plot.barh() plt.xlabel(&#39;Subscribed [%]&#39;); . . ğŸ“ˆğŸ“ˆ Plotting the clients on the basis of age group ğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ It is very clear the relation betweem the subscription rate and age of customers: âœï¸ 45.5% of Seniors (+60 years old) subscribed to the term deposit âœï¸ less than 1 in 10 Adults (&gt;30 and &lt;=60 years old) subscribed âœï¸ Young people were the 2nd group that subscribed the deposit corresponding to 1/6 of all young people. âœï¸ Senior subscribers alone were almost as much as Young and Adults subscribers, respectively, all together. . Feature : campaign . num_histplot(&#39;campaign&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: number of contacts performed during this campaign and for this client included last contact&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;campaign&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: number of contacts performed during this campaign and for this client included last contact ğŸ“ƒğŸ“ƒ . num_boxplot_wrt_Y(&#39;campaign&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Boxplot of &#39;campaign&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . âŒâŒ ğŸ‘ğŸ»ğŸ‘ğŸ» Outliers Present ğŸ‘ğŸ»ğŸ‘ğŸ» âŒâŒ . Parameters Values . 1 Skewness | 4.76 | . 2 Kurtosis | 36.97 | . 3 Median | 2.00 | . 4 Count | 41176.00 | . 5 Mean | 2.57 | . 6 Stand. Dev. | 2.77 | . 7 Minimum | 1.00 | . 8 Q1 (25%) | 1.00 | . 9 Q2 (50%) | 2.00 | . 10 Q3 (75%) | 3.00 | . 11 Maximum | 56.00 | . 12 IQR | 2.00 | . 13 Lower outliers Limit | -2.00 | . 14 Upper outliers Limit | 6.00 | . 15 Lower outliers count | 0 (0.0%) | . 16 Upper outliers count | 2406 (5.8%) | . 17 Outliers count | 2406 (5.8%) | . ğŸ“ğŸ“ The &#39;campaign&#39; attribute shows the total no. of contacts performed by the bank during the marketing campaigns. We can see that the plot is right-skewed which shows that the no. of clients contacted who did not subscribed to the bank term deposit policy was more than the no. of clients were contacted that did subscribed to the bank term deposit. . Feature : pdays . num_histplot(&#39;pdays&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: number of days that passed by after the client was last contacted from a previous campaign&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;pdays&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: number of days that passed by after the client was last contacted from a previous campaign ğŸ“ƒğŸ“ƒ . ğŸ“ğŸ“ This attribute shows the no. of days that passed by after the client was previously contacted by the campaign team. The value of pdays from 999 and upwards, means the client was not contacted previously. . print(&#39; nUnique Values in &quot;pdays&quot; n n&#39; ,&quot; &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data[&quot;pdays&quot;].value_counts().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#035753&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Unique Values in &#34;pdays&#34; ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . pdays . 999 39661 | . 3 439 | . 6 412 | . 4 118 | . 9 64 | . 2 61 | . 7 60 | . 12 58 | . 10 52 | . 5 46 | . 13 36 | . 11 28 | . 1 26 | . 15 24 | . 14 20 | . 8 18 | . 0 15 | . 16 11 | . 17 8 | . 18 7 | . 19 3 | . 22 3 | . 21 2 | . 26 1 | . 20 1 | . 25 1 | . 27 1 | . ğŸ“ğŸ“ It can be seen that the lesser the no. of days that passed by the higher the no. of contacts made previously. Most of the values are 999, which means that the most of the customers have never been contacted before. . num_boxplot_wrt_Y(&#39;pdays&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Boxplot of &#39;pdays&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . âŒâŒ ğŸ‘ğŸ»ğŸ‘ğŸ» Outliers Present ğŸ‘ğŸ»ğŸ‘ğŸ» âŒâŒ . Parameters Values . 1 Skewness | -4.92 | . 2 Kurtosis | 22.22 | . 3 Median | 999.00 | . 4 Count | 41176.00 | . 5 Mean | 962.46 | . 6 Stand. Dev. | 186.94 | . 7 Minimum | 0.00 | . 8 Q1 (25%) | 999.00 | . 9 Q2 (50%) | 999.00 | . 10 Q3 (75%) | 999.00 | . 11 Maximum | 999.00 | . 12 IQR | 0.00 | . 13 Lower outliers Limit | 999.00 | . 14 Upper outliers Limit | 999.00 | . 15 Lower outliers count | 1515 (3.7%) | . 16 Upper outliers count | 0 (0.0%) | . 17 Outliers count | 1515 (3.7%) | . print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;No. of days passed by after the client was last contacted from a previous campaign&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) print(&quot;&quot;) plt.figure(figsize=(10,8)) sns.histplot(data=data, x=&quot;pdays&quot;, hue=&quot;target&quot;, multiple=&quot;stack&quot;) plt.show() . . ğŸ“ˆğŸ“ˆ No. of days passed by after the client was last contacted from a previous campaign ğŸ“ˆğŸ“ˆ . # counting the days that passed by after previous campaign print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Plotting the days that passed by after previous campaign&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) dummy = data.loc[(data[&#39;pdays&#39;]!=999) &amp; (data[&#39;target&#39;] == 1), &#39;pdays&#39;] print(&#39;Median: {:.2} n&#39;.format(dummy.median())) dummy.hist().grid(True) plt.title(&#39;Histogram&#39;) plt.xlabel(&#39;Counting days after contact n for those who subscribed&#39;); . . ğŸ“ˆğŸ“ˆ Plotting the days that passed by after previous campaign ğŸ“ˆğŸ“ˆ Median: 6.0 . ğŸ“ğŸ“ Considering only the clients who had subscribed let&#39;s count the days that passed by after contact from a previous campaign. Most of the people will respond on the 6th day and within 8 days. . Feature : previous . num_histplot(&#39;previous&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: number of contacts performed before this campaign and for this client&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;previous&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: number of contacts performed before this campaign and for this client ğŸ“ƒğŸ“ƒ . print(&#39; nUnique Values in &quot;previous&quot; n n&#39; ,&quot; &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data[&quot;previous&quot;].value_counts().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#436C3A&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Unique Values in &#34;previous&#34; ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . previous . 0 35551 | . 1 4561 | . 2 754 | . 3 216 | . 4 70 | . 5 18 | . 6 5 | . 7 1 | . print(&quot;People who were previously contacted with success&quot; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;)*2 ,&quot; {} n&quot;.format(data.poutcome.value_counts()[1])) data[data[&quot;target&quot;]==1][&quot;previous&quot;].value_counts().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#436C3A&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . People who were previously contacted with success ğŸ‘‰ğŸ»ğŸ‘‰ğŸ» 4252 . previous . 0 3140 | . 1 967 | . 2 350 | . 3 128 | . 4 38 | . 5 13 | . 6 3 | . print(&quot;People who were previously contacted with failure&quot; ,emoji.emojize(&quot;:backhand_index_pointing_right_light_skin_tone:&quot;)*2 ,&quot; {} n&quot;.format(data.poutcome.value_counts()[2])) data[data[&quot;target&quot;]==0][&quot;previous&quot;].value_counts().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#436C3A&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . People who were previously contacted with failure ğŸ‘‰ğŸ»ğŸ‘‰ğŸ» 1373 . previous . 0 32411 | . 1 3594 | . 2 404 | . 3 88 | . 4 32 | . 5 5 | . 6 2 | . 7 1 | . ğŸ“ğŸ“ People that were previously contacted subscribed in a much higher rate to the term deposit. The vast majority were never been contacted before. . print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Plotting the people who were previously contacted&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) plt.figure(figsize=(10,8)) sns.kdeplot(data=data, x=&quot;previous&quot;, hue=&quot;target&quot;, multiple=&quot;stack&quot;) plt.show() . . ğŸ“ˆğŸ“ˆ Plotting the people who were previously contacted ğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ The previous feature is similarly distributed for both the classes in the target variable. From here we can see that mostly people who were not contacted previously have not subscribed to the bank term deposit. . print(&#39; n t&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Boxplot of the people who were previously contacted&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) plt.figure(figsize=(10,8)) sns.boxplot(x=&#39;previous&#39;, data=data) plt.xlabel(&#39;previous&#39;) plt.show() num_boxplot_wrt_Y(&#39;previous&#39;, data) . . ğŸ“ˆğŸ“ˆ Boxplot of the people who were previously contacted ğŸ“ˆğŸ“ˆ . ğŸ“ˆğŸ“ˆğŸ“ˆ Boxplot of &#39;previous&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . âŒâŒ ğŸ‘ğŸ»ğŸ‘ğŸ» Outliers Present ğŸ‘ğŸ»ğŸ‘ğŸ» âŒâŒ . Parameters Values . 1 Skewness | 3.83 | . 2 Kurtosis | 20.10 | . 3 Median | 0.00 | . 4 Count | 41176.00 | . 5 Mean | 0.17 | . 6 Stand. Dev. | 0.49 | . 7 Minimum | 0.00 | . 8 Q1 (25%) | 0.00 | . 9 Q2 (50%) | 0.00 | . 10 Q3 (75%) | 0.00 | . 11 Maximum | 7.00 | . 12 IQR | 0.00 | . 13 Lower outliers Limit | 0.00 | . 14 Upper outliers Limit | 0.00 | . 15 Lower outliers count | 0 (0.0%) | . 16 Upper outliers count | 5625 (13.7%) | . 17 Outliers count | 5625 (13.7%) | . Feature : emp_var_rate . num_histplot(&#39;emp_var_rate&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: employment variation rate - quarterly indicator&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;emp_var_rate&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: employment variation rate - quarterly indicator ğŸ“ƒğŸ“ƒ . print(&#39; nUnique Values in &quot;emp_var_rate&quot; n n&#39; ,&quot; t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data[&quot;emp_var_rate&quot;].value_counts().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#424B17&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Unique Values in &#34;emp_var_rate&#34; ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . emp_var_rate . 1.4 16228 | . -1.8 9182 | . 1.1 7762 | . -0.1 3682 | . -2.9 1662 | . -3.4 1070 | . -1.7 773 | . -1.1 635 | . -3.0 172 | . -0.2 10 | . print(&#39; nEmployment Variation Rate with successful subscription n n&#39; ,&quot; t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data[data[&quot;target&quot;]==1][&quot;emp_var_rate&quot;].value_counts().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#424B17&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Employment Variation Rate with successful subscription ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . emp_var_rate . -1.8 1461 | . 1.4 866 | . -2.9 593 | . -3.4 454 | . -1.7 403 | . -1.1 301 | . 1.1 240 | . -0.1 232 | . -3.0 88 | . -0.2 1 | . print(&#39; nEmployment Variation Rate with failure in subscription n n&#39; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data[data[&quot;target&quot;]==0][&quot;emp_var_rate&quot;].value_counts().to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#424B17&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Employment Variation Rate with failure in subscription ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . emp_var_rate . 1.4 15362 | . -1.8 7721 | . 1.1 7522 | . -0.1 3450 | . -2.9 1069 | . -3.4 616 | . -1.7 370 | . -1.1 334 | . -3.0 84 | . -0.2 9 | . num_boxplot_wrt_Y(&#39;emp_var_rate&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Boxplot of &#39;emp_var_rate&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . âœ”âœ” ğŸ‘ğŸ»ğŸ‘ğŸ» NO Outliers ğŸ‘ğŸ»ğŸ‘ğŸ» âœ”âœ” . Parameters Values . 1 Skewness | -0.72 | . 2 Kurtosis | -1.06 | . 3 Median | 1.10 | . 4 Count | 41176.00 | . 5 Mean | 0.08 | . 6 Stand. Dev. | 1.57 | . 7 Minimum | -3.40 | . 8 Q1 (25%) | -1.80 | . 9 Q2 (50%) | 1.10 | . 10 Q3 (75%) | 1.40 | . 11 Maximum | 1.40 | . 12 IQR | 3.20 | . 13 observation count w/o outliers | 41176 (100.0%) | . ğŸ“ğŸ“ When the emp_var_rate (the employment rate) is negative there is a higher positive response to the campaign. . Feature : cons_price_idx . num_histplot( &#39;cons_price_idx&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: consumer price index - monthly indicator&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;cons_price_idx&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: consumer price index - monthly indicator ğŸ“ƒğŸ“ƒ . num_boxplot_wrt_Y( &#39;cons_price_idx&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Boxplot of &#39;cons_price_idx&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . âœ”âœ” ğŸ‘ğŸ»ğŸ‘ğŸ» NO Outliers ğŸ‘ğŸ»ğŸ‘ğŸ» âœ”âœ” . Parameters Values . 1 Skewness | -0.23 | . 2 Kurtosis | -0.83 | . 3 Median | 93.75 | . 4 Count | 41176.00 | . 5 Mean | 93.58 | . 6 Stand. Dev. | 0.58 | . 7 Minimum | 92.20 | . 8 Q1 (25%) | 93.08 | . 9 Q2 (50%) | 93.75 | . 10 Q3 (75%) | 93.99 | . 11 Maximum | 94.77 | . 12 IQR | 0.92 | . 13 observation count w/o outliers | 41176 (100.0%) | . ğŸ“ğŸ“ When the cons_price_idx (consumer price index) increases there is a strong negative response from the clients&#39; subscriptions. . Feature : cons_conf_idx . num_histplot(&#39;cons_conf_idx&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: consumer confidence index - monthly indicator&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;cons_conf_idx&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: consumer confidence index - monthly indicator ğŸ“ƒğŸ“ƒ . num_boxplot_wrt_Y(&#39;cons_conf_idx&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Boxplot of &#39;cons_conf_idx&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . âŒâŒ ğŸ‘ğŸ»ğŸ‘ğŸ» Outliers Present ğŸ‘ğŸ»ğŸ‘ğŸ» âŒâŒ . Parameters Values . 1 Skewness | 0.30 | . 2 Kurtosis | -0.36 | . 3 Median | -41.80 | . 4 Count | 41176.00 | . 5 Mean | -40.50 | . 6 Stand. Dev. | 4.63 | . 7 Minimum | -50.80 | . 8 Q1 (25%) | -42.70 | . 9 Q2 (50%) | -41.80 | . 10 Q3 (75%) | -36.40 | . 11 Maximum | -26.90 | . 12 IQR | 6.30 | . 13 Lower outliers Limit | -52.15 | . 14 Upper outliers Limit | -26.95 | . 15 Lower outliers count | 0 (0.0%) | . 16 Upper outliers count | 446 (1.1%) | . 17 Outliers count | 446 (1.1%) | . Feature : euribor3m . num_histplot(&#39;euribor3m&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: euribor 3 month rate - daily indicator&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;euribor3m&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: euribor 3 month rate - daily indicator ğŸ“ƒğŸ“ƒ . num_boxplot_wrt_Y(&#39;euribor3m&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Boxplot of &#39;euribor3m&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . âœ”âœ” ğŸ‘ğŸ»ğŸ‘ğŸ» NO Outliers ğŸ‘ğŸ»ğŸ‘ğŸ» âœ”âœ” . Parameters Values . 1 Skewness | -0.71 | . 2 Kurtosis | -1.41 | . 3 Median | 4.86 | . 4 Count | 41176.00 | . 5 Mean | 3.62 | . 6 Stand. Dev. | 1.73 | . 7 Minimum | 0.63 | . 8 Q1 (25%) | 1.34 | . 9 Q2 (50%) | 4.86 | . 10 Q3 (75%) | 4.96 | . 11 Maximum | 5.04 | . 12 IQR | 3.62 | . 13 observation count w/o outliers | 41176 (100.0%) | . ğŸ“ğŸ“ The lower the euribor3m is, the higher the number of subscriptions. . Feature : nr_employed . num_histplot(&#39;nr_employed&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: number of employed citizens - quarterly indicator&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;nr_employed&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: number of employed citizens - quarterly indicator ğŸ“ƒğŸ“ƒ . print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Plotting no. of employed people&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) plt.figure(figsize=(10,8)) sns.histplot(data=data, x=&quot;nr_employed&quot;, hue=&quot;target&quot;, multiple=&quot;stack&quot;) plt.show() . . ğŸ“ˆğŸ“ˆ Plotting no. of employed people ğŸ“ˆğŸ“ˆ . num_boxplot_wrt_Y(&#39;nr_employed&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Boxplot of &#39;nr_employed&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . âœ”âœ” ğŸ‘ğŸ»ğŸ‘ğŸ» NO Outliers ğŸ‘ğŸ»ğŸ‘ğŸ» âœ”âœ” . Parameters Values . 1 Skewness | -1.04 | . 2 Kurtosis | -0.00 | . 3 Median | 5191.00 | . 4 Count | 41176.00 | . 5 Mean | 5167.03 | . 6 Stand. Dev. | 72.25 | . 7 Minimum | 4963.60 | . 8 Q1 (25%) | 5099.10 | . 9 Q2 (50%) | 5191.00 | . 10 Q3 (75%) | 5228.10 | . 11 Maximum | 5228.10 | . 12 IQR | 129.00 | . 13 observation count w/o outliers | 41176 (100.0%) | . ğŸ“ğŸ“ From here, we can see that people that were contacted had higher rates of subscription. The above histogram shows that the first contacts were exclusively made to known clients resulting in a much more efficient campaign with a low number of employed people. . 8. Exploring categorical features. . # displaying each categorical feature with its unique no. of categories categorical_features = [feature for feature in data.columns if ((data[feature].dtypes==&#39;O&#39;) &amp; (feature not in [&#39;y&#39;]))] for feature in categorical_features: print(&quot; nThe variable &#39;{}&#39; has {} categories &quot;.format(feature,len(data[feature].unique())) ,&quot; n&quot; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3 ,&quot; n&quot; ,&quot;{}&quot;.format(data[feature].unique())) . . The variable &#39;job&#39; has 12 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;housemaid&#39; &#39;services&#39; &#39;admin.&#39; &#39;blue-collar&#39; &#39;technician&#39; &#39;retired&#39; &#39;management&#39; &#39;unemployed&#39; &#39;self-employed&#39; &#39;unknown&#39; &#39;entrepreneur&#39; &#39;student&#39;] The variable &#39;marital&#39; has 4 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;married&#39; &#39;single&#39; &#39;divorced&#39; &#39;unknown&#39;] The variable &#39;education&#39; has 8 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;basic.4y&#39; &#39;high.school&#39; &#39;basic.6y&#39; &#39;basic.9y&#39; &#39;professional.course&#39; &#39;unknown&#39; &#39;university.degree&#39; &#39;illiterate&#39;] The variable &#39;default&#39; has 3 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;no&#39; &#39;unknown&#39; &#39;yes&#39;] The variable &#39;housing&#39; has 3 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;no&#39; &#39;yes&#39; &#39;unknown&#39;] The variable &#39;loan&#39; has 3 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;no&#39; &#39;yes&#39; &#39;unknown&#39;] The variable &#39;contact&#39; has 2 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;telephone&#39; &#39;cellular&#39;] The variable &#39;month&#39; has 10 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;may&#39; &#39;jun&#39; &#39;jul&#39; &#39;aug&#39; &#39;oct&#39; &#39;nov&#39; &#39;dec&#39; &#39;mar&#39; &#39;apr&#39; &#39;sep&#39;] The variable &#39;day_of_week&#39; has 5 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;mon&#39; &#39;tue&#39; &#39;wed&#39; &#39;thu&#39; &#39;fri&#39;] The variable &#39;poutcome&#39; has 3 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;nonexistent&#39; &#39;failure&#39; &#39;success&#39;] . ğŸ“ğŸ“ There are 10 categorical features with the above listed unique values.The variables &#39;job&#39; and &#39;month&#39; have the highest no. of categories. . 9. Distribution of categorical features. . def cat_countplot(cat_feature, dataset): &quot;&quot;&quot; Takes categorical feature and dataset as input and plots the countplot in decreasing order of the xlabel for the paricular. &quot;&quot;&quot; if cat_feature in categorical_features: print() print(&quot; n t t &quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3 ,&quot;Distribution of &#39;{}&#39;&quot;.format(cat_feature) ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(10,8)) Y = data[cat_feature] total = len(Y)*1 ax = sns.countplot(x=cat_feature,data=data,order=data[cat_feature].value_counts().index) plt.xlabel(cat_feature, fontsize = 20) #put 11 ticks (for 10 steps), from 0 to the total number of rows in the dataframe ax.yaxis.set_ticks(np.linspace(0, total, 11)) #adjust the ticklabel to the desired format, without changing the position of the ticks ax.set_yticklabels(map(&#39;{:.1f}%&#39;.format, 100*ax.yaxis.get_majorticklocs()/total)) ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=&quot;right&quot;) for p in ax.patches: ax.annotate(&#39;{:.1f}%&#39;.format(100*p.get_height()/total), (p.get_x()+0.1, p.get_height()+5), fontsize = 8) plt.show() else: print(emoji.emojize(&quot;:cross_mark:&quot;)*2 ,&quot;The entered feature is not categorical !!!!&quot; ,emoji.emojize(&quot;:cross_mark:&quot;)*2) . . 10. Relationship b/w categorical features and the target label . #finding the relationship b/w categorical variables and target label def cat_countplot_wrt_Y(cat_feature, dataset): &quot;&quot;&quot; Takes categorical feature and dataset as input and plots the countplot of the categorical feature w.r.t the target in decreasing order of the xalabel. &quot;&quot;&quot; if cat_feature in categorical_features: print(&quot; n t t &quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3 ,&quot;Distribution of &#39;{}&#39; w.r.t target&quot;.format(cat_feature) ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*3) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(10,8)) Y = data[cat_feature] total = len(Y)*1 ax = sns.countplot(x=cat_feature,data=data, hue=&quot;target&quot;,order=data[cat_feature].value_counts().index, palette=&#39;hls&#39;) plt.xlabel(cat_feature, fontsize = 20) #put 11 ticks (for 10 steps), from 0 to the total number of rows in the dataframe ax.yaxis.set_ticks(np.linspace(0, total, 11)) #adjust the ticklabel to the desired format, without changing the position of the ticks ax.set_yticklabels(map(&#39;{:.1f}%&#39;.format, 100*ax.yaxis.get_majorticklocs()/total)) ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=&quot;right&quot;) for p in ax.patches: ax.annotate(&#39;{:.1f}%&#39;.format(100*p.get_height()/total), (p.get_x()+0.1, p.get_height()+5), fontsize = 8) plt.show() else: print(emoji.emojize(&quot;:cross_mark:&quot;)*2 ,&quot;The entered feature is not categorical !!!!&quot; ,emoji.emojize(&quot;:cross_mark:&quot;)*2) . . Feature : job . cat_countplot(&#39;job&#39;, data) print(&quot; n t t t&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: type of job&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;job&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: type of job ğŸ“ƒğŸ“ƒ . ğŸ“ğŸ“ The plot of job shows that the &#39;admin.&#39; job has the highest no. of employees. The &#39;blue-collar&#39; job has the second highest no. of employees. The rest can be clearly interpreted in a similar manner from the bar-plot. The 5 most common jobs are enough to represent 80% of the data. . cat_countplot_wrt_Y(&#39;job&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;job&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ From the bar-plot of job we can see that the job type that subscribed to a term deposit the most is &#39;admin.&#39;, followed by &#39;blue-collar&#39; and &#39;students&#39; have the least subscription to a term deposit. . print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Plotting customers by profession and age&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) type_pivot = data.pivot_table( columns=&quot;target&quot;, index=&quot;job&quot;, values=&quot;age&quot;, aggfunc=np.mean) type_pivot.sort_values(by=[&quot;job&quot;], ascending=True).plot(kind=&quot;bar&quot;, title=(&quot;Type of customer by professional occupation and age&quot;), figsize=(6,4), fontsize = 12); . . ğŸ“ˆğŸ“ˆ Plotting customers by profession and age ğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ We see that â€˜retiredâ€™ and â€˜housemaidâ€™ are the oldest clients and the ones who have accepted the subscription more than any of the other classes. . Feature : marital . cat_countplot(&#39;marital&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: Marital situation&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;marital&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: Marital situation ğŸ“ƒğŸ“ƒ . ğŸ“ğŸ“ The clients who are &#39;married&#39; are high in records. . cat_countplot_wrt_Y(&#39;marital&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: Marriage Status&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;marital&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: Marriage Status ğŸ“ƒğŸ“ƒ . ğŸ“ğŸ“ The no. of people who have not subscribed to a term deposit most are &#39;married&#39;. . Feature : education . cat_countplot(&#39;education&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: Education level&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;education&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: Education level ğŸ“ƒğŸ“ƒ . ğŸ“ğŸ“ â€˜Educationâ€™ has 8 unique values. The top 4 education levels correspond to 80% of the data. The people having &#39;university.degree&#39; as highest education show more interest in a term deposit followed by the people having done &#39;high.school&#39; and &#39;illiterate&#39; people are not interested in a term deposit. . cat_countplot_wrt_Y(&#39;education&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;education&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ Also the highest refusal to the subscription is made by the &#39;university.degree&#39; holders followed by &#39;high.school&#39; graduated. Clients with 4 years basic or illiterate are the oldest and prone to subscribe to the product. . Feature : default . cat_countplot(&#39;default&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: has credit in default?&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;default&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: has credit in default? ğŸ“ƒğŸ“ƒ . ğŸ“ğŸ“ The highest interest in a term deposit is shown by the people who are having no credit &#39;defaults&#39;. . cat_countplot_wrt_Y(&#39;default&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;default&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ People with no credit defaults are not so interested in subscribing a term deposit. With 3 unique values, the class â€˜yesâ€™ is meaningless, the variable is unexpressive and totally imbalanced. So we will drop this column. . Feature: housing . cat_countplot(&#39;housing&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: has housing loan?&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;housing&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: has housing loan? ğŸ“ƒğŸ“ƒ . ğŸ“ğŸ“ The proportion of â€˜yesâ€™ and â€˜noâ€™ is very tight might reduce its predictive power. . cat_countplot_wrt_Y(&#39;housing&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;housing&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ People with &#39;housing&#39; loan or without &#39;housing&#39; loan do not make much difference in the subscription of a term deposit. . Feature: loan . cat_countplot(&#39;loan&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: has personal loan?&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;loan&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: has personal loan? ğŸ“ƒğŸ“ƒ . ğŸ“ğŸ“ â€˜loanâ€™ shows a high number of non-subscribers, this variable has some similarities with â€˜housingâ€™ in the sense that, proportionally, â€˜yesâ€™ and â€˜noâ€™ are very even. Once again, it might reduce its predictive power. . cat_countplot_wrt_Y(&#39;loan&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;loan&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ People with no personal &#39;loan&#39; tends to be more interested in a term deposit subscription. . Feature: contact . cat_countplot(&#39;contact&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: contact communication type&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;contact&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: contact communication type ğŸ“ƒğŸ“ƒ . ğŸ“ğŸ“ Most of the people are contacted through &#39;cellular&#39;. . cat_countplot_wrt_Y(&#39;contact&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;contact&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ When the people are contacted through &#39;cellular&#39; tends to subscribe as well as unsubscribe more than contacted through &#39;telephone&#39;. . Feature: month . cat_countplot(&#39;month&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: last contact month of year&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;month&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: last contact month of year ğŸ“ƒğŸ“ƒ . ğŸ“ğŸ“ Most of the people were last contacted in the month of May then followed by July &amp; August to a term subscription and least contacted lastly in December. . cat_countplot_wrt_Y(&#39;month&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;month&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ The highest no. of subscription and unsubscription are also made in the month of May then followed by July &amp; August. . Feature: day_of_week . cat_countplot(&#39;day_of_week&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: last contact day of the week&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;day_of_week&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: last contact day of the week ğŸ“ƒğŸ“ƒ . cat_countplot_wrt_Y(&#39;day_of_week&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;day_of_week&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ This feature doesnt make any significant change on the subscription of a term deposit and it can be dropped. . Feature: poutcome . cat_countplot(&#39;poutcome&#39;, data) print(&quot; n&quot;,emoji.emojize(&quot;:page_with_curl:&quot;)*2 ,&quot;Discription: outcome of the previous marketing campaign&quot; ,emoji.emojize(&quot;:page_with_curl:&quot;)*2) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;poutcome&#39; ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ƒğŸ“ƒ Discription: outcome of the previous marketing campaign ğŸ“ƒğŸ“ƒ . ğŸ“ğŸ“ The outcome in the previous campaigns is mostly as &#39;nonexistent&#39; followed by &#39;failure&#39; &amp; &#39;success&#39;. . cat_countplot_wrt_Y(&#39;poutcome&#39;, data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Distribution of &#39;poutcome&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ The plot shows &#39;nonexistent&#39; as the outcome where clients that didn&#39;t subscribed to a term deposit is more than the no. of clients that subscribed, &#39;failure&#39; is the outcome where large no. of clients refused to subscribe than to subscribe to a term deposit, &#39;success&#39; is the outcome where large no. of clients have subscribed than not subscribed a term deposit in the bank. Interestingly between the clients previously contacted from previous promotional campaigns that actually succeed, the majority subscribed this time. . 11. Exploring most common categories of categorical features . # What are the most common categories? print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*10 ,&#39; t&#39; ,&#39;Plotting most common categories&#39; ,&#39; t&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*10,&#39; n&#39;) total = len(categorical_features) plotnumber=1 plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(16,14)) for feature in data[categorical_features]: ax = plt.subplot(round(total/2),round(total/3), plotnumber) sns.countplot(x=data[feature], data=data,order=data[feature].value_counts().index) plt.xticks(rotation=90) plt.title(feature) plotnumber+=1 plt.tight_layout() . . ğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆ Plotting most common categories ğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ Most common categories are: âœï¸ job: administrative âœï¸ marital state: married âœï¸ education: university degree âœï¸ credit in default: no âœï¸ housing: yes, however not having a housing loan is very close âœï¸ loan: no âœï¸ poutcome: did not participate in previous campaigns All the features include the category â€˜unknownâ€™ except the â€˜poutcomeâ€™ variable. . 12. Exploring the influence of common categories on the target . # How these categories influence the target variable? print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*10 ,&#39; t&#39; ,&#39;Plotting most common categories influencing the target&#39; ,&#39; t&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*10,&#39; n&#39;) total = len(categorical_features) plotnumber=1 plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(16,14)) for feature in data[categorical_features]: ax = plt.subplot(round(total/2),round(total/3), plotnumber) data.groupby([feature])[&#39;target&#39;].mean().multiply(100).plot.barh() plt.xlabel(&#39;Subscribed [%]&#39;) plt.title(feature) plotnumber+=1 plt.tight_layout() . . ğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆ Plotting most common categories influencing the target ğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ Influence of common categories on the target : âœï¸ â€˜Studentâ€™ and â€˜retiredâ€™ have the highest percentage of subscriptions (&gt;25%) whereas â€˜blue-collarâ€™ and â€˜servicesâ€™ have the lowest. âœï¸ â€˜Illiterateâ€™ people have the highest percentage of subscriptions (&gt;20%), on the other hand â€˜basic 9yâ€™, â€˜basic 6yâ€™ and â€˜basic 4yâ€™ have the lowest. âœï¸ People with credit in default did not subscribe. âœï¸ More than 60% of the people previously contacted to other campaigns subscribed. âœï¸ Marital state, existence of loans, and housing do not influence much the subscription rate. . 13. Exploring correlation between the numerical features . # Subdivision of target ynum = num_data.target Xnum = num_data.drop([&quot;target&quot;], axis= &quot;columns&quot;) . . # The numeric most correlated with the target (Pearson) print(&#39; nNumeric features most correlated with the target n n&#39; ,&quot; t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) pearson = num_data.corr() corr_target = pearson.target corr1 = corr_target.sort_values(ascending=False) corr1 = corr1.to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#139BB4&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).applymap(above_zero).format({&#39;target&#39;: &quot;{:.3f}&quot;}) corr1 . . Numeric features most correlated with the target ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . target . target 1.000 | . duration 0.405 | . previous 0.230 | . cons_conf_idx 0.055 | . age 0.030 | . campaign -0.066 | . cons_price_idx -0.136 | . emp_var_rate -0.298 | . euribor3m -0.308 | . pdays -0.325 | . nr_employed -0.355 | . print(&quot; nOrdered by rank in absolute values n n&quot; ,&quot; t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) corr2 = corr_target.abs().sort_values(ascending=False) corr2 = corr2.to_frame().style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#139BB4&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).applymap(above_zero).format({&#39;target&#39;: &quot;{:.3f}&quot;}) corr2 . . Ordered by rank in absolute values ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . target . target 1.000 | . duration 0.405 | . nr_employed 0.355 | . pdays 0.325 | . euribor3m 0.308 | . emp_var_rate 0.298 | . previous 0.230 | . cons_price_idx 0.136 | . campaign 0.066 | . cons_conf_idx 0.055 | . age 0.030 | . # Heatmap visualization: Pearson print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8 ,&#39; t&#39; ,&#39;Heatmap visualization: Pearson&#39; ,&#39; t&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8,&#39; n&#39;) mask = np.triu(num_data.corr(method=&quot;pearson&quot;), 1) plt.figure(figsize=(19, 9)) sns.heatmap(num_data.corr(method=&quot;pearson&quot;), annot=True, vmax=1, vmin = -1, square=True, mask=mask) plt.title(&#39;Correlation b/w different features&#39;) plt.show() . . ğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆ Heatmap visualization: Pearson ğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ â€˜Nr_employedâ€™ is the most correlated with the target followed by â€˜pdaysâ€™, â€˜euribor3mâ€™, and â€˜emp_avr_rateâ€™ and at the same time, the strength of their relationships with the target is low. . # Checking the predicting power of the features print(&quot; nChecking the predicting power of the features n n&quot; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) # Identifying variables with predictive power (Pearson Correlation p-value) corr_df = pd.DataFrame( [scipy.stats.pearsonr(Xnum[col], ynum) for col in Xnum.columns], columns=[&quot;Pearson Corr.&quot;, &quot;p-value&quot;], index=Xnum.columns, ) corr_df.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#636A92&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).applymap(above_zero, subset=[&#39;Pearson Corr.&#39;]).format({&#39;Pearson Corr.&#39;: &quot;{:.3f}&quot;,&#39;p-value&#39; : &quot;{:.3f}&quot;}) . . Checking the predicting power of the features ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . Pearson Corr. p-value . age 0.030 | 0.000 | . duration 0.405 | 0.000 | . campaign -0.066 | 0.000 | . pdays -0.325 | 0.000 | . previous 0.230 | 0.000 | . emp_var_rate -0.298 | 0.000 | . cons_price_idx -0.136 | 0.000 | . cons_conf_idx 0.055 | 0.000 | . euribor3m -0.308 | 0.000 | . nr_employed -0.355 | 0.000 | . ğŸ“ğŸ“ It leads us to conclude that all features have predictive power. . ğŸ“ğŸ“ All linear variableâ€™s relationships are monotonic at the same time but the inverse is not always true, simply because we can have both monotonic non-linear correlations. . # Numeric variables with higher monotonicity (spearman) data_spearman = num_data.copy() data_spearman.drop([&quot;target&quot;], axis=1, inplace=True) . . print(&quot; nThe 10 most correlated numerical pairs by Spearman method n n&quot; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) spearman_rank = pg.pairwise_corr(data_spearman, method=&#39;spearman&#39;).loc[:,[&#39;X&#39;,&#39;Y&#39;,&#39;r&#39;]] pos = spearman_rank.sort_values(kind=&quot;quicksort&quot;, by=[&#39;r&#39;], ascending=False).iloc[:5,:] neg = spearman_rank.sort_values(kind=&quot;quicksort&quot;, by=[&#39;r&#39;], ascending=False).iloc[-5:,:] con = pd.concat([pos,neg], axis=0) corr_pairs = pd.DataFrame(con.reset_index(drop=True)) corr_pairs.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#053975&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).applymap(above_zero, subset=[&#39;r&#39;]).format({&#39;r&#39; : &quot;{:.3f}&quot;}).set_properties(**{&#39;background-color&#39;: &#39;#D4E0EE&#39;}, subset=[&#39;X&#39;,&#39;Y&#39;]) . . The 10 most correlated numerical pairs by Spearman method ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . X Y r . 0 emp_var_rate | nr_employed | 0.945 | . 1 emp_var_rate | euribor3m | 0.940 | . 2 euribor3m | nr_employed | 0.929 | . 3 emp_var_rate | cons_price_idx | 0.665 | . 4 cons_price_idx | euribor3m | 0.491 | . 5 previous | cons_price_idx | -0.283 | . 6 previous | emp_var_rate | -0.435 | . 7 previous | nr_employed | -0.439 | . 8 previous | euribor3m | -0.455 | . 9 pdays | previous | -0.510 | . # Heatmap visualization: Spearman print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8 ,&#39; t&#39; ,&#39;Heatmap visualization: Spearman&#39; ,&#39; t&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*8,&#39; n&#39;) mask = np.triu(data_spearman.corr(method=&#39;spearman&#39;), 1) plt.figure(figsize=(19, 9)) sns.heatmap(data_spearman.corr(method=&#39;spearman&#39;), annot=True, vmax=1, vmin = -1, square=True, mask=mask) plt.title(&#39;Correlation b/w different features&#39;) plt.show() . . ğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆ Heatmap visualization: Spearman ğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ â€˜nr_employedâ€™ is the most correlated with the target. The variables â€˜emp_var_rateâ€™, â€˜nr_employedâ€™, and â€˜euribor3mâ€™ are very redundant but believe this does not represent a big issue. So we will keep all features for the time being. . Data Preprocessing . # To have a glimpse of the data print(&quot; nGlimpse of Categorical data : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) #Replacing &#39;unknown&#39; by NaN cat_data.replace(to_replace=&quot;unknown&quot;, value=np.nan, inplace=True) cat_data.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#610646&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).highlight_null(null_color=&#39;#CCB3C5&#39;) . . Glimpse of Categorical data : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . job marital education default housing loan contact month day_of_week poutcome . 0 housemaid | married | basic.4y | no | no | no | telephone | may | mon | nonexistent | . 1 services | married | high.school | nan | no | no | telephone | may | mon | nonexistent | . 2 services | married | high.school | no | yes | no | telephone | may | mon | nonexistent | . 3 admin. | married | basic.6y | no | no | no | telephone | may | mon | nonexistent | . 4 services | married | high.school | no | no | yes | telephone | may | mon | nonexistent | . 5 services | married | basic.9y | nan | no | no | telephone | may | mon | nonexistent | . 6 admin. | married | professional.course | no | no | no | telephone | may | mon | nonexistent | . 7 blue-collar | married | nan | nan | no | no | telephone | may | mon | nonexistent | . 8 technician | single | professional.course | no | yes | no | telephone | may | mon | nonexistent | . 9 services | single | high.school | no | yes | no | telephone | may | mon | nonexistent | . 1. Removing Unwanted Columns . #removing &#39;duration&#39;,&#39;default&#39;,&#39;day_of_week&#39; attributes cat_data = cat_data.drop([&quot;default&quot;,&quot;day_of_week&quot;],axis=1) num_data = num_data.drop(&quot;duration&quot;,axis=1) print(emojis.encode(&quot;:scissors:&quot;)*18 ,&quot; n nUnwanted columns successfully removed !!! n n&quot; ,emojis.encode(&quot;:scissors:&quot;)*18) . . âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸ Unwanted columns successfully removed !!! âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸ . print(&#39; nColumns &amp; Rows of both the dataframes n n&#39; ,&quot; t &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;Categorical Dataframe : &quot;,cat_data.shape) print(&quot;Numerical Dataframe : &quot;,num_data.shape) . . Columns &amp; Rows of both the dataframes ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» Categorical Dataframe : (41188, 8) Numerical Dataframe : (41188, 10) . 2. Handling missing values . # Imputation of missing values by the modal value print(&#39; nImputating missing values by the modal value : &#39; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) #cat_data = cat_data.fillna(cat_data.value_counts().index[0]) cat_data_imputed = cat_data.apply(lambda x: x.fillna(x.value_counts().index[0])) cat_data_imputed.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#2B0E46&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Imputating missing values by the modal value : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . job marital education housing loan contact month poutcome . 0 housemaid | married | basic.4y | no | no | telephone | may | nonexistent | . 1 services | married | high.school | no | no | telephone | may | nonexistent | . 2 services | married | high.school | yes | no | telephone | may | nonexistent | . 3 admin. | married | basic.6y | no | no | telephone | may | nonexistent | . 4 services | married | high.school | no | yes | telephone | may | nonexistent | . 5 services | married | basic.9y | no | no | telephone | may | nonexistent | . 6 admin. | married | professional.course | no | no | telephone | may | nonexistent | . 7 blue-collar | married | university.degree | no | no | telephone | may | nonexistent | . 8 technician | single | professional.course | yes | no | telephone | may | nonexistent | . 9 services | single | high.school | yes | no | telephone | may | nonexistent | . 3. Handling Outliers . Feature : age . ğŸ“ğŸ“ It looks like normal distribution. The skewness value should be between -1 and +1, and any major deviation from this range indicates the presence of extreme values(outliers). Assuming that age follows a Gaussian Distribution we will calculate the boundaries which differentiate the outliers. We have computed Interquantile range as 15. . #calculating the boundaries which differentiate the outliers IQR = num_data.age.quantile(0.75)-num_data.age.quantile(0.25) lower_bridge = num_data[&#39;age&#39;].quantile(0.25)-(IQR*1.5) upper_bridge = num_data[&#39;age&#39;].quantile(0.75)+(IQR*1.5) print(&#39; nLower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;lower boundary limit : {} nupper boundary limit : {}&quot;.format(lower_bridge,upper_bridge)) . . Lower &amp; Upper Limits ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» lower boundary limit : 9.5 upper boundary limit : 69.5 . ğŸ“ğŸ“ The maximum value of age is 98 years and from here we can see that the age value above 69.5 should be treated as outliers. . #replace outliers with upper boundary limit num_data.loc[num_data[&#39;age&#39;]&gt;=69.5,&#39;age&#39;] = 69.5 print(emoji.emojize(&quot;:plus:&quot;)*14 ,&quot; n nOutliers successfully replaced n n&quot; ,emoji.emojize(&quot;:plus:&quot;)*14) . . â•â•â•â•â•â•â•â•â•â•â•â•â•â• Outliers successfully replaced â•â•â•â•â•â•â•â•â•â•â•â•â•â• . #boxplot and other information after handling outlier num_boxplot_wrt_Y(&#39;age&#39;, num_data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Boxplot of &#39;age&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . âœ”âœ” ğŸ‘ğŸ»ğŸ‘ğŸ» NO Outliers ğŸ‘ğŸ»ğŸ‘ğŸ» âœ”âœ” . Parameters Values . 1 Skewness | 0.57 | . 2 Kurtosis | -0.25 | . 3 Median | 38.00 | . 4 Count | 41188.00 | . 5 Mean | 39.94 | . 6 Stand. Dev. | 10.13 | . 7 Minimum | 17.00 | . 8 Q1 (25%) | 32.00 | . 9 Q2 (50%) | 38.00 | . 10 Q3 (75%) | 47.00 | . 11 Maximum | 69.50 | . 12 IQR | 15.00 | . 13 observation count w/o outliers | 41188 (100.0%) | . # displaying each categorical feature with its unique no. of categories # without the category &#39;unknown&#39; cat_features = [feature for feature in cat_data_imputed.columns if (cat_data_imputed[feature].dtypes==&#39;O&#39;)] print(&quot;Categorical variables after imputing &#39;unknown&#39; values.&quot;) for feature in cat_features: print(&quot; nThe variable &#39;{}&#39; has {} categories&quot;.format(feature,len(cat_data_imputed[feature].unique())) ,&quot; n&quot; ,&quot; t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3 ,&quot; n&quot; ,&quot;{}&quot;.format(cat_data_imputed[feature].unique())) . . Categorical variables after imputing &#39;unknown&#39; values. The variable &#39;job&#39; has 11 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;housemaid&#39; &#39;services&#39; &#39;admin.&#39; &#39;blue-collar&#39; &#39;technician&#39; &#39;retired&#39; &#39;management&#39; &#39;unemployed&#39; &#39;self-employed&#39; &#39;entrepreneur&#39; &#39;student&#39;] The variable &#39;marital&#39; has 3 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;married&#39; &#39;single&#39; &#39;divorced&#39;] The variable &#39;education&#39; has 7 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;basic.4y&#39; &#39;high.school&#39; &#39;basic.6y&#39; &#39;basic.9y&#39; &#39;professional.course&#39; &#39;university.degree&#39; &#39;illiterate&#39;] The variable &#39;housing&#39; has 2 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;no&#39; &#39;yes&#39;] The variable &#39;loan&#39; has 2 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;no&#39; &#39;yes&#39;] The variable &#39;contact&#39; has 2 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;telephone&#39; &#39;cellular&#39;] The variable &#39;month&#39; has 10 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;may&#39; &#39;jun&#39; &#39;jul&#39; &#39;aug&#39; &#39;oct&#39; &#39;nov&#39; &#39;dec&#39; &#39;mar&#39; &#39;apr&#39; &#39;sep&#39;] The variable &#39;poutcome&#39; has 3 categories ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» [&#39;nonexistent&#39; &#39;failure&#39; &#39;success&#39;] . Feature : campaign . ğŸ“ğŸ“ The histogram shows that &#39;campaign&#39; attribute is right-skewed and also the skewness value is 4.8. Now we will calculate the extreme boundaries which differentiate the outliers. We have computed Interquantile range as 2. . # Upper &amp; lower limits of outliers print(&#39; nLower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;lower boundary limit : {} nupper boundary limit : {}&quot;.format(lower_bridge,upper_bridge)) . . Lower &amp; Upper Limits ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» lower boundary limit : 9.5 upper boundary limit : 69.5 . # Extreme outliers print(&#39; n Extreme Lower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; t&quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;lower boundary limit : {} nupper boundary limit : {}&quot;.format(num_data[&#39;campaign&#39;].quantile(0.10),num_data[&#39;campaign&#39;].quantile(0.90))) . . Extreme Lower &amp; Upper Limits ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» lower boundary limit : 1.0 upper boundary limit : 5.0 . #replace outliers with extreme lower &amp; upper boundary limit num_data.loc[num_data[&#39;campaign&#39;]&lt;=1,&#39;campaign&#39;] = 1 num_data.loc[num_data[&#39;campaign&#39;]&gt;=5,&#39;campaign&#39;] = 5 print(emoji.emojize(&quot;:plus:&quot;)*20 ,&quot; n nOutliers successfully replaced with extreme limits n n&quot; ,emoji.emojize(&quot;:plus:&quot;)*20) . . â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• Outliers successfully replaced with extreme limits â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• . #boxplot and other information after handling outliers num_boxplot_wrt_Y(&#39;campaign&#39;, num_data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Boxplot of &#39;campaign&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . âœ”âœ” ğŸ‘ğŸ»ğŸ‘ğŸ» NO Outliers ğŸ‘ğŸ»ğŸ‘ğŸ» âœ”âœ” . Parameters Values . 1 Skewness | 0.94 | . 2 Kurtosis | -0.38 | . 3 Median | 2.00 | . 4 Count | 41188.00 | . 5 Mean | 2.19 | . 6 Stand. Dev. | 1.37 | . 7 Minimum | 1.00 | . 8 Q1 (25%) | 1.00 | . 9 Q2 (50%) | 2.00 | . 10 Q3 (75%) | 3.00 | . 11 Maximum | 5.00 | . 12 IQR | 2.00 | . 13 observation count w/o outliers | 41188 (100.0%) | . Feature : previous . ğŸ“ğŸ“ The method used to identify outliers selects many records which is 14% . so decided to keep the records because the data seems to have been measured correctly and reflects reality. To emphasize that the model is not affected by the extension of the no. of outliers, we will only guarantee that we will use a standardization technique that does not neglect to detail the distances between the central values. . Feature : cons_conf_idx . ğŸ“ğŸ“ Assuming that &#39;cons_conf_idx&#39; follows a Gaussian Distribution we will calculate the boundaries which differentiate the outliers. We have computed Interquantile range as 6.3. . #calculating the boundaries which differentiate the outliers print(&#39; nLower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) IQR = num_data.cons_conf_idx.quantile(0.75)-num_data.cons_conf_idx.quantile(0.25) lower_bridge = num_data[&#39;cons_conf_idx&#39;].quantile(0.25)-(IQR*1.5) upper_bridge = num_data[&#39;cons_conf_idx&#39;].quantile(0.75)+(IQR*1.5) print(&quot;lower boundary limit : {:.2f} nupper boundary limit : {:.2f}&quot;.format(lower_bridge,upper_bridge)) . . Lower &amp; Upper Limits ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» lower boundary limit : -52.15 upper boundary limit : -26.95 . # Extreme outliers print(&#39; nExtreme Lower &amp; Upper Limits n&#39; ,&quot; n&quot; ,&quot; &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) print(&quot;lower boundary limit : {} nupper boundary limit : {}&quot;.format(num_data[&#39;cons_conf_idx&#39;].quantile(0.10),num_data[&#39;cons_conf_idx&#39;].quantile(0.90))) . . Extreme Lower &amp; Upper Limits ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» lower boundary limit : -46.2 upper boundary limit : -36.1 . #replace outliers with upper boundary limit num_data.loc[num_data[&#39;cons_conf_idx&#39;]&gt;=-36,&#39;cons_conf_idx&#39;] = -36 print(emoji.emojize(&quot;:plus:&quot;)*20 ,&quot; n nOutliers successfully replaced with extreme limits n n&quot; ,emoji.emojize(&quot;:plus:&quot;)*20) . . â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• Outliers successfully replaced with extreme limits â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• . #boxplot and other information after handling outliers num_boxplot_wrt_Y(&#39;cons_conf_idx&#39;, num_data) . . ğŸ“ˆğŸ“ˆğŸ“ˆ Boxplot of &#39;cons_conf_idx&#39; w.r.t target ğŸ“ˆğŸ“ˆğŸ“ˆ . âœ”âœ” ğŸ‘ğŸ»ğŸ‘ğŸ» NO Outliers ğŸ‘ğŸ»ğŸ‘ğŸ» âœ”âœ” . Parameters Values . 1 Skewness | -0.19 | . 2 Kurtosis | -1.21 | . 3 Median | -41.80 | . 4 Count | 41188.00 | . 5 Mean | -40.82 | . 6 Stand. Dev. | 4.07 | . 7 Minimum | -50.80 | . 8 Q1 (25%) | -42.70 | . 9 Q2 (50%) | -41.80 | . 10 Q3 (75%) | -36.40 | . 11 Maximum | -36.00 | . 12 IQR | 6.30 | . 13 observation count w/o outliers | 41188 (100.0%) | . 4.Transformation . Feature : pdays . ğŸ“ğŸ“ Our dataset is not evenly distributed as the values in pdays are out of range so we need to scale it. splitting &#39;pdays&#39; feature into 2 features - &#39;pdays1&#39; and &#39;pdays2&#39; . # creating a new column named &quot;pdays2&quot; based on the value in &quot;pdays&quot; column def function (row): if(row[&#39;pdays&#39;]==999): return 0; return 1; num_data[&#39;pdays2&#39;]=num_data.apply(lambda row: function(row),axis=1) # changing the value 999 in pdays column to value 30 def function1 (row): if(row[&#39;pdays&#39;]==999): return 30; return row[&#39;pdays&#39;]; num_data[&#39;pdays&#39;]=num_data.apply(lambda row: function1(row),axis=1) print(emoji.emojize(&quot;:plus:&quot;)*17 ,&quot; n nNew column &#39;pdays2&#39; created &amp; replaced n the 999 values with 30 in &#39;pdays&#39; n n&quot; ,emoji.emojize(&quot;:plus:&quot;)*17) . . â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• New column &#39;pdays2&#39; created &amp; replaced the 999 values with 30 in &#39;pdays&#39; â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• . #changing the type of pdays to int num_data[&#39;pdays&#39;]=num_data[&#39;pdays&#39;].astype(&#39;int64&#39;) #renaming column pdays to pdays1 num_data.rename(columns={&#39;pdays&#39;: &#39;pdays1&#39;},inplace=True) print(emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*20 ,&quot; n nSuccessfully converted the type of &#39;pdays&#39; n&amp; renamed to &#39;pdays1&#39; n n&quot; ,emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*20) . . ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ» Successfully converted the type of &#39;pdays&#39; &amp; renamed to &#39;pdays1&#39; ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ» . 5. Balancing the target variable . print(&quot; nComplete Dataset : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) data_new = pd.concat([num_data,cat_data_imputed],axis=1) data_new.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A60B2E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Complete Dataset : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . age campaign pdays1 previous emp_var_rate cons_price_idx cons_conf_idx euribor3m nr_employed target pdays2 job marital education housing loan contact month poutcome . 0 56.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | housemaid | married | basic.4y | no | no | telephone | may | nonexistent | . 1 57.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | services | married | high.school | no | no | telephone | may | nonexistent | . 2 37.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | services | married | high.school | yes | no | telephone | may | nonexistent | . 3 40.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | admin. | married | basic.6y | no | no | telephone | may | nonexistent | . 4 56.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | services | married | high.school | no | yes | telephone | may | nonexistent | . 5 45.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | services | married | basic.9y | no | no | telephone | may | nonexistent | . 6 59.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | admin. | married | professional.course | no | no | telephone | may | nonexistent | . 7 41.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | blue-collar | married | university.degree | no | no | telephone | may | nonexistent | . 8 24.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | technician | single | professional.course | yes | no | telephone | may | nonexistent | . 9 25.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | services | single | high.school | yes | no | telephone | may | nonexistent | . data_new.shape . . (41188, 19) . ğŸ“ğŸ“ Data corresponding to y is very skewed, so we duplicate the tuples corresponding to &#39;yes&#39; . #handling the imbalance dataset data1=data_new.copy() data2=data1[data1.target==1] data1=pd.concat([data1, data2]) data1=pd.concat([data1, data2]) data1=pd.concat([data1, data2]) data1=pd.concat([data1, data2]) data1=pd.concat([data1, data2]) data1=pd.concat([data1, data2]) data1=pd.concat([data1, data2]) bal_data=data1 print(emojis.encode(&quot;:crossed_flags:&quot;)*9 ,&quot; n nDataset is balanced n n&quot; ,emojis.encode(&quot;:crossed_flags:&quot;)*9) . . ğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒ Dataset is balanced ğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒ . #Checking the dataset is balanced or not based on target values in the classification. print(&#39; n&#39;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&#39;Plotting the balanced dataset&#39; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&#39; n&#39;) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) total = len(bal_data[&#39;target&#39;])*1 ax=sns.countplot(x=&#39;target&#39;,data=bal_data) for p in ax.patches: ax.annotate(&#39;{:.1f}%&#39;.format(100*p.get_height()/total), (p.get_x()+0.1, p.get_height()+5), fontsize = 12) . . ğŸ“ˆğŸ“ˆ Plotting the balanced dataset ğŸ“ˆğŸ“ˆ . ğŸ“ğŸ“ Now the data looks much more balanced. . 6. Encoding the data . Label Encoding . print(&#39; nEncoded Data : &#39; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) le_data = bal_data.copy() le = preprocessing.LabelEncoder() le_data.job = le.fit_transform(le_data.job) le_data.marital = le.fit_transform(le_data.marital) le_data.education = le.fit_transform(le_data.education) le_data.housing = le.fit_transform(le_data.housing) le_data.loan = le.fit_transform(le_data.loan) le_data.contact = le.fit_transform(le_data.contact) le_data.month = le.fit_transform(le_data.month) le_data.poutcome = le.fit_transform(le_data.poutcome) le_data.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A15F86&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Encoded Data : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . age campaign pdays1 previous emp_var_rate cons_price_idx cons_conf_idx euribor3m nr_employed target pdays2 job marital education housing loan contact month poutcome . 0 56.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 3 | 1 | 0 | 0 | 0 | 1 | 6 | 1 | . 1 57.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 7 | 1 | 3 | 0 | 0 | 1 | 6 | 1 | . 2 37.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 7 | 1 | 3 | 1 | 0 | 1 | 6 | 1 | . 3 40.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 6 | 1 | . 4 56.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 7 | 1 | 3 | 0 | 1 | 1 | 6 | 1 | . 5 45.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 7 | 1 | 2 | 0 | 0 | 1 | 6 | 1 | . 6 59.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 0 | 1 | 5 | 0 | 0 | 1 | 6 | 1 | . 7 41.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 1 | 1 | 6 | 0 | 0 | 1 | 6 | 1 | . 8 24.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 9 | 2 | 5 | 1 | 0 | 1 | 6 | 1 | . 9 25.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 7 | 2 | 3 | 1 | 0 | 1 | 6 | 1 | . 7. Splitting the data into train &amp; test . # In label encoded data print(&#39; nLabel Encoded Data without Target :&#39; ,&quot; &quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) # addinq a new column &#39;Target&#39; and dropping old column &#39;target&#39; ds = le_data.copy() ds[&quot;Target&quot;] = ds[&quot;target&quot;] ds = ds.drop(&quot;target&quot;,axis=1) ds.head(10).style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#436C3A&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ) . . Label Encoded Data without Target : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . age campaign pdays1 previous emp_var_rate cons_price_idx cons_conf_idx euribor3m nr_employed pdays2 job marital education housing loan contact month poutcome Target . 0 56.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 3 | 1 | 0 | 0 | 0 | 1 | 6 | 1 | 0 | . 1 57.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 7 | 1 | 3 | 0 | 0 | 1 | 6 | 1 | 0 | . 2 37.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 7 | 1 | 3 | 1 | 0 | 1 | 6 | 1 | 0 | . 3 40.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 6 | 1 | 0 | . 4 56.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 7 | 1 | 3 | 0 | 1 | 1 | 6 | 1 | 0 | . 5 45.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 7 | 1 | 2 | 0 | 0 | 1 | 6 | 1 | 0 | . 6 59.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 0 | 1 | 5 | 0 | 0 | 1 | 6 | 1 | 0 | . 7 41.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 1 | 1 | 6 | 0 | 0 | 1 | 6 | 1 | 0 | . 8 24.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 9 | 2 | 5 | 1 | 0 | 1 | 6 | 1 | 0 | . 9 25.000000 | 1 | 30 | 0 | 1.100000 | 93.994000 | -36.400000 | 4.857000 | 5191.000000 | 0 | 7 | 2 | 3 | 1 | 0 | 1 | 6 | 1 | 0 | . # Dividing the label encoded dataset into independent and dependent variables X = ds.iloc[:, : -1].values y = ds.iloc[:, -1].values # Splitting into train &amp; test data X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 42) print(emojis.encode(&quot;:scissors:&quot;)*22 ,&quot; n nData divided into Dependent &amp; Independent Variables nand Split into Train &amp; Test data n n&quot; ,emojis.encode(&quot;:scissors:&quot;)*22) . . âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸ Data divided into Dependent &amp; Independent Variables and Split into Train &amp; Test data âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸âœ‚ï¸ . 8. Feature Scaling . # Standardization in the label encoded data scaler = StandardScaler() X_train[:, : 10] = scaler.fit_transform(X_train[:, : 10]) X_test[:, : 10] = scaler.transform(X_test[:, : 10]) print(emoji.emojize(&quot;:triangular_flag:&quot;)*18 ,&quot; n nSuccessfully accomplished feature scaling n n&quot; ,emoji.emojize(&quot;:triangular_flag:&quot;)*18) . . ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš© Successfully accomplished feature scaling ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš©ğŸš© . #convert array into dataframe X_train = pd.DataFrame(X_train) X_test = pd.DataFrame(X_test) y_train = pd.DataFrame(y_train) y_test = pd.DataFrame(y_test) print(emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*18 ,&quot; n nX_train,X_test,y_train &amp; y_test converted into dataframe n n&quot; ,emoji.emojize(&quot;:thumbs_up_light_skin_tone:&quot;)*18) . . ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ» X_train,X_test,y_train &amp; y_test converted into dataframe ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ» . Building Models . # Confusion Matrix def confusion_mat(y_test,y_pred): &quot;&quot;&quot; Takes y_test &amp; y_pred of a model as input and return styler object of accuracy, precision,recall &amp; R1 Score for that particular model. &quot;&quot;&quot; cm = confusion_matrix(y_test,y_pred) print(&quot;Confusion Matrix : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3,&quot; n&quot;) plt.style.use(&#39;dark_background&#39;) sns.heatmap(cm,annot=True,fmt=&#39;d&#39;) plt.show() print(&quot;&quot;) accuracy = metrics.accuracy_score(y_pred, y_test)*100 precision = metrics.precision_score(y_pred,y_test)*100 recall = metrics.recall_score(y_pred,y_test)*100 r1_score = metrics.f1_score(y_pred,y_test)*100 v1 = pd.DataFrame({&#39;Parameters&#39;: &#39;Accuracy Score&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(accuracy)},index={&#39;1&#39;}) v2 = pd.DataFrame({&#39;Parameters&#39;: &#39;Precision Score&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(precision)},index={&#39;2&#39;}) v3 = pd.DataFrame({&#39;Parameters&#39;: &#39;Recall Score&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(recall)},index={&#39;3&#39;}) v4 = pd.DataFrame({&#39;Parameters&#39;: &#39;R1 Score&#39; , &#39;Values&#39; : &quot;{:.2f}&quot;.format(r1_score)},index={&#39;4&#39;}) result = pd.concat([v1,v2,v3,v4]) result.columns = [&#39;Parameters&#39;,&#39;Values&#39;] result = result.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#34495E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#FEF5E7&#39;}, subset=[&#39;Parameters&#39;]) return result . . # AUC_ROC curve def plot_roc_curve(y_test,y_pred,label,color): &quot;&quot;&quot; Takes y_test, y_pred, label &amp; color as input and plots the ROC curve. &quot;&quot;&quot; print(&quot; n&quot;,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2 ,&quot;Plotting ROC Curve&quot; ,emoji.emojize(&quot;:chart_with_upwards_trend:&quot;, use_aliases=True)*2,&quot; n&quot;) print(&quot;&quot;) plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(8,5)) plt.title(&#39;ROC Curve&#39;) # Computing False postive rate, and True positive rate fpr,tpr,threshold=roc_curve(y_test,y_pred) # Calculating Area under the curve to display on the plot auc = metrics.roc_auc_score(y_test,y_pred) # Now, plotting the computed values plt.plot(fpr, tpr,label = label , color=color, linewidth=2) # Custom settings for the plot plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.axis([-0.005, 1, 0, 1.005]) plt.xticks(np.arange(0,1, 0.05), rotation=90) plt.xlabel(&quot;False Positive Rate&quot;) plt.ylabel(&quot;True Positive Rate (Recall)&quot;) plt.legend(loc=&quot;lower right&quot;) plt.show() . . def kf_score(acc): &quot;&quot;&quot; Takes accuracy of the model as input and returns styler object of mean and standard deviation of accuracy of that particular model. &quot;&quot;&quot; r1 = pd.DataFrame({&#39;Score&#39;: &#39;Accuracy&#39;, &#39;Values&#39; : &quot;{:.2f}&quot;.format(acc.mean()*100)},index={&#39;1&#39;}) r2 = pd.DataFrame({&#39;Score&#39;: &#39;Stand. Dev.&#39;, &#39;Values&#39; : &quot;{:.2f}&quot;.format(acc.std()*100)},index={&#39;2&#39;}) res = pd.concat([r1,r2]) res.columns = [&#39;Score&#39;,&#39;Values&#39;] res=res.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#A60B2E&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#F2DBE0&#39;}, subset=[&#39;Score&#39;]) return res . . def grid_cv_params(model,param1,param2): &quot;&quot;&quot; Takes model and two parameters for which values are to be determined and returns styler object of Acuuracy and Best parameters. &quot;&quot;&quot; print(&quot;After Tuning Parameters : &quot; ,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3,&quot; n&quot;) p1 = model.best_params_[param1] p2 = model.best_params_[param2] r1 = pd.DataFrame({&#39;Parameters&#39;: &#39;Accuracy&#39;, &#39;Values&#39; : &quot;{:.2f}&quot;.format((model.best_score_)*100)},index={&#39;1&#39;}) r2 = pd.DataFrame({&#39;Parameters&#39;: &#39;Best Parameters&#39;, &#39;Values&#39; : &quot;&#39;{}&#39;: {} {} &#39;{}&#39;: &#39;{}&#39;&quot;.format(param1,p1,&quot;,&quot;,param2,p2)},index={&#39;2&#39;}) res = pd.concat([r1,r2]) res.columns = [&#39;Parameters&#39;,&#39;Values&#39;] res=res.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#035753&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#CAEEE5&#39;}, subset=[&#39;Parameters&#39;]) return res . . 1. Linear Logistic Regression . Training the model . # Training the model model1 = LogisticRegression() model1 = model1.fit(X_train,y_train) . . Testing the model . y_pred1 = model1.predict(X_test) . . Confusion Matrix . confusion_mat(y_test,y_pred1) . . Confusion Matrix : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . . Parameters Values . 1 Accuracy Score | 73.42 | . 2 Precision Score | 65.09 | . 3 Recall Score | 78.23 | . 4 R1 Score | 71.06 | . ROC Curve . plot_roc_curve(y_test,y_pred1,&quot;LLR&quot;,&quot;blue&quot;) . . ğŸ“ˆğŸ“ˆ Plotting ROC Curve ğŸ“ˆğŸ“ˆ . Applying k-fold cross validation . acc_llr_kf = cross_val_score(estimator=model1, X=X_train, y=y_train, cv=10) . . kf_score(acc_llr_kf) . . Score Values . 1 Accuracy | 73.21 | . 2 Stand. Dev. | 0.55 | . 2. Polynomial Logistic Regression(with degree=3) . Training the model . poly_feat = PolynomialFeatures(degree=3) X_train_poly = poly_feat.fit_transform(X_train) . . # Training the model on the label encoded training set model2 = LogisticRegression() model2 = model2.fit(X_train_poly,y_train) . . Testing the model . y_pred2 = model2.predict(poly_feat.fit_transform(X_test)) . . Confusion Matrix . confusion_mat(y_test,y_pred2) . . Confusion Matrix : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . . Parameters Values . 1 Accuracy Score | 74.29 | . 2 Precision Score | 66.00 | . 3 Recall Score | 79.25 | . 4 R1 Score | 72.02 | . ROC Curve . plot_roc_curve(y_test,y_pred2,&quot;PLR&quot;,&quot;darkorange&quot;) . . ğŸ“ˆğŸ“ˆ Plotting ROC Curve ğŸ“ˆğŸ“ˆ . Applying k-fold cross validation . acc_plr_kf = cross_val_score(estimator=model2, X=X_train_poly, y=y_train, cv=10) . . kf_score(acc_plr_kf) . . Score Values . 1 Accuracy | 74.25 | . 2 Stand. Dev. | 0.46 | . 3. K-NN . Training the model . # Training the model on the label encoded training set model3 = KNeighborsClassifier(n_neighbors=5, weights=&#39;distance&#39;, metric=&#39;minkowski&#39;, p=2) model3 = model3.fit(X_train,y_train) . . Testing the model . y_pred3 = model3.predict(X_test) . . Confusion Matrix . confusion_mat(y_test,y_pred3) . . Confusion Matrix : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . . Parameters Values . 1 Accuracy Score | 89.29 | . 2 Precision Score | 99.93 | . 3 Recall Score | 82.44 | . 4 R1 Score | 90.34 | . ROC Curve . plot_roc_curve(y_test,y_pred3,&quot;K_NN&quot;,&quot;green&quot;) . . ğŸ“ˆğŸ“ˆ Plotting ROC Curve ğŸ“ˆğŸ“ˆ . Tuning parameters with GridsearchCV . param_grid3 = {&#39;n_neighbors&#39;:[3,4,5,6,7,8,9,10,11,12], &#39;weights&#39;:[&#39;uniform&#39;,&#39;distance&#39;]} model3_gscv = GridSearchCV(KNeighborsClassifier(), param_grid = param_grid3, cv=10) model3_gscv = model3_gscv.fit(X_train,y_train) . . grid_cv_params(model3_gscv,&#39;n_neighbors&#39;,&#39;weights&#39;) . . After Tuning Parameters : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . Parameters Values . 1 Accuracy | 90.84 | . 2 Best Parameters | &#39;n_neighbors&#39;: 3 , &#39;weights&#39;: &#39;distance&#39; | . 4. Random Forest . Training the model . # Training the model on the label encoded training set model4 = RandomForestClassifier(oob_score=True,n_estimators=100 , max_features=6, n_jobs=-1) model4 = model4.fit(X_train,y_train) . . Testing the model . y_pred4 = model4.predict(X_test) . . Confusion Matrix . confusion_mat(y_test,y_pred4) . . Confusion Matrix : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . . Parameters Values . 1 Accuracy Score | 95.84 | . 2 Precision Score | 99.95 | . 3 Recall Score | 92.38 | . 4 R1 Score | 96.02 | . ROC Curve . plot_roc_curve(y_test,y_pred4,&quot;RF&quot;,&quot;red&quot;) . . ğŸ“ˆğŸ“ˆ Plotting ROC Curve ğŸ“ˆğŸ“ˆ . Feature Importance from Random Forest Model . model4.feature_importances_ indices = np.argsort(model4.feature_importances_)[::-1] . . X_train = pd.DataFrame(X_train) indices = np.argsort(model4.feature_importances_)[::-1] feature_rank = pd.DataFrame( columns = [&#39;rank&#39;, &#39;feature&#39;, &#39;importance&#39;] ) for f in range(X_train.shape[1]): feature_rank.loc[f] = [f+1, X_train.columns[indices[f]], model4.feature_importances_[indices[f]]] plt.style.use(&#39;dark_background&#39;) ax=sns.barplot( y = &#39;feature&#39;, x = &#39;importance&#39;, data = feature_rank) plt.xticks(rotation=90) plt.show() . . Tuning parameters with GridsearchCV . param_grid4 = {&#39;max_features&#39;:[5,6,7,8,9,10,11,12],&#39;n_estimators&#39;:[50,60,70,80,90,100]} model4_gscv = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid4, cv=5,verbose=True, n_jobs=-1) model4_detector = model4_gscv.fit(X_train,y_train) . . Fitting 5 folds for each of 48 candidates, totalling 240 fits . grid_cv_params(model4_detector,&#39;n_estimators&#39;,&#39;max_features&#39;) . . After Tuning Parameters : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . Parameters Values . 1 Accuracy | 95.27 | . 2 Best Parameters | &#39;n_estimators&#39;: 70 , &#39;max_features&#39;: &#39;5&#39; | . 5. XGBoost . Training the model . # Training the model on the label encoded training set model5 = XGBClassifier(n_estimators=200 ,random_state = 42,n_jobs=-1,verbose=1) model5 = model5.fit(X_train,y_train) . . Testing the model . y_pred5 = model5.predict(X_test) . . Confusion Matrix . confusion_mat(y_test,y_pred5) . . Confusion Matrix : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . . Parameters Values . 1 Accuracy Score | 75.50 | . 2 Precision Score | 65.85 | . 3 Recall Score | 81.74 | . 4 R1 Score | 72.94 | . ROC Curve . plot_roc_curve(y_test,y_pred5,&quot;XGB&quot;,&quot;purple&quot;) . . ğŸ“ˆğŸ“ˆ Plotting ROC Curve ğŸ“ˆğŸ“ˆ . After tuning all parameters . xgb_estimator = XGBClassifier( learning_rate=0.01,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=1, subsample=0.8,colsample_bytree=0.8,n_jobs=-1,reg_alpa=1,scale_pos_weight=1,random_state=42,verbose=1) . . xgb_estimator.fit(X_train, y_train) . . XGBClassifier(colsample_bytree=0.8, gamma=1, learning_rate=0.01, max_depth=5, n_estimators=1000, n_jobs=-1, random_state=42, reg_alpa=1, subsample=0.8, verbose=1) . y_pred_xgb = xgb_estimator.predict(X_test) . . confusion_mat(y_test,y_pred_xgb) . . Confusion Matrix : ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . . Parameters Values . 1 Accuracy Score | 76.16 | . 2 Precision Score | 66.45 | . 3 Recall Score | 82.61 | . 4 R1 Score | 73.65 | . plot_roc_curve(y_test,y_pred_xgb,&quot;Tuned XGB&quot;,&quot;pink&quot;) . . ğŸ“ˆğŸ“ˆ Plotting ROC Curve ğŸ“ˆğŸ“ˆ . Plotting AUC_ROC of all the Models . # AUC_ROC curve plt.style.use(&#39;dark_background&#39;) plt.figure(figsize=(8,5)) plt.title(&#39;AUC_ROC Curve&#39;) # Computing False postive rate, and True positive rate fpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred1) # Calculating Area under the curve to display on the plot auc1 = metrics.roc_auc_score(y_test,y_pred1) # Now, plotting the computed values plt.plot(fpr1, tpr1, label = &#39;LLG&#39;, color=&#39;blue&#39;) # Computing False postive rate, and True positive rate fpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred2) # Calculating Area under the curve to display on the plot auc2 = metrics.roc_auc_score(y_test,y_pred2) # Now, plotting the computed values plt.plot(fpr2, tpr2,label = &#39;PLG&#39;, color=&#39;darkorange&#39;) # Computing False postive rate, and True positive rate fpr3, tpr3, thresholds3 = roc_curve(y_test, y_pred3) # Calculating Area under the curve to display on the plot auc3 = metrics.roc_auc_score(y_test,y_pred3) # Now, plotting the computed values plt.plot(fpr3, tpr3, label = &#39;K-NN&#39;, color=&#39;green&#39;) # Computing False postive rate, and True positive rate fpr4, tpr4, thresholds4 = roc_curve(y_test, y_pred4) # Calculating Area under the curve to display on the plot auc4 = metrics.roc_auc_score(y_test,y_pred4) # Now, plotting the computed values plt.plot(fpr4, tpr4, label = &#39;RF&#39;, color=&#39;red&#39;) # Computing False postive rate, and True positive rate fpr5, tpr5, thresholds5 = roc_curve(y_test, y_pred5) # Calculating Area under the curve to display on the plot auc5 = metrics.roc_auc_score(y_test,y_pred5) # Now, plotting the computed values plt.plot(fpr5, tpr5, label = &#39;XGB&#39;, color=&#39;purple&#39;) x = np.linspace(0,1,num=50) plt.plot(x,x,color=&#39;lightgrey&#39;,linestyle=&#39;--&#39;,marker=&#39;&#39;,lw=2,label=&#39;random guess&#39;) plt.legend(fontsize = 10) plt.xlabel(&#39;False positive rate&#39;, fontsize = 18) plt.ylabel(&#39;True positive rate&#39;, fontsize = 18) plt.xlim(0,1.1) plt.ylim(0,1.1) plt.show() . . Conclusion . print(&quot; nModels accuracy metrics before tuning the parameters n n&quot; ,&quot; t t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) a1 = pd.DataFrame({&#39;Models&#39;: &#39;Linear Logistic Regression&#39; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(metrics.accuracy_score(y_pred1, y_test)*100) ,&#39;Precision&#39; : &quot;{:.2f}%&quot;.format(metrics.precision_score(y_pred1, y_test)*100) ,&#39;Recall&#39; : &quot;{:.2f}%&quot;.format(metrics.recall_score(y_pred1, y_test)*100) ,&#39;R1 Score&#39; : &quot;{:.2f}%&quot;.format(metrics.f1_score(y_pred1, y_test)*100)},index={&#39;1&#39;}) a2 = pd.DataFrame({&#39;Models&#39;: &#39;Polynomial Logistic Regression&#39; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(metrics.accuracy_score(y_pred2, y_test)*100) ,&#39;Precision&#39; : &quot;{:.2f}%&quot;.format(metrics.precision_score(y_pred2, y_test)*100) ,&#39;Recall&#39; : &quot;{:.2f}%&quot;.format(metrics.recall_score(y_pred2, y_test)*100) ,&#39;R1 Score&#39; : &quot;{:.2f}%&quot;.format(metrics.f1_score(y_pred2, y_test)*100)},index={&#39;2&#39;}) a3 = pd.DataFrame({&#39;Models&#39;: &#39;K-Nearest Neighbors Classifier&#39; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(metrics.accuracy_score(y_pred3, y_test)*100) ,&#39;Precision&#39; : &quot;{:.2f}%&quot;.format(metrics.precision_score(y_pred3, y_test)*100) ,&#39;Recall&#39; : &quot;{:.2f}%&quot;.format(metrics.recall_score(y_pred3, y_test)*100) ,&#39;R1 Score&#39; : &quot;{:.2f}%&quot;.format(metrics.f1_score(y_pred3, y_test)*100)},index={&#39;3&#39;}) a4 = pd.DataFrame({&#39;Models&#39;: &#39;Random Forest Classifier&#39; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(metrics.accuracy_score(y_pred4, y_test)*100) ,&#39;Precision&#39; : &quot;{:.2f}%&quot;.format(metrics.precision_score(y_pred4, y_test)*100) ,&#39;Recall&#39; : &quot;{:.2f}%&quot;.format(metrics.recall_score(y_pred4, y_test)*100) ,&#39;R1 Score&#39; : &quot;{:.2f}%&quot;.format(metrics.f1_score(y_pred4, y_test)*100)},index={&#39;4&#39;}) a5 = pd.DataFrame({&#39;Models&#39;: &#39;X-Gradient Boosting Classifier&#39; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(metrics.accuracy_score(y_pred5, y_test)*100) ,&#39;Precision&#39; : &quot;{:.2f}%&quot;.format(metrics.precision_score(y_pred5, y_test)*100) ,&#39;Recall&#39; : &quot;{:.2f}%&quot;.format(metrics.recall_score(y_pred5, y_test)*100) ,&#39;R1 Score&#39; : &quot;{:.2f}%&quot;.format(metrics.f1_score(y_pred5, y_test)*100)},index={&#39;5&#39;}) res_table = pd.concat([a1,a2,a3,a4,a5]) res_table.columns = [&#39;Models&#39;,&#39;Accuracy&#39;,&#39;Precision&#39;,&#39;Recall&#39;,&#39;R1 Score&#39;] res_table=res_table.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#636A92&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#FEF5E7&#39;}, subset=[&#39;Models&#39;]) res_table . . Models accuracy metrics before tuning the parameters ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . Models Accuracy Precision Recall R1 Score . 1 Linear Logistic Regression | 73.42% | 65.09% | 78.23% | 71.06% | . 2 Polynomial Logistic Regression | 74.29% | 66.00% | 79.25% | 72.02% | . 3 K-Nearest Neighbors Classifier | 89.29% | 99.93% | 82.44% | 90.34% | . 4 Random Forest Classifier | 95.84% | 99.95% | 92.38% | 96.02% | . 5 X-Gradient Boosting Classifier | 75.50% | 65.85% | 81.74% | 72.94% | . print(&quot; nModels accuracy after tuning the parameters n n&quot; ,&quot; t t t&quot;,emoji.emojize(&quot;:backhand_index_pointing_down_light_skin_tone:&quot;)*3) print(&quot;&quot;) b1 = pd.DataFrame({&#39;Models&#39;: &quot;Linear Logistic Regression(With K-Fold Cross Validation)&quot; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(acc_llr_kf.mean()*100)},index={&#39;1&#39;}) b2 = pd.DataFrame({&#39;Models&#39;: &quot;Polynomial Logistic Regression(With K-Fold Cross Validation)&quot; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(acc_plr_kf.mean()*100)},index={&#39;2&#39;}) b3 = pd.DataFrame({&#39;Models&#39;: &quot;K-Nearest Neighbors Classifier(With Gridsearch CV)&quot; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format((model3_gscv.best_score_)*100)},index={&#39;3&#39;}) b4 = pd.DataFrame({&#39;Models&#39;: &quot;Random Forest Classifier(With Gridsearch CV)&quot; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format((model4_detector.best_score_)*100)},index={&#39;4&#39;}) b5 = pd.DataFrame({&#39;Models&#39;: &quot;X-Gradient Boosting Classifier(With tuning all parameters)&quot; ,&#39;Accuracy&#39; : &quot;{:.2f}%&quot;.format(metrics.accuracy_score(y_pred_xgb, y_test)*100)},index={&#39;5&#39;}) result_table = pd.concat([b1,b2,b3,b4,b5]) result_table.columns = [&#39;Models&#39;,&#39;Accuracy&#39;] result_table=result_table.style.set_table_styles( [{&#39;selector&#39;: &#39;th&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#E06689&#39;), (&#39;color&#39;, &#39;white&#39;), (&#39;font-family&#39;, &#39;verdana&#39;), (&#39;font-size&#39;, &#39;10pt&#39;)]}, {&#39;selector&#39;: &#39;td&#39;, &#39;props&#39;: [(&#39;font-family&#39;, &#39;verdana&#39;), (&#39;padding&#39;,&#39;0em 0em&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(odd)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;#ABB2B9&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:nth-of-type(even)&#39;, &#39;props&#39;: [(&#39;background&#39;, &#39;white&#39;), (&#39;color&#39;, &#39;black&#39;)]}, {&#39;selector&#39;: &#39;tr:hover&#39;, &#39;props&#39;: [(&#39;background-color&#39;, &#39;pink&#39;)]}, {&#39;selector&#39;: &#39;th:hover&#39;, &#39;props&#39;: [(&#39;font-size&#39;, &#39;18pt&#39;)]}, {&#39;selector&#39;: &#39;tr:hover td:hover&#39;, &#39;props&#39;: [(&#39;max-width&#39;, &#39;1000px&#39;), (&#39;font-size&#39;, &#39;18pt&#39;)]} ] ).set_properties(**{&#39;background-color&#39;: &#39;#FEF5E7&#39;}, subset=[&#39;Models&#39;]) result_table . . Models accuracy after tuning the parameters ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ» . Models Accuracy . 1 Linear Logistic Regression(With K-Fold Cross Validation) | 73.21% | . 2 Polynomial Logistic Regression(With K-Fold Cross Validation) | 74.25% | . 3 K-Nearest Neighbors Classifier(With Gridsearch CV) | 90.84% | . 4 Random Forest Classifier(With Gridsearch CV) | 95.27% | . 5 X-Gradient Boosting Classifier(With tuning all parameters) | 76.16% | . Comparing the results of all the Models . df1_styler = res_table.set_table_attributes(&quot;style=&#39;display:inline&#39;&quot;).set_caption(&#39;Before tuning the parameters&#39;) df2_styler = result_table.set_table_attributes(&quot;style=&#39;display:inline&#39;&quot;).set_caption(&#39;After tuning the parameters&#39;) space = &quot; xa0&quot; * 10 display_html(df1_styler._repr_html_()+ space + df2_styler._repr_html_(), raw=True) . . Before tuning the parameters Models Accuracy Precision Recall R1 Score . 1 Linear Logistic Regression | 73.42% | 65.09% | 78.23% | 71.06% | . 2 Polynomial Logistic Regression | 74.29% | 66.00% | 79.25% | 72.02% | . 3 K-Nearest Neighbors Classifier | 89.29% | 99.93% | 82.44% | 90.34% | . 4 Random Forest Classifier | 95.84% | 99.95% | 92.38% | 96.02% | . 5 X-Gradient Boosting Classifier | 75.50% | 65.85% | 81.74% | 72.94% | . Â Â Â Â Â Â Â Â Â Â After tuning the parameters Models Accuracy . 1 Linear Logistic Regression(With K-Fold Cross Validation) | 73.21% | . 2 Polynomial Logistic Regression(With K-Fold Cross Validation) | 74.25% | . 3 K-Nearest Neighbors Classifier(With Gridsearch CV) | 90.84% | . 4 Random Forest Classifier(With Gridsearch CV) | 95.27% | . 5 X-Gradient Boosting Classifier(With tuning all parameters) | 76.16% | .",
            "url": "https://swati5140.github.io/my_data_science_portfolio/2021/09/21/_08_26_bank_marketing_project.html",
            "relUrl": "/2021/09/21/_08_26_bank_marketing_project.html",
            "date": " â€¢ Sep 21, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.â†© . 2. This is the other footnote. You can even have a link!â†© .",
            "url": "https://swati5140.github.io/my_data_science_portfolio/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " â€¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a â€œlevel 1 headingâ€ in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Hereâ€™s a footnote 1. Hereâ€™s a horizontal rule: . . Lists . Hereâ€™s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes â€¦andâ€¦ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.Â &#8617; . |",
            "url": "https://swati5140.github.io/my_data_science_portfolio/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " â€¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Portfolio 1. . Website showcasing Ml projectsÂ &#8617; . |",
          "url": "https://swati5140.github.io/my_data_science_portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://swati5140.github.io/my_data_science_portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}